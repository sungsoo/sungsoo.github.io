\section{Background and Problem Statement}
In this paper, we address the following problems: (1) parameterize the action space in terms of a generating set of primitive motions, (2) reconstruction of new trajectories in terms previously learned primitives, and (3) finite-state machine learning.
\subsection{Definitions and Notation}
%\textbf{SK: What is the typical level of formality in a RSS paper? I made it as formal as made sense we can dial it back if we think this is unneccessary-AG:I think formal is fine.}
Segmentation without an objective function is an ill-defined operation. 
In the literature, many different techniques have been proposed; optimizing widely different criteria.
In this section, we try to abstract of the key components to avoid an apples to oranges comparison of different approaches.
We also introduce the notation that we will use throughout the paper.

Let $\mathcal{D}=\{d_1,d_2,...,d_n\}$ be a set of demonstrations.
Each demonstration is a ordered sequence of $T_i$ observations of a d-dimensional state $x( t_i ) \in \mathbb{R}^d $.
Thus, each $d_i\in \mathbb{R}^{d \times T_i}$.
Let $\omega(d,s)$ be the windowing operation; that it applies a sliding window  of size $s$ to $d$ and breaks $d$ into a set of smaller observations.
Formally, $\omega$ maps every $d_i$ to a set of $T_i-s$ sequences of observations of size $\{ \mathbb{R}^{d \times s} \}$.
We define the windowing operation $\Omega$ on the full set of demonstrations to be:
\[\Omega(\mathcal{D},s) = \cup_i^n \omega(d_i,s)\]

\textbf{SK: most specific definition that can encompass DS3, Submodularity, and BP}
\begin{definition} Set of Motion Primitives. 
We represent the set of all possible windows as $\Theta$.
$\Theta$ is a semigroup that has an operation:
\[*:\Theta \times \Theta \mapsto \Theta \]
called the combination operation that takes two windows as input and maps them to another window.
This operation is commutative and associative.
Under this semigroup $<\Theta,*>$, the set of primitves $\mathcal{P}$ is 
a generating set for $\Theta$.
That is, \[\forall \theta \in \Theta \exists p \subset \mathcal{P}: p_1*p_2*p_3*... = \theta\]
\end{definition}

This definition, is intentionally pedantic and broad, and we will find that in fact different trajectory segmentation methods 
in the literature make different assumptions with respect to this definition to make their computation tractable. 
\begin{itemize}
\item The DS3 model restricts $\Theta$ to only those sliding windows that are observed.
The combination operation $*$ in this model is a rotation invariant convex convex combination.
\item The Beta Process model relaxes the assumption on $\Theta$ treating each window as a sample from an
infinitely supported continuous probability distribution. However, it additionally adds the restriction on 
$*$ that it can only be a Euclidean addition.
\item The Submodular approach, like the DS3 model, restricts $\Theta$ to only those sliding windows that are observed.
Where the $*$ operation is a union all other elements within some $\epsilon$ of similarity.
\end{itemize}

An important point to note is that these algorithms require a definition of similarity between two elements in $\Theta$, which we abstract away
in our semigroup model of the motion primitive.
We treat that operations as external to the combination operator $*$.
Formalizing this notion of similarity becomes imporant when we want to define reconstruction.
Metrics are imporant when we want to discuss the optimality of a primitive representation,
as there may be many possible generating sets.
The concept of reconstruction gives us a framework to analyze this, however it requires us
to formalize reconstruction in terms of a metric space on $\Theta$.

\begin{definition} Reconstruction. 
Define a new operation:
\[ d:\Theta \times \Theta \mapsto \mathbb{R} \]
If $d$ is a metric, then $<\Theta,d>$ is a metric space.
For a $t \not \in \Theta$, we can define a projection onto
$\Theta$:
\[ e = \min d(t,\Theta) \]
Since the projection exists in $\Theta$, it can be constructed 
from the generating set of primitives.
Therefore, the \emph{Reconstruction Error} of $t$ is e; the closest
the generating set can come to reconstructing $t$.
\end{definition}

This definition of reconstruction gives informs us on how we should evaluate differing approaches. 
If we ensure that the distance metric is consistent, then the projection error measures a consistent value.
\textbf{SK: problem with the definitions below:}
However, there are nuances as different approaches to segmentation actually define $\Theta$ differently.
There might be only a subset of the observed $\Theta$ that is exactly represented by the generating set of primitives.

Finally, we need to introduce a notion of temporality.
We model temporality in a Markovian way, where every 
next action (trajectory window) is conditionally independent 
with history given the current state.

\begin{definition} Markov Chain of Primitives.
Given a set of primitives $\mathcal{P}$.
Let $Z$ be a binary vector of dimension $\mid\mathcal{P}\mid$:
\[Z\in \mathbb{Z}_2^{\mid\mathcal{P}\mid}\].
Given a demonstration $d_i$ and an \emph{ordered} sequence of windows
$\omega(d_i,s)$.
For each $d_i$, there exists an ``active" subset of $\mathcal{P}$ such 
that $d_i$ is generated from that subset of $\mathcal{P}$.
If we assign each $p\in \mathcal{P}$ an index, then we can represent
the \emph{state} of each $d_i$ with $Z$ (one if active, zero if not).
Then, for the $2^{\mid\mathcal{P}\mid}$ states, we learn a Markov
Chain based on the observed transitions.
\end{definition}

\textbf{SK problem!}
Obviously, learning a state machine, whether probablistic or deterministic, faces an immense problem with the curse of
dimensionality. For a set of $k$ primitives, there are $2^k$ possible states the in which the system may be.


\subsection{Problem Statement}

\subsubsection{Problem 1. Trajectory Segmentation}
We take as input a set of demonstrations $\mathcal{D}$. 
The output is a set of motion primitives as defined in the previous sub-section.
\textbf{SK: explain why we should care....}

\subsubsection{Problem 2. Reconstruction}
We take as input a set of primitives $\mathcal{P}$ and a new trajectory $t$. 
The output is a reconstruction of $t$ with the elements in $\mathcal{P}$ that minimizes reconstruction error as defined before.
\textbf{SK: explain why we should care....}

\subsubsection{Problem 3. Markov Chain Learning}
We take as input a set of primitives $\mathcal{P}$ and a set of demonstrations $mathcal{D}$. 
The output is a Markov chain showing the state transition probabilities.
This can be turned into an FSM by taking the most likely path through the chain.
\textbf{SK: explain why we should care....}
