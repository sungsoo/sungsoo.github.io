---
layout: post
title: BlinkDB Review
date: 2015-10-09
categories: [computer science]
tags: [big data]

---

# BlinkDB Review

## Introduction

<div class="sites-embed-align-right-wrapping-on"><div class="sites-embed-border-off sites-embed" style="width:250px;"><div class="sites-embed-content sites-embed-type-toc"><div class="goog-toc sites-embed-toc-maxdepth-6"><p>Contents</p><ol class="goog-toc"><li class="goog-toc"><a href="#TOC-Introduction"><strong>1 </strong>Introduction</a></li><li class="goog-toc"><a href="#TOC-System-Overview-"><strong>2 </strong>System Overview&nbsp;</a><ol class="goog-toc"><li class="goog-toc"><a href="#TOC-Settings-and-Assumptions"><strong>2.1 </strong>Settings and Assumptions</a></li><li class="goog-toc"><a href="#TOC-Architecture"><strong>2.2 </strong>Architecture</a><ol class="goog-toc"><li class="goog-toc"><a href="#TOC-Offline-Sample-Creation-and-Maintenance"><strong>2.2.1 </strong>Offline Sample Creation and Maintenance</a></li><li class="goog-toc"><a href="#TOC-Run-time-Sample-Selection"><strong>2.2.2 </strong>Run-time Sample Selection</a></li></ol></li></ol></li><li class="goog-toc"><a href="#TOC-Sample-Creation"><strong>3 </strong>Sample Creation</a><ol class="goog-toc"><li class="goog-toc"><a href="#TOC-Multi-resolution-stratified-Samples"><strong>3.1 </strong>Multi-resolution stratified Samples</a></li><li class="goog-toc"><a href="#TOC-Optimization-Framework"><strong>3.2 </strong>Optimization Framework</a></li></ol></li><li class="goog-toc"><a href="#TOC-BlinkDB-Running"><strong>4 </strong>BlinkDB Running</a><ol class="goog-toc"><li class="goog-toc"><a href="#TOC-Selecting-the-Sample-Family"><strong>4.1 </strong>Selecting the Sample Family</a></li><li class="goog-toc"><a href="#TOC-Selecting-the-Sample-Size"><strong>4.2 </strong>Selecting the Sample Size</a></li></ol></li><li class="goog-toc"><a href="#TOC-Implementation"><strong>5 </strong>Implementation</a></li><li class="goog-toc"><a href="#TOC-Evaluation-and-Results"><strong>6 </strong>Evaluation and Results</a></li><li class="goog-toc"><a href="#TOC-Conclusion"><strong>7 </strong>Conclusion</a></li></ol></div></div></div></div></h2><div style="text-align:left">BlinkDB is a massively parallel, sample-based approximate query engine for running ad-hoc,&nbsp;interactive SQL queries on large volumes of data.&nbsp;The core idea of &nbsp;BlinkDB is by enabling interactive queries over massive data by running queries on data &nbsp;samples and representing results annotated with meaningful error bars to achieve a trade-off query accuracy for response time. To achieve this,&nbsp;BlinkDB uses two key ideas that differentiate it from previous&nbsp;work in this area: (1) an adaptive optimization framework&nbsp;that builds and maintains a set of multi-dimensional,&nbsp;multi-resolution samples from original data over time, and&nbsp;(2) a dynamic sample selection strategy that selects an appropriately&nbsp;sized sample based on a query’s accuracy and/or&nbsp;response time requirements.</div><div style="text-align:left">When dealing with query orders from users,&nbsp;traditionally&nbsp;but inefficient, data process systems need to scan large fractions of a database to compute the appropriate statistics. However, the <b>response time</b> of the system is proportional to the data scale and also the response time to some extend decide the potential profit. Another problem is uniform samples had poorly performance for skewed distributions and also uniform samples may not contain certain infrequent subgroups which may leads to missing rows in the final output of queries. Facing these problems mentioned above, BlinkDB provides Multi-dimensional, Multi-resolution strategy which make three main contributions as following:</div><div style="text-align:left"><ol><li>Provides faster convergence,&nbsp;minimizes missing results in the output , and provides error/latency guarantees for&nbsp;ad-hoc workloads.</li><li>cast the decision of what stratified samples to build&nbsp;as an optimization problem that takes into account: (i)&nbsp;the skew of the data distribution, (ii) query templates,&nbsp;and (iii) the storage overhead of each sample.</li><li>develop a run-time dynamic sample selection strategy&nbsp;that uses multiple smaller samples to quickly estimate&nbsp;query selectivity and choose the best samples&nbsp;for satisfying the response time and error guarantees.</li></ol>In summary,&nbsp;BlinkDB is a massively parallel query processing system&nbsp;that incorporates these ideas. In this project, I will try to understand the whole design of BlinkDB and try to go through the experiment environments which using&nbsp;both the TPC-H benchmarks and a real-world&nbsp;workload derived from Conviva Inc as given in the paper. Hopefully, the experiments can show&nbsp;multi-dimensional sampling approach,&nbsp;versus just using single dimensional samples&nbsp;&nbsp;can improve query response times and some graphs may given at the last.</div><div style="text-align:left"><br></div><div style="text-align:left"><br></div><div style="text-align:left"><br></div><div>
<h2><a name="TOC-System-Overview-"></a>System Overview&nbsp;</h2><div>As presented in this paper,&nbsp;BlinkDB supports a hybrid&nbsp;programming model that allows users to write SQL-style declarative queries with custom user defined functions&nbsp;(UDFs). In addition, for aggregation queries (i.e., AVG,&nbsp;SUM, PERCENTILE etc.), users can annotate queries with&nbsp;either a maximum error or maximum execution time constraint.&nbsp;Based on these constraints, BlinkDB selects an appropriately&nbsp;sized data sample at runtime on which the query&nbsp;operates. specifically, to&nbsp;specify an error bound, the user supplies a bound of the form&nbsp;(r.;C), indicating that the query should return an answer that&nbsp;is within (-r,+r) .. of the true answer with a confidence C. An example is given as:</div><div><div style="text-align:left">SELECT COUNT(*)</div><div style="text-align:left">FROM Sessions</div><div style="text-align:left">WHERE Genre = ‘western’</div><div style="text-align:left">GROUP BY OS</div><div style="text-align:left">ERROR WITHIN 10% AT CONFIDENCE 95%</div></div><div>Under which the output is should be given within a relative error of (-10%, +10%) and with confidence 95%. What is more, users can also request a time bound through change&nbsp;"ERROR WITHIN 10% AT CONFIDENCE 95%" &nbsp;to "WITHIN 5 SECONDS". All above&nbsp;enables a user&nbsp;to perform rapid exploratory analysis on massive amounts of&nbsp;data, wherein he/she can progressively tweak the query bounds&nbsp;until the desired accuracy is achieved.</div><h3><a name="TOC-Settings-and-Assumptions"></a>Settings and Assumptions</h3><div>Before the experiments, there are several necessary assumptions to make in BlinkDB system.</div><ul><li>Query with Joins.&nbsp;</li></ul>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; BlinkDB supports two types of joins (1) Arbitrary joins are allowed whenever there is a &nbsp; stratified sample on one of the join tables that contains the join key its column set. (2) Under the case which in the absence of any suitable stratified sample, the join is still allowed as long as one of the two tables fits in the memory. And the second condition is more common is practice.&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;<br><ul><li>Workload Characteristics.</li></ul>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Since BlinkDB's workload is targeted at ad-hoc queries, this paper assume the query templates remain fairly stable over time when choosing which samples to create. What is worth mention is that &nbsp;although BlinkDB creates a set of stratified&nbsp;samples based on past query templates, at runtime, it can still&nbsp;use the set of available samples to answer any query, even if<div>it is not from one of the historical templates.</div></div><div><ul><li>Closed-Form Aggregates.</li></ul>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; This paper focus on a small set of aggregation operators: COUNT, SUM, MEAN, MEDIAN/QUANTILE. BlinkDB estimate error of these functions&nbsp;using standard estimates of closed-form error. However,&nbsp;closed-form estimates&nbsp;can be easily derived for any combination of these&nbsp;basic aggregates as well as any algebraic function that is&nbsp;mean-like and asymptotically normal.</div><div><ul><li>Offline Sampling.</li></ul>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;BlinkDB computes samples of input data&nbsp;and reuses them across many queries. It may happen that&nbsp;there is a small but non-zero probability that a given sample&nbsp;may be non-representative of the true data, which may leads in un-meet the requirements on response time or error rate. Rather than existing solutions towards the problems, BlinkDB prefer&nbsp;periodically replace samples&nbsp;with new ones in the background with taking the economical and stability.</div><div><h3><a name="TOC-Architecture"></a>Architecture</h3><div><div style="display:block;text-align:center;margin-right:auto;margin-left:auto"><a href="https://sites.google.com/site/cps516haosproject/project-definition/architecture.png?attredirects=0" imageanchor="1"><img src="https://sites.google.com/site/cps516haosproject/_/rsrc/1351820963455/project-definition/architecture.png" border="0"></a></div><div style="display:block;text-align:center;margin-right:auto;margin-left:auto">Figure 1 Architecture of BlinkDB framework</div><div style="text-align:left;display:block;margin-right:auto;margin-left:auto">As we can see, Figure 1 shows the whole architecture of BlinkDB's framework, &nbsp;BlinkDB&nbsp;builds on the Apache Hive framework&nbsp;and adds two&nbsp;major components to it: (1) an offline sampling module that&nbsp;creates and maintains samples over time, and (2) a run-time&nbsp;sample selection module that creates an Error-Latency Profile&nbsp;(ELP) for ad-hoc queries. The ELP characterizes the&nbsp;rate at which the error (or response time) decreases (or increases)&nbsp;as the size of the sample on which the query operates&nbsp;increases. This is used to select a sample that best&nbsp;satisfies the user’s constraints. BlinkDB augments the query&nbsp;parser, optimizer, and a number of aggregation operators to&nbsp;allow queries to specify constraints for accuracy, or execution</div><div style="margin-right:auto;margin-left:auto">time.</div></div><h4><a name="TOC-Offline-Sample-Creation-and-Maintenance"></a>Offline Sample Creation and Maintenance</h4><div><div>This part is responsible for creating and maintaining&nbsp;a set of uniform and stratified samples. BlinkDB use uniform&nbsp;samples over the entire dataset to handle queries on groups&nbsp;of columns with relatively uniform distributions, and stratified&nbsp;samples (on one or more columns) to handle queries&nbsp;on groups of columns with less uniform distributions. This&nbsp;component consists of three sub-components:</div><div><ol><li>Offline Sample Creation.&nbsp;<span style="font-style:italic">Based on statistics collected from the data and historic query templates, BlinkDB computes a set of uniform samples and multiple sets of stratified samples from the underlying data. Intuitively, the optimization framework builds stratified samples over column(s) that are (a) most useful for the query templates in the workload, and (b) most skewed.</span></li><li>Sample &nbsp; Maintenance.&nbsp;&nbsp;<i>As new data arrives, BlinkDB periodically update the initial set of samples. Its update strategy is designed to minimize performance overhead and avoid service interruption. A monitoring module observes overall system performance, detecting any significant changes in data distribution, and triggers periodic sample replacement, and updates, deletions, or creations of new samples.</i></li><li>Storage optimizations.<i>&nbsp;In addition to caching samples in memory, to maximize disk throughput, BlinkDB partition each sample into many small files, and leverage the block distribution strategy of HDFS to spread those files across the&nbsp;</i><i>nodes in a cluster. additionally, we optimize the storage&nbsp;</i><i>overhead, by recursively building larger samples as a union&nbsp;</i><i>of smaller samples that are built on the same set of columns.</i></li></ol><h4><a name="TOC-Run-time-Sample-Selection"></a><i>Run-time Sample Selection</i></h4></div></div></div><div><div>Given a query, BlinkDB select an optimal sample at runtime so&nbsp;as to meet its accuracy or response time constraints. &nbsp;BlinkDB&nbsp;do&nbsp;this by dynamically running the query on smaller samples&nbsp;to estimate the query’s selectivity, error rate, and response&nbsp;time, and then extrapolate to a sample size that will satisfy&nbsp;user-specified error or response time goals.&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</div></div><div><div style="margin-right:auto;margin-left:auto"><br></div><h2><a name="TOC-Sample-Creation"></a>Sample Creation</h2>This part is about the details in creating multi-dimensional. multi-resolution samples.<br><h3><a name="TOC-Multi-resolution-stratified-Samples"></a>Multi-resolution stratified Samples</h3><div>In order to achieve the goal that fast scan of skewed distributions and contain every instance of certain subgroups, I'd like to introduce to&nbsp;use of stratified sampling in BlinkDB. Firstly, we given table 1 as notations of&nbsp;various.</div><div><div style="text-align:center;display:block"><a href="https://sites.google.com/site/cps516haosproject/project-definition/table%201.png?attredirects=0" imageanchor="1"><img src="https://sites.google.com/site/cps516haosproject/_/rsrc/1351820990886/project-definition/table%201.png" border="0"></a></div><div style="text-align:center;display:block">Table 1 notations of terms may used</div><div style="text-align:left;display:block"><br></div></div><div>Let&nbsp;<a href="https://sites.google.com/site/cps516haosproject/project-definition/1.png?attredirects=0" imageanchor="1"><img src="https://sites.google.com/site/cps516haosproject/_/rsrc/1351822103563/project-definition/1.png" border="0"></a>&nbsp;be a subset of columns in the original table, T.&nbsp;For any such subset we define a sample&nbsp;family as a sequence of stratified samples over&nbsp;<a href="https://sites.google.com/site/cps516haosproject/project-definition/2.png?attredirects=0" imageanchor="1"><img src="https://sites.google.com/site/cps516haosproject/_/rsrc/1351822284629/project-definition/2.png" border="0"></a>&nbsp;:</div><div>&nbsp;<div style="text-align:center;display:block"><a href="https://sites.google.com/site/cps516haosproject/project-definition/3.png?attredirects=0" imageanchor="1"><img src="https://sites.google.com/site/cps516haosproject/_/rsrc/1351822637612/project-definition/3.png" border="0"></a></div><div style="text-align:left;display:block">where m is the number of samples in the family.&nbsp;By maintaining&nbsp;multiple stratified samples for the same column subset <a href="https://sites.google.com/site/cps516haosproject/project-definition/2.png?attredirects=0" imageanchor="1"><img src="https://sites.google.com/site/cps516haosproject/_/rsrc/1351822284629/project-definition/2.png" border="0"></a>&nbsp;. we allow a finer granularity tradeoff between query&nbsp;accuracy and response time.</div><div style="text-align:left;display:block">A stratified sample S(<a href="https://sites.google.com/site/cps516haosproject/project-definition/2.png?attredirects=0" imageanchor="1"><img src="https://sites.google.com/site/cps516haosproject/_/rsrc/1351822284629/project-definition/2.png" border="0"></a>, K(i)&nbsp;) on the set of columns,&nbsp;<a href="https://sites.google.com/site/cps516haosproject/project-definition/2.png?attredirects=0" imageanchor="1"><img src="https://sites.google.com/site/cps516haosproject/_/rsrc/1351822284629/project-definition/2.png" border="0"></a>&nbsp;caps the frequency of every value x in&nbsp;<div style="display:block;text-align:left"><a href="https://sites.google.com/site/cps516haosproject/project-definition/2.png?attredirects=0" imageanchor="1"><img src="https://sites.google.com/site/cps516haosproject/_/rsrc/1351822284629/project-definition/2.png" border="0"></a>to K(i). What is more, tuple x=&lt;x(1), x(2),...., x(k)&gt; where x(i) is the value in column c(i), and let F(<a href="https://sites.google.com/site/cps516haosproject/project-definition/2.png?attredirects=0" imageanchor="1"><img src="https://sites.google.com/site/cps516haosproject/_/rsrc/1351822284629/project-definition/2.png" border="0"></a>, T, x)&nbsp;be the frequency of x&nbsp;in column set<a href="https://sites.google.com/site/cps516haosproject/project-definition/2.png?attredirects=0" imageanchor="1"><img src="https://sites.google.com/site/cps516haosproject/_/rsrc/1351822284629/project-definition/2.png" border="0"></a>&nbsp;. in the original table, T. &nbsp;we design that if&nbsp;F(<a href="https://sites.google.com/site/cps516haosproject/project-definition/2.png?attredirects=0" imageanchor="1" style="background-color:rgb(238,232,221)"><img src="https://sites.google.com/site/cps516haosproject/_/rsrc/1351822284629/project-definition/2.png" border="0"></a>, T, x)<a href="https://sites.google.com/site/cps516haosproject/project-definition/4.png?attredirects=0" imageanchor="1"><img src="https://sites.google.com/site/cps516haosproject/_/rsrc/1351823837570/project-definition/4.png" border="0"></a>K(i), then means&nbsp;S(<a href="https://sites.google.com/site/cps516haosproject/project-definition/2.png?attredirects=0" imageanchor="1" style="background-color:rgb(238,232,221)"><img src="https://sites.google.com/site/cps516haosproject/_/rsrc/1351822284629/project-definition/2.png" border="0"></a>, K(i)&nbsp;)&nbsp;contains all rows containing x in T, so BlinkDB answers a query Q&nbsp;exact as&nbsp;the sample contains all rows from the original table.&nbsp;Otherwise,</div><div style="display:block;text-align:left"><div>if&nbsp;F(<a href="https://sites.google.com/site/cps516haosproject/project-definition/2.png?attredirects=0" imageanchor="1" style="background-color:rgb(238,232,221)"><img src="https://sites.google.com/site/cps516haosproject/_/rsrc/1351822284629/project-definition/2.png" border="0"></a>, T, x)&gt;K(i). Then&nbsp;&nbsp;S(<a href="https://sites.google.com/site/cps516haosproject/project-definition/2.png?attredirects=0" imageanchor="1" style="background-color:rgb(238,232,221)"><img src="https://sites.google.com/site/cps516haosproject/_/rsrc/1351822284629/project-definition/2.png" border="0"></a>, K(i)&nbsp;)&nbsp;contains K(i) randomly chosen rows from T that contains x and the answer with same Q will&nbsp;based&nbsp;on K random rows in the original table.&nbsp;It is worth mention that when there are different samples with different sizes, there is no need to independently&nbsp;allocate storage for each sample. Instead, we can construct&nbsp;smaller samples from the larger ones, and thus need an&nbsp;amount of storage equivalent to maintaining only the largest&nbsp;sample.</div><div><b>Two Properties.</b></div><div><ol><ol><li>For a query with response time constraints, the response&nbsp;time of the query running on&nbsp;&nbsp;S(<a href="https://sites.google.com/site/cps516haosproject/project-definition/2.png?attredirects=0" imageanchor="1" style="background-color:rgb(238,232,221)"><img src="https://sites.google.com/site/cps516haosproject/_/rsrc/1351822284629/project-definition/2.png" border="0"></a>, K'(i)&nbsp;)&nbsp;is within&nbsp;a factor of c of the response time of the query running on the optimal-sized sample,&nbsp;<a href="https://sites.google.com/site/cps516haosproject/project-definition/5.png?attredirects=0" imageanchor="1"><img src="https://sites.google.com/site/cps516haosproject/_/rsrc/1351868516340/project-definition/5.png" border="0"></a>&nbsp;.</li><li>For a query with error constraints, the standard deviation&nbsp;of the query running on&nbsp;&nbsp;S(<a href="https://sites.google.com/site/cps516haosproject/project-definition/2.png?attredirects=0" imageanchor="1" style="background-color:rgb(238,232,221)"><img src="https://sites.google.com/site/cps516haosproject/_/rsrc/1351822284629/project-definition/2.png" border="0"></a>, K'(i)&nbsp;) is within a factor of&nbsp;<a href="https://sites.google.com/site/cps516haosproject/project-definition/6.png?attredirects=0" imageanchor="1"><img src="https://sites.google.com/site/cps516haosproject/_/rsrc/1351868947367/project-definition/6.png" border="0"></a>of the response time of the query running on&nbsp;<a href="https://sites.google.com/site/cps516haosproject/project-definition/5.png?attredirects=0" imageanchor="1" style="color:rgb(0,63,92);background-color:rgb(238,232,221)"><img src="https://sites.google.com/site/cps516haosproject/_/rsrc/1351868516340/project-definition/5.png" border="0"></a></li></ol></ol>These two properties are defined to the issue that how "good" is a sample family. Most of the terms are showed at table 1. And&nbsp;<a href="https://sites.google.com/site/cps516haosproject/project-definition/5.png?attredirects=0" imageanchor="1" style="color:rgb(0,63,92);background-color:rgb(238,232,221)"><img src="https://sites.google.com/site/cps516haosproject/_/rsrc/1351868516340/project-definition/5.png" border="0"></a>&nbsp;be the samllest possible stratified sample on&nbsp;<div style="display:block;text-align:left"><a href="https://sites.google.com/site/cps516haosproject/project-definition/2.png?attredirects=0" imageanchor="1"><img src="https://sites.google.com/site/cps516haosproject/_/rsrc/1351822284629/project-definition/2.png" border="0"></a>that&nbsp;satisfies the error or&nbsp;response&nbsp;time constraints of Q.</div><div style="display:block;text-align:left">Another factor should be taken in consideration is the overhead storage, we may talk it in details later.</div><h3><a name="TOC-Optimization-Framework"></a>Optimization Framework</h3></div><div>As we have discussed before, we could say in confidence that&nbsp;stratified&nbsp;samples on multiple columns that are frequently queried together&nbsp;can lead to significant improvements in both query&nbsp;accuracy and latency, especially when the set of columns&nbsp;have a skewed joint distribution. However, these samples&nbsp;lead to an increase in the storage overhead because (1) samples&nbsp;on multiple columns can be larger than single-column&nbsp;samples since multiple &nbsp;columns often contains more unique&nbsp;values than individual columns, and (2) there are an exponential&nbsp;number of subsets of columns, all of which may not&nbsp;fit in our storage budget. So we may want do some optimal work in this part. &nbsp;There factor should taken into account in problem formulation:</div><div><ol><li><b>Non-uniformity (skew) of the data.</b>&nbsp;&nbsp;Intuitively, the greater&nbsp;the skew for a set of columns, the more important it is to have&nbsp;a stratified sample on those columns. We let D(<a href="https://sites.google.com/site/cps516haosproject/project-definition/2.png?attredirects=0" imageanchor="1"><img src="https://sites.google.com/site/cps516haosproject/_/rsrc/1351822284629/project-definition/2.png" border="0"></a>) denote&nbsp;the set of all distinct values appearing in&nbsp;<a href="https://sites.google.com/site/cps516haosproject/project-definition/2.png?attredirects=0" imageanchor="1"><img src="https://sites.google.com/site/cps516haosproject/_/rsrc/1351822284629/project-definition/2.png" border="0"></a>&nbsp;and&nbsp;F(<a href="https://sites.google.com/site/cps516haosproject/project-definition/2.png?attredirects=0" imageanchor="1" style="background-color:rgb(238,232,221)"><img src="https://sites.google.com/site/cps516haosproject/_/rsrc/1351822284629/project-definition/2.png" border="0"></a>, T, v) be the&nbsp;is the frequency of value v in&nbsp;<a href="https://sites.google.com/site/cps516haosproject/project-definition/2.png?attredirects=0" imageanchor="1"><img src="https://sites.google.com/site/cps516haosproject/_/rsrc/1351822284629/project-definition/2.png" border="0"></a>. Let&nbsp;<a href="https://sites.google.com/site/cps516haosproject/project-definition/8.png?attredirects=0" imageanchor="1"><img src="https://sites.google.com/site/cps516haosproject/_/rsrc/1351871395180/project-definition/8.png" border="0"></a>&nbsp;be a non-uniformity metric on the distribution of the&nbsp;values in&nbsp;<a href="https://sites.google.com/site/cps516haosproject/project-definition/2.png?attredirects=0" imageanchor="1"><img src="https://sites.google.com/site/cps516haosproject/_/rsrc/1351822284629/project-definition/2.png" border="0"></a>. We define the notion of non-uniformity as:</li><div style="text-align:center"><a href="https://sites.google.com/site/cps516haosproject/project-definition/9.png?attredirects=0" imageanchor="1"><img src="https://sites.google.com/site/cps516haosproject/_/rsrc/1351871860988/project-definition/9.png" border="0"></a></div><div>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Where K represents the cap corresponding to the largest sample in the family &nbsp; &nbsp; &nbsp;S(<a href="https://sites.google.com/site/cps516haosproject/project-definition/2.png?attredirects=0" imageanchor="1"><img src="https://sites.google.com/site/cps516haosproject/_/rsrc/1351822284629/project-definition/2.png" border="0"></a>,K).</div><li><b>Workload.&nbsp;</b>BlinkDB use a query workload define &nbsp;as a&nbsp;set of m query templates and their weights:&nbsp;<a href="https://sites.google.com/site/cps516haosproject/project-definition/10.png?attredirects=0" imageanchor="1" style="text-align:center"><img src="https://sites.google.com/site/cps516haosproject/_/rsrc/1351877469498/project-definition/10.png" border="0"></a>&nbsp;where 0&lt;w(i)&lt;1 is the weight of the ith query template and&nbsp;<a href="https://sites.google.com/site/cps516haosproject/project-definition/11.png?attredirects=0" imageanchor="1" style="background-color:rgb(238,232,221)"><img src="https://sites.google.com/site/cps516haosproject/_/rsrc/1351877669571/project-definition/11.png" border="0"></a>&nbsp;is the set of columns appearing in the ith template's WHERE and GROUP BY clauses.</li><li><b>Storage cost. </b>In this paper, the author&nbsp;use Store(<a href="https://sites.google.com/site/cps516haosproject/project-definition/2.png?attredirects=0" imageanchor="1"><img src="https://sites.google.com/site/cps516haosproject/_/rsrc/1351822284629/project-definition/2.png" border="0"></a>.) to denote the storage&nbsp;cost of building a sample family on a set of&nbsp;columns .<a href="https://sites.google.com/site/cps516haosproject/project-definition/2.png?attredirects=0" imageanchor="1"><img src="https://sites.google.com/site/cps516haosproject/_/rsrc/1351822284629/project-definition/2.png" border="0"></a>.</li></ol>Given the three factors above, BlinkDB&nbsp;maximize the following mixed&nbsp;linear integer program (MILP):</div><div><div style="text-align:center;display:block"><a href="https://sites.google.com/site/cps516haosproject/project-definition/12.png?attredirects=0" imageanchor="1"><img src="https://sites.google.com/site/cps516haosproject/_/rsrc/1351879636659/project-definition/12.png" border="0"></a></div><div style="text-align:left">subject to</div><div style="text-align:left"><div style="text-align:center;display:block"><a href="https://sites.google.com/site/cps516haosproject/project-definition/13.png?attredirects=0" imageanchor="1"><img src="https://sites.google.com/site/cps516haosproject/_/rsrc/1351879684168/project-definition/13.png" border="0"></a></div><div style="text-align:left">and&nbsp;</div><div style="text-align:left"><div style="text-align:center;display:block"><a href="https://sites.google.com/site/cps516haosproject/project-definition/14.png?attredirects=0" imageanchor="1"><img src="https://sites.google.com/site/cps516haosproject/_/rsrc/1351879735190/project-definition/14.png" border="0"></a></div><div style="text-align:left;display:block">where&nbsp;<a href="https://sites.google.com/site/cps516haosproject/project-definition/15.png?attredirects=0" imageanchor="1"><img src="https://sites.google.com/site/cps516haosproject/_/rsrc/1351879801859/project-definition/15.png" border="0"></a>.</div><div style="text-align:left;display:block">we need to weigh the coverage of each set of &nbsp;columns by their importance: a set of columns <div style="display:block;text-align:left"><a href="https://sites.google.com/site/cps516haosproject/project-definition/11.png?attredirects=0" imageanchor="1"><img src="https://sites.google.com/site/cps516haosproject/_/rsrc/1351877669571/project-definition/11.png" border="0"></a>is more&nbsp;important to cover when (1) it has a higher frequency, which&nbsp;is represented by w(i), or (2) when the joint distribution of&nbsp;.&nbsp;is more skewed (non-uniform), which is represented by&nbsp;<a href="https://sites.google.com/site/cps516haosproject/project-definition/16.png?attredirects=0" imageanchor="1"><img src="https://sites.google.com/site/cps516haosproject/_/rsrc/1351881306499/project-definition/16.png" border="0"></a>&nbsp;. Thus, the best solution is when we maximize the&nbsp;sum of w(i) *y(i) *<a href="https://sites.google.com/site/cps516haosproject/project-definition/16.png?attredirects=0" imageanchor="1"><img src="https://sites.google.com/site/cps516haosproject/_/rsrc/1351881306499/project-definition/16.png" border="0"></a><br> for all query templates, as captured by&nbsp;our goal function.</div></div>Consider in practice, BlinkDB&nbsp;restrict the candidate subsets to only those that&nbsp;have appeared together at least in one of the query templates.&nbsp;This does not affect the optimality&nbsp;of the solution, because a column A that has not appeared&nbsp;with the rest of the columns in .<a href="https://sites.google.com/site/cps516haosproject/project-definition/2.png?attredirects=0" imageanchor="1"><img src="https://sites.google.com/site/cps516haosproject/_/rsrc/1351822284629/project-definition/2.png" border="0"></a>can be safely removed without&nbsp;affecting any of the query templates. Also, BlinkDB&nbsp;limit candidate</div><div>subsets to those consisting of no more than a fixed&nbsp;number of columns, like 3 or 4 columns.</div><div>In handing data/workload variations in BlinkDB, the optimization&nbsp;formulation is designed to avoid over-fitting samples&nbsp;to past queries by: (i) only looking at the set of columns&nbsp;that appear in the query templates instead optimizing for specific&nbsp;constants in queries and (ii) considering infrequent subsets&nbsp;with a high degree of skew.In addition, BlinkDB periodically&nbsp;updates&nbsp;data and workload statistics to decide whether the current set&nbsp;of sample families are still effective or if the optimization&nbsp;problem needs to be re-solved based on the new input parameters.&nbsp;</div><div><div>Specifically, BlinkDB allows&nbsp;the administrator to decide what percentage of the sample&nbsp;families (in terms of storage cost) can be discarded/added&nbsp;to the system whenever BlinkDB triggers the sample creation&nbsp;module as a result of changes in data or workload distribution.</div><div><div style="text-align:center;display:block"><a href="https://sites.google.com/site/cps516haosproject/project-definition/17.png?attredirects=0" imageanchor="1"><img src="https://sites.google.com/site/cps516haosproject/_/rsrc/1351883647678/project-definition/17.png" border="0"></a></div><div style="text-align:left;display:block">Here .<a href="https://sites.google.com/site/cps516haosproject/project-definition/18.png?attredirects=0" imageanchor="1"><img src="https://sites.google.com/site/cps516haosproject/_/rsrc/1351883792164/project-definition/18.png" border="0"></a>’s are additional input parameters stating whether&nbsp;.<a href="https://sites.google.com/site/cps516haosproject/project-definition/19.png?attredirects=0" imageanchor="1"><img src="https://sites.google.com/site/cps516haosproject/_/rsrc/1351883988908/project-definition/19.png" border="0"></a>already exists in the system (when .<a href="https://sites.google.com/site/cps516haosproject/project-definition/18.png?attredirects=0" imageanchor="1" style="color:rgb(0,63,92);background-color:rgb(238,232,221)"><img src="https://sites.google.com/site/cps516haosproject/_/rsrc/1351883792164/project-definition/18.png" border="0"></a>= 1) or it does&nbsp;not (.<a href="https://sites.google.com/site/cps516haosproject/project-definition/18.png?attredirects=0" imageanchor="1" style="color:rgb(0,63,92);background-color:rgb(238,232,221)"><img src="https://sites.google.com/site/cps516haosproject/_/rsrc/1351883792164/project-definition/18.png" border="0"></a>&nbsp;= 0).&nbsp; &nbsp;&nbsp;</div><h2><a name="TOC-BlinkDB-Running"></a><span style="font-style:normal">BlinkDB Running</span></h2></div></div><div><div style="font-style:normal">In this section, we provide an overview of query execution&nbsp;in BlinkDB. and our approach for online sample selection.&nbsp;Given a query Q, the goal is to select one&nbsp;sample at run-time that meet the specified time or error&nbsp;constraints and then compute answers over them. Selecting&nbsp;a sample involves first selecting a sample family (i.e., dimension),&nbsp;and then selecting a sample resolution within that&nbsp;family.</div><h3><a name="TOC-Selecting-the-Sample-Family"></a>Selecting the Sample Family</h3></div></div></div></div></div></div><div>Choosing an appropriate sample family for a query primarily&nbsp;depends on the set of columns used for filtering&nbsp;and/or grouping. The WHERE clause itself may either&nbsp;consist of conjunctive predicates (condition1 AND&nbsp;condition2), disjunctive predicates (condition1 OR&nbsp;condition2) or a combination of the two. where:</div><div><ol><li><b>Queries with Conjunctive Predicates.&nbsp;</b>Consider a query Q whose WHERE clause contains only&nbsp;conjunctive predicates. Let&nbsp;<a href="https://sites.google.com/site/cps516haosproject/project-definition/2.png?attredirects=0" imageanchor="1"><img src="https://sites.google.com/site/cps516haosproject/_/rsrc/1351822284629/project-definition/2.png" border="0"></a>. be the set of columns that appear&nbsp;in these clause predicates.&nbsp;Q has multiple WHERE&nbsp;and/or GROUP BY clauses, then . represents the union of&nbsp;the columns that appear in each of these predicates. If&nbsp;BlinkDB finds one or more stratified sample family on a set&nbsp;of columns .<a href="https://sites.google.com/site/cps516haosproject/project-definition/20.png?attredirects=0" imageanchor="1" style="background-color:rgb(238,232,221)"><img src="https://sites.google.com/site/cps516haosproject/_/rsrc/1351885086608/project-definition/20.png" border="0"></a>such that . .<a href="https://sites.google.com/site/cps516haosproject/project-definition/21.png?attredirects=0" imageanchor="1"><img src="https://sites.google.com/site/cps516haosproject/_/rsrc/1351884812043/project-definition/21.png" border="0"></a>&nbsp;, we simply pick the .<a href="https://sites.google.com/site/cps516haosproject/project-definition/20.png?attredirects=0" imageanchor="1"><img src="https://sites.google.com/site/cps516haosproject/_/rsrc/1351885086608/project-definition/20.png" border="0"></a>&nbsp;with the smallest number of columns, and run the query on SFam(<a href="https://sites.google.com/site/cps516haosproject/project-definition/20.png?attredirects=0" imageanchor="1" style="background-color:rgb(238,232,221)"><img src="https://sites.google.com/site/cps516haosproject/_/rsrc/1351885086608/project-definition/20.png" border="0"></a>).&nbsp;However, if there is no stratified sample on a&nbsp;column set that is a superset of<a href="https://sites.google.com/site/cps516haosproject/project-definition/2.png?attredirects=0" imageanchor="1"><img src="https://sites.google.com/site/cps516haosproject/_/rsrc/1351822284629/project-definition/2.png" border="0"></a>&nbsp;., we run Q in parallel on&nbsp;the smallest sample of all sample families currently maintained&nbsp;by the system.&nbsp;Then, out of these samples we select&nbsp;the one that corresponds to the highest ratio of (i) the number&nbsp;of rows selected by Q, to (ii) the number of rows read&nbsp;by Q.</li><li><b>Queries with Disjunctive Predicates. &nbsp;</b>Consider a query Q with disjunctions in its WHERE&nbsp;clause. In this case, we rewrite Q as a union of queries&nbsp;<a href="https://sites.google.com/site/cps516haosproject/project-definition/22.png?attredirects=0" imageanchor="1"><img src="https://sites.google.com/site/cps516haosproject/_/rsrc/1351885416725/project-definition/22.png" border="0"></a>where each query Q(i) contains only conjunctive&nbsp;predicates.&nbsp;Let .<a href="https://sites.google.com/site/cps516haosproject/project-definition/19.png?attredirects=0" imageanchor="1"><img src="https://sites.google.com/site/cps516haosproject/_/rsrc/1351883988908/project-definition/19.png" border="0"></a>&nbsp;be the set of columns in Q(j) ’s&nbsp;predicates.&nbsp;Then, we associate with every query Q(i) an error&nbsp;constraint or time constraint,&nbsp;such that we can still satisfy Q’s error/time constraints when&nbsp;aggregating the results over Q(i) (1&lt;= . i &lt;= . p) in parallel.&nbsp;</li></ol><h3><a name="TOC-Selecting-the-Sample-Size"></a>Selecting the Sample Size</h3><div>To&nbsp;select&nbsp;an appropriately sized sample in that family based on the&nbsp;query’s response time or error constraints, BlinkDB&nbsp;constructing an Error-Latency Profile (ELP) for the&nbsp;query. The ELP characterizes the rate at which the error decreases&nbsp;with increasing&nbsp;sample sizes, and is built simply by running the query on&nbsp;smaller samples to estimate the selectivity and project latency&nbsp;and error for larger samples.&nbsp;For a distributed query,&nbsp;its runtime scales with sample size, with the scaling rate&nbsp;depending on the exact query structure, physical placement of it’s inputs and the underlying&nbsp;data distribution. As shown in Table 2, the variation&nbsp;of error primarily depends&nbsp;on the variance of the underlying data distribution and the&nbsp;actual number of tuples processed in the sample, which in&nbsp;turn depends on the selectivity of a query’s predicates.</div></div><div><div style="text-align:center;display:block"><a href="https://sites.google.com/site/cps516haosproject/project-definition/23.png?attredirects=0" imageanchor="1"><img src="https://sites.google.com/site/cps516haosproject/_/rsrc/1351886789932/project-definition/23.png?height=81&amp;width=400" height="81" border="0" width="400"></a></div><div style="text-align:left;display:block"><b>Error Profile:</b>&nbsp;An error profile is created for all queries with&nbsp;error constraints. If Q specifies an error&nbsp;constraint, the BlinkDB error profile tries to predict&nbsp;the size of the smallest sample that satisfies Q’s error constraint.</div><div style="text-align:left;display:block"><b>Latency Profile:</b>&nbsp;Similarly, a latency profile is created for all&nbsp;queries with response time constraints. If Q specifies a response&nbsp;time constraint, we select the sample family on which</div><div>to run Q the same way as above.&nbsp;Again, let SFam(<a href="https://sites.google.com/site/cps516haosproject/project-definition/20.png?attredirects=0" imageanchor="1"><img src="https://sites.google.com/site/cps516haosproject/_/rsrc/1351885086608/project-definition/20.png" border="0"></a>),&nbsp;be&nbsp;the selected family and let&nbsp;<a href="https://sites.google.com/site/cps516haosproject/project-definition/24.png?attredirects=0" imageanchor="1"><img src="https://sites.google.com/site/cps516haosproject/_/rsrc/1351887327600/project-definition/24.png" border="0"></a>&nbsp;be the number of rows that&nbsp;Q reads when running on S(.<a href="https://sites.google.com/site/cps516haosproject/project-definition/20.png?attredirects=0" imageanchor="1"><img src="https://sites.google.com/site/cps516haosproject/_/rsrc/1351885086608/project-definition/20.png" border="0"></a>,K(m)).In addition, let n be&nbsp;the maximum number of rows that Q can read without exceeding&nbsp;its response time constraint.</div><div><div>Although BlinkDB requires a query to operate on smaller&nbsp;samples to construct its ELP, the intermediate data produced&nbsp;in the process is effectively utilized when the query runs on</div><div>larger samples.Physically, each progressively bigger&nbsp;logical sample (A, B or C) consists of all data blocks&nbsp;of the smaller samples in the same family. BlinkDB maintains&nbsp;a transparent &nbsp;mapping between logical samples and&nbsp;data blocks, i.e., A maps to (I), B maps to (I, II) and C maps&nbsp;to (I, II, III). Now, consider a query Q on this data. First,&nbsp;BlinkDB creates an ELP for Q by running it on the smallest&nbsp;sample A,it operates on the first two data blocks to estimate&nbsp;various query parameters described above and caches&nbsp;all intermediate data in this process.So if&nbsp;sample&nbsp;C is chosen based on the Q’s error/latency requirements,&nbsp;BlinkDB only operates on the additional data blocks, utilizing&nbsp;the previously cached intermediate data.</div></div></div><h2><a name="TOC-Implementation"></a><span style="font-style:normal">Implementation</span></h2></div><div><span style="font-style:normal"><span>Firstly in this part, the&nbsp;</span></span>entire BlinkDB ecosystem should be given:</div><div><div style="text-align:center;display:block"><a href="https://sites.google.com/site/cps516haosproject/project-definition/25.png?attredirects=0" imageanchor="1"><img src="https://sites.google.com/site/cps516haosproject/_/rsrc/1351887780162/project-definition/25.png" border="0"></a></div><div style="text-align:center;display:block">Figure 2 The BlinkDB ecosystem</div><div style="text-align:left;display:block">Basically&nbsp;BlinkDB is&nbsp;built on top of the Hive Query Engine, supports both&nbsp;Hadoop MapReduce &nbsp;and Spark &nbsp;at &nbsp;the execution layer and uses the Hadoop Distributed File&nbsp;System at the storage layer.</div><div style="text-align:left;display:block"><div>Our implementation required changes in a few key components.&nbsp;We added a shim layer of BlinkDB Query Interface&nbsp;to the HiveQL parser that enables queries with response time&nbsp;and error bounds. Furthermore, it detects data input, which&nbsp;causes the Sample Creation and Maintenance module to create&nbsp;or update the set of random and multi-dimensional samples</div><div>at multiple&nbsp;granularity. BlinkDB&nbsp;further&nbsp;extend the HiveQL parser to implement a Sample Selection&nbsp;module that re-writes the query and iteratively assigns it an&nbsp;appropriately sized biased or random sample.&nbsp;We also added an Uncertainty Propagation module to&nbsp;modify the pre-existing aggregation functions&nbsp;to return errors bars and confidence intervals in&nbsp;addition to the result. Finally, we extended the SQLite based&nbsp;Hive Metastore to create BlinkDB Metastore that maintains&nbsp;a transparent mapping between the non-overlapping logical&nbsp;samples and physical HDFS data blocks.</div><div><div>In BlinkDB, uniform samples are generally created in a few&nbsp;hundred seconds. This is because the time taken to create&nbsp;them only depends on the disk/memory bandwidth and the&nbsp;degree of parallelism. On the other hand, creating stratified&nbsp;samples on a set of columns takes anywhere between a 5--30 minutes depending on the number of unique values to&nbsp;stratify on, which decides the number of reducers and the&nbsp;amount of data shuffled.</div></div><h2><a name="TOC-Evaluation-and-Results"></a><span style="font-style:normal">Evaluation and Results</span></h2><div><div style="font-style:normal">The paper evaluate BlinkDB’s performance on a&nbsp;100 node EC2 cluster using two workloads: a workload from&nbsp;Conviva Inc. and the well-known TPC-H benchmark.Where:</div><div><ul><li style="font-style:normal"><b>Conviva Workload.</b> The Conviva data represents information&nbsp;about video streams viewed by Internet users. We use&nbsp;query traces from their SQL-based ad-hoc querying system&nbsp;which is used for problem diagnosis and data analytics on a&nbsp;log of media accesses by Conviva users. These access logs&nbsp;are 1:7 TB in size and constitute a small fraction of data collected&nbsp;across 30 days. Based on their underlying data distribution,&nbsp;we generated a 17 TB dataset for our experiments&nbsp;and partitioned it across 100 nodes. The data consists of a&nbsp;single large fact table with 104 columns, such as, customer&nbsp;ID, city, media URL, genre, date, time, user OS, browser&nbsp;type, request response time, etc. The 17 TB dataset has about&nbsp;5:5 billion rows.</li><li></li><li><b>TPC-HWorkload.</b> We also ran a smaller number of experiments&nbsp;on TPC-H to demonstrate the generality of our results,&nbsp;with respect to a standard benchmark. All the TPC-H experiments&nbsp;ran on the same 100 node cluster, on 1 TB of data. The 22 benchmark queries in TPC-H&nbsp;were mapped to 6 unique query templates.</li></ul></div><div><div style="font-style:normal">First, we compare BlinkDB to query execution on full-sized&nbsp;datasets to demonstrate how even a small trade-off in the&nbsp;accuracy of final answers can result in orders-of-magnitude&nbsp;improvements in query response times. Second, we evaluate&nbsp;the accuracy and convergence properties of our optimal&nbsp;multi-dimensional, multi-granular stratified-sampling&nbsp;approach against both random sampling and single-column&nbsp;stratified-sampling approaches. Third, we evaluate the effectiveness&nbsp;of our cost models and error projections at meeting&nbsp;the user’s accuracy/response time requirements. Finally,&nbsp;we demonstrate BlinkDB’s ability to scale gracefully with increasing&nbsp;cluster size. And I prefer to put some result graphs from the paper as following:</div><div style="font-style:normal"><div style="text-align:center;display:block"><a href="https://sites.google.com/site/cps516haosproject/project-definition/26.png?attredirects=0" imageanchor="1"><img src="https://sites.google.com/site/cps516haosproject/_/rsrc/1351888634234/project-definition/26.png" border="0"></a></div><div style="display:block;text-align:left"><br></div><div style="text-align:center">Figure 3.Sampling vs. No sampling</div></div><div style="text-align:center"><div style="font-style:normal;text-align:center;display:block"><a href="https://sites.google.com/site/cps516haosproject/project-definition/27.png?attredirects=0" imageanchor="1"><img src="https://sites.google.com/site/cps516haosproject/_/rsrc/1351888818288/project-definition/27.png" border="0"></a></div><div style="font-style:normal;text-align:center;display:block">Figure 4. Error Comparison in Conviva</div><div style="text-align:center;display:block"><div style="font-style:normal;text-align:center;display:block"><a href="https://sites.google.com/site/cps516haosproject/project-definition/28.png?attredirects=0" imageanchor="1"><img src="https://sites.google.com/site/cps516haosproject/_/rsrc/1351888918701/project-definition/28.png" border="0"></a></div><div style="font-style:normal;text-align:center;display:block">Figure 5. Error Convergence in Conviva</div><div style="text-align:center;display:block"><div style="font-style:normal;text-align:center;display:block"><a href="https://sites.google.com/site/cps516haosproject/project-definition/29.png?attredirects=0" imageanchor="1"><img src="https://sites.google.com/site/cps516haosproject/_/rsrc/1351889036745/project-definition/29.png" border="0"></a></div><div style="font-style:normal;text-align:center;display:block">Figure 6. Response Time Bounds</div><div style="text-align:center;display:block"><div style="font-style:normal;text-align:center;display:block"><a href="https://sites.google.com/site/cps516haosproject/project-definition/30.png?attredirects=0" imageanchor="1"><img src="https://sites.google.com/site/cps516haosproject/_/rsrc/1351889141051/project-definition/30.png" border="0"></a></div><div style="font-style:normal;text-align:center;display:block">Figure 7. Scale up</div><h2 style="text-align:left"><a name="TOC-Conclusion"></a><span style="font-style:normal">Conclusion</span></h2><div style="text-align:left"><div>In this paper, we presented BlinkDB, a parallel, sampling-based&nbsp;approximate query engine that provides support for&nbsp;ad-hoc queries with error and response time constraints.&nbsp;BlinkDB is based on two key ideas: (i) a multi-dimensional,&nbsp;multi-granularity sampling strategy that builds and maintains &nbsp;a large variety of samples, and (ii) a run-time dynamic&nbsp;sample selection strategy that uses smaller samples to estimate&nbsp;query selectivity and choose the best samples for satisfying&nbsp;query constraints.</div><div>Basically what I do is learnt the whole structure of BlinkDB and try to do some simulation job based on EC2. Hopefully share something interesting with others.</div></div></div>&nbsp;<span style="font-style:normal">&nbsp;&nbsp; &nbsp;</span>