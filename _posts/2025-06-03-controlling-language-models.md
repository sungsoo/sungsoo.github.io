---
layout: post
title: Controlling Language Models
date: 2025-06-03
categories: [artificial intelligence]
tags: [artificial general intelligence]

---

# [Controlling Language Models–Lisa Li (Stanford)](https://www.youtube.com/watch?v=tEQ9N5JjGW0) 

## Abstract

Controlling language models is key to unlocking their full potential and making them useful for downstream tasks. Successfully deploying these models often requires both task-specific customization and rigorous auditing of their behavior. In this talk, I will begin by introducing a customization method called Prefix-Tuning, which adapts language models by updating only 0.1% of their parameters. Next, I will address the need for robust auditing by presenting a Frank-Wolfe-inspired algorithm for red-teaming language models, which provides a principled framework for discovering diverse failure modes. Finally, I will rethink the root cause of these control challenges, and propose a new generative model for text, called Diffusion-LM, which is controllable by design.

### Bio
Lisa Li is a PhD candidate at Stanford University, where she is advised by Percy Liang and Tatsunori Hashimoto. Her research focuses on developing methods to make language models more capable and controllable. Lisa is supported by the Two Sigma PhD fellowship and Stanford Graduate Fellowship and is the recipient of an EMNLP Best Paper award.

본 영상 강연에서 Lisa Li는 **언어 모델을 제어하는 것**이 얼마나 중요한지 강조하며 강연을 시작합니다. 그녀는 언어 모델을 특정 작업에 효율적으로 적용하는 **파라미터 효율적인 미세 조정** 방법인 **Prefix Tuning**을 설명하고, 원하는 동작에서 벗어나는 경우를 파악하기 위해 **다양한 실패 사례**를 효과적으로 식별하는 **레드 티밍** 기법을 제시합니다. 마지막으로, 그녀는 현재의 자동 회귀 모델의 한계를 극복하고 **본질적으로 제어 가능한** **Diffusion LM**과 같은 **새로운 아키텍처**를 통해 언어 모델의 **구조를 재고**하여 제어 문제를 해결할 수 있다고 제안합니다. 전반적으로 이 발표는 언어 모델의 제어, 평가 및 아키텍처에 대한 혁신적인 연구 방향을 제시합니다.

<iframe width="600" height="400" src="https://www.youtube.com/embed/tEQ9N5JjGW0?si=kO5rBLxiOxj1UlwD" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

---

## 거대 언어 모델 제어 및 평가

거대 언어 모델을 제어하고 평가하는 주요 방법은 다음과 같습니다.

### **거대 언어 모델 제어**

언어 모델은 유용한 제품으로 변화하기 위해 **제어**가 핵심입니다. 언어 모델은 사전 학습된 상태에서 특정 사용 사례에 맞게 제어 및 조정됩니다. 다양한 사용 사례(예: 회사별 챗봇, 데이터 처리, 엔터테인먼트용 역할극 등) 때문에 언어 모델 사용자 정의가 서비스화되었지만, 각 요청에 대해 모델을 파인튜닝하는 것은 매우 비용이 많이 듭니다.

제어를 위한 주요 접근 방식은 다음과 같습니다.

*   **매개변수 효율적인 파인튜닝 (Parameter Efficient Fine-Tuning, PEFT)**: 이 방법은 언어 모델을 조정하는 데 필요한 매개변수 수를 크게 줄여 비용 효율성을 높이고 언어 모델 사용자 정의를 민주화합니다.
    *   PEFT는 **프롬프팅(prompting)**의 효율성(모델 매개변수 업데이트 불필요)과 **파인튜닝(fine-tuning)**의 정밀성(데이터 분포를 잘 맞춤) 사이의 절충점을 찾습니다.
    *   **프리픽스 튜닝 (Prefix Tuning)**은 이러한 PEFT 방법 중 하나입니다. 프롬프팅에서 영감을 받아 이산적인 프롬프트 탐색 공간의 한계를 극복하기 위해 연속적인 자유 매개변수를 도입합니다.
    *   프리픽스 튜닝은 작은 연속적인 작업별 벡터(프리픽스 매개변수)를 최적화하며, 이는 가상 프롬프트 토큰 시퀀스처럼 작동합니다.
    *   이를 통해 전체 모델 매개변수는 고정하고 프리픽스 매개변수만 최적화하여 데이터의 가능도를 최대화합니다.
    *   프리픽스 튜닝은 전체 파인튜닝과 유사한 성능을 달성하면서도 매개변수 효율성을 수천 배 향상시킬 수 있습니다. 또한 학습 분포와 다른 테스트 분포에서도 더 나은 일반화 성능을 보이는 경향이 있습니다.
    *   프리픽스 튜닝 아이디어는 **프롬프트 압축**에도 적용되어, 프롬프트 길이로 인한 추론 지연 및 계산 비용 문제를 완화할 수 있습니다.
    *   프리픽스 튜닝은 PEFT 연구 방향을 개척했으며, **LoRA** 및 **프롬프트 튜닝**과 같은 후속 연구에 영감을 주었고, 현재 언어 모델 사용자 정의의 사실상 표준 방식이 되었습니다.

*   **내재적으로 제어하기 쉬운 모델 아키텍처 재설계**: 기존의 언어 모델은 주로 왼쪽에서 오른쪽으로 토큰을 생성하는 **자기회귀적 (auto-regressive)** 방식이며, 이는 생성 유연성을 제한하고 제어를 어렵게 만드는 구조적 한계입니다. 제어를 두 개의 추가 단계(제어 및 평가)로 다루는 것이 아니라, 애초에 제어가 용이하도록 모델을 재설계하는 것을 고려할 수 있습니다.
    *   **비자기회귀적 언어 모델 (Non-order Regressive Language Model)**은 텍스트의 생성 순서를 다시 생각하여 모든 토큰을 동시에 생성할 수 있도록 설계되었습니다.
    *   이러한 모델은 설계상 제어하기 쉽습니다.
    *   **Diffusion LM**은 텍스트를 위한 최초의 연속적인 확산 모델(diffusion model)입니다. 이 모델은 연속적인 완화(continuous relaxation) 및 반복적인 개선(iterative refinement)이라는 두 가지 원칙을 기반으로 구축되어 제어 가능한 언어 모델을 만듭니다.
    *   Diffusion LM은 연속적인 잠재 공간에서 작동하며, 가우시안 노이즈 벡터 시퀀스를 점진적으로 노이즈 제거하여 단어에 해당하는 벡터로 만들고 어휘 분포로 투영합니다.
    *   Diffusion LM은 **플러그 앤 플레이(plug-and-play)** 추론을 가능하게 합니다. 제어 기준(미분 가능한 스코어링 함수)을 **사후 분포(posterior distribution)**에 조건으로 부여하여 텍스트를 샘플링합니다 ($$P(X | C)$$).
    *   여러 제어 기준을 함께 결합하여 동시에 만족하는 텍스트를 생성할 수 있습니다.

### **거대 언어 모델 평가**

모델에 대한 제어가 성공적인지 확인하기 위해 **평가**가 필요합니다. 이는 모델의 실패 사례를 탐지하는 것을 포함합니다.

평가를 위한 주요 접근 방식은 다음과 같습니다.

*   **레드팀 (Red Teaming)**: 이는 제어를 평가하는 한 가지 방법입니다. 구체적으로, 원치 않는 행동(예: 유해하거나 오해의 소지가 있는 응답)을 유발하는 프롬프트 또는 입력을 검색하는 것입니다.
    *   기존의 연구는 레드팀을 검색 문제(주어진 원치 않는 응답 Y에 대해 Y의 생성을 유발하는 입력 프롬프트 X를 검색, $$P(Y|X)$$ 최대화)로 다루었습니다. 그러나 이는 단 하나의 모드(strategy)만 발견하는 경향이 있습니다.
    *   목표는 단일 전략을 발견하는 것이 아니라 **더 많은 실패 모드를 포괄적으로 탐지**하는 것입니다.
    *   이를 위해 **사후 분포 추정** 접근 방식을 제안합니다. 이는 X가 주어졌을 때 Y의 확률을 찾는 것 대신, Y가 주어졌을 때 X의 사후 분포($$P(X|Y)$$)를 추정하여 다양성을 명시적으로 고려합니다.
    *   사후 분포 추정 문제는 **변분 추론(variational inference)** 기술을 사용하여 해결할 수 있습니다.
    *   사후 분포의 여러 모드를 포괄하기 위해 **반복적인 개선 (Iterative Refinement)** 아이디어에 기반한 알고리즘을 사용합니다. 이 알고리즘은 Frank Wolf 최적화 알고리즘에서 영감을 받았으며, 각 반복에서 새로운 모드/전략을 발견하고 이를 이전 발견과 혼합하여 분포를 구성합니다.
    *   이 방법을 통해 이전 접근 방식을 사용하여 발견된 전략의 대다수를 커버할 수 있음을 보여줍니다.
    *   이러한 **유도(elicitation)** 방법은 언어 모델의 오류를 사전에 검색하여 모델 개발자가 이러한 오류를 수정하도록 안내할 수 있습니다. 또한 이러한 방법은 단순히 실패를 검색하는 것뿐만 아니라 성공을 검색하는 데(예: 가장 좋은 프롬프트 찾기) 사용될 수도 있습니다.
    *   이 방법으로 발견된 프롬프트는 다른 모델로도 일반화될 수 있습니다.

*   **평가 재고**: 정적 벤치마크를 넘어선 평가를 재고하는 연구도 진행되고 있습니다. 제어의 어려움은 모델이 다른 관점('wheels')에서 일관되지 않기 때문일 수 있습니다. 동일한 근본적인 문제에 대한 질문 방식이 다르더라도 모델이 일관되게 작동해야 신뢰성이 향상됩니다.

요약하자면, 언어 모델 제어의 주요 방법은 PEFT 기법(프리픽스 튜닝 등)을 통한 조정과 Diffusion LM과 같은 내재적으로 제어 가능한 새로운 아키텍처 설계이며, 평가는 주로 레드팀과 같은 실패 탐지 기술을 사용하되, 다양성을 높이기 위해 사후 분포 추정 및 반복적 개선 알고리즘을 활용하는 것입니다.

---

## Diffusion LM의 장점

Diffusion LM의 주요 장점은 거대 언어 모델 제어의 어려움을 해결하고 새로운 가능성을 열어준다는 점입니다. Diffusion LM의 핵심 장점은 다음과 같습니다.

*   **설계상 내재적인 제어 용이성**: 기존의 언어 모델은 주로 왼쪽에서 오른쪽으로 토큰을 생성하는 자기회귀적(auto-regressive) 방식이며, 이는 제어를 어렵게 만드는 구조적 한계로 지적됩니다. Diffusion LM은 텍스트 생성 순서를 재고하여 모든 토큰을 동시에 생성할 수 있도록 설계된 **비자기회귀적 언어 모델**입니다. 이러한 모델은 설계상 제어하기 쉽도록 만들어졌습니다. 이는 제어를 위해 추가적인 단계를 할당하는 대신, 모델 자체를 제어하기 용이하게 재설계하는 접근 방식을 따릅니다.
*   **플러그 앤 플레이(Plug-and-Play) 제어 지원**: Diffusion LM은 **연속적인 완화(continuous relaxation)**와 **반복적인 개선(iterative refinement)**이라는 두 가지 원칙을 기반으로 구축되어 제어 가능한 언어 모델을 만듭니다. 특히, 이는 **플러그 앤 플레이 추론**을 가능하게 합니다. 원하는 제어 기준(미분 가능한 스코어링 함수)을 사후 분포(posterior distribution)에 조건으로 부여하여 텍스트를 샘플링할 수 있습니다 (P(X | C)). 이는 모델 생성과 제어 기준을 결합하여 두 가지를 모두 만족하는 텍스트를 생성하도록 유도합니다.
*   **다양한 제어 기준의 동시 적용**: Diffusion LM은 여러 제어 기준을 함께 결합하여 동시에 만족하는 텍스트를 생성할 수 있다는 장점이 있습니다.
*   **성능 향상 및 유연성**: 자기회귀 모델 위에 구축된 미세 조정(fine-tuning) 또는 다른 플러그 앤 플레이 기준선과 비교했을 때, Diffusion LM 접근 방식은 구조화된 구문 제어 문제에서 두 가지 기준선보다 훨씬 뛰어난 성능을 보이며, 다양한 제어의 조합에서도 매우 잘 작동합니다. 반복적인 개선 아이디어 덕분에 다양한 모드를 모델링하는 데도 좋습니다.
*   **향상된 추론 속도 가능성**: 최근 Diffusion LM의 핵심 아이디어에 기반한 스타트업(Inception)이 출범했으며, 이 회사는 자기회귀 모델보다 5~10배 빠른 디코딩 시간을 구현할 수 있다는 점을 주요 경쟁력으로 내세웁니다.
*   **혁신적인 접근 방식**: Diffusion LM은 텍스트를 위한 최초의 연속적인 확산 모델이라는 점에서 혁신적입니다. 이미지 분야에서 널리 사용되는 확산 모델을 이산적인 텍스트 영역에 성공적으로 적용했습니다.

요약하자면, Diffusion LM은 기존 자기회귀 모델의 제어 한계를 극복하기 위해 설계되었으며, **내재적인 제어 용이성**, **유연한 플러그 앤 플레이 제어**, **여러 제어의 효과적인 결합**, 그리고 잠재적으로 **더 빠른 추론 속도** 및 **뛰어난 성능**을 제공하는 것이 주요 장점입니다.

---

## 언어 모델 제어의 중요성

언어 모델을 제어하는 주요 이유는 다음과 같습니다.

1.  **사전 학습된 언어 모델을 유용하고 실용적인 제품 및 서비스로 전환하기 위해서입니다**. ChatGPT가 갑자기 인기를 얻은 이유는 사전 학습된 언어 모델을 마침내 **유용한 작업**을 수행하도록 제어할 수 있게 되었기 때문입니다.
2.  **특정 작업 또는 사용 사례에 맞게 모델을 맞춤화하고 조정하기 위해서입니다**. 언어 모델은 다양한 사용 사례를 가지고 있으며, 많은 회사들이 특정 비즈니스 목적을 위해 모델을 맞춤화하고 있습니다.
    *   **회사별 챗봇 구축**.
    *   **특정 목적의 데이터 처리**.
    *   **새로운 형태의 엔터테인먼트 (예: 특정 역사적 또는 가상의 인물 역할극)**.
    *   **개인화** (예: 사용자의 글쓰기 스타일 모방).
    *   **도메인 적응**.
    *   **엣지 디바이스와 같은 특정 환경에서 작업을 수행하기 위한 소규모 모델 특화**.
3.  **모델의 생성 출력이 원하는 특정 기준, 형식 또는 제약 조건을 충족하도록 보장하기 위해서입니다**.
    *   **코드 자동 완성**과 같이 특정 도메인에 맞춰 생성.
    *   **검색 질의에 대한 요약** 생성.
    *   **JSON 형식**과 같이 특정 출력 형식 준수.
    *   **유효한 수학적 증명** 생성과 같이 특정 논리적/수학적 기준 충족.
    *   **안전하고 유해하지 않은 답변**을 생성하고, 유해한 행동(실패 사례)을 유발하는 입력을 식별하여 모델을 개선하기 위해 **레드팀(red teaming)** 목적 달성.
4.  **기존 자기회귀 모델의 구조적 한계(주로 왼쪽에서 오른쪽으로 생성)로 인해 제어가 어려운 문제를 해결하고, 더 유연한 생성을 가능하게 하기 위해서입니다**. 텍스트 생성 순서를 재고하고 모든 토큰을 동시에 생성할 수 있는 **비자기회귀적(non-order regressive) 모델**을 구축하는 것은 제어를 **설계상 용이하게** 만들기 위한 시도입니다.
5.  **언어 모델의 행동을 더 예측 가능하고 신뢰할 수 있게 만들기 위해서입니다**. 제어 및 일관성 확보는 모델의 신뢰성을 높이는 핵심 요소입니다.
6.  **모델 맞춤화에 필요한 비용과 자원을 절감하기 위해서입니다**. 다양한 사용 사례에 대해 매일 수천 건의 맞춤화 요청이 있을 때, 각 요청마다 전체 모델을 파인튜닝하고 저장하는 것은 매우 비싸기 때문에, 접두사 튜닝(Prefix Tuning)과 같이 **매개변수 효율적인(parameter efficient)** 방식으로 모델을 조정하는 것이 중요합니다.

요약하자면, 언어 모델 제어는 강력한 사전 학습 모델을 다양한 **실제 응용 프로그램에 적용**하고, **특정 요구사항을 충족**하는 출력을 생성하며, 기존 모델의 **제한을 극복**하고, 모델의 **신뢰성과 유용성을 향상**시키기 위한 필수적인 과정입니다.

---

## 접두사 튜닝(Prefix Tuning)

**접두사 튜닝(Prefix Tuning)**은 언어 모델을 제어하고 특정 작업 또는 사용 사례에 맞게 **조정(adapt)**하는 방법론입니다. 이는 언어 모델을 맞춤화하는 데 있어 **매개변수 효율성(parameter efficiency)**을 혁신적으로 개선한 방법으로 평가됩니다.

접두사 튜닝에 대한 자세한 내용은 다음과 같습니다.

1.  **등장 배경 및 목표**:
    *   언어 모델은 다양한 사용 사례를 가지며, 많은 회사들이 특정 목적(예: 회사별 챗봇, 특수 데이터 처리, 개인화된 글쓰기 도구, 엣지 디바이스 특화 등)을 위해 모델을 맞춤화하고 있습니다.
    *   모델을 맞춤화하는 일반적인 방법 중 하나는 전체 **파인튜닝(fine-tuning)**인데, 각 사용 사례마다 모델 전체(수십억 개의 매개변수)를 조정하고 저장하는 것은 **매우 비용이 많이 듭니다**. 하루에 수천 건의 맞춤화 요청이 올 경우 이러한 방식은 비효율적입니다.
    *   다른 방법인 **프롬프팅(prompting)**은 효율적이지만, 모델 매개변수를 업데이트하지 않아 **정밀성이 부족**하고 미묘한 스타일이나 복잡한 규칙을 포착하는 데 어려움이 있습니다. 또한, 상세한 지침과 예시가 포함되면 프롬프트가 매우 길어지고, 이는 추론 지연 및 계산 비용 증가로 이어집니다.
    *   접두사 튜닝의 목표는 **더 적은 매개변수**로 언어 모델을 조정하면서도 **작업 성능의 손실 없이** 파인튜닝과 유사한 성능을 달성하는 것입니다.

2.  **작동 방식**:
    *   접두사 튜닝은 프롬프팅에서 영감을 받았습니다. 기존 프롬프팅의 **이산적인 탐색 공간(discrete search space)**이 표현력을 제한하고 최적화를 어렵게 만든다는 점을 관찰했습니다.
    *   이를 해결하기 위해 **연속적인 자유 매개변수(continuous free parameters)**를 도입합니다. 작업별로 작고 **연속적인 벡터**를 최적화하는데, 이를 **접두사 매개변수(prefix parameter)**라고 부릅니다 (소스에서는 분홍색 H로 표시).
    *   이 접두사 매개변수는 마치 **가상의 프롬프트 토큰(virtual prompt tokens)** 시퀀스처럼 작동합니다.
    *   핵심은 언어 모델의 **원래 매개변수(θ)는 고정**하고, **오직 접두사 매개변수(H)만 최적화**하여 데이터의 가능도(likelihood)를 최대화하는 것입니다.
    *   접두사 튜닝은 탐색 공간이 연속적이므로 **매우 표현력이 풍부**하고, 그래디언트 디센트(gradient descent)와 같은 도구를 활용하여 **최적화하기 매우 쉽습니다**.

3.  **주요 이점**:
    *   **매개변수 효율성**: 전체 모델 매개변수는 고정하고 소수의 접두사 매개변수만 조정하여, 전체 파인튜닝에 비해 **매개변수 효율성을 수천 배** 향상시킬 수 있습니다.
    *   **비용 및 자원 절감**: 매개변수 효율성 덕분에 모델 사용자 정의에 필요한 **학습 및 저장 비용**을 크게 절감할 수 있으며, 이는 적은 자원을 가진 사람들도 모델을 쉽게 조정할 수 있도록 **민주화**에 기여합니다.
    *   **파인튜닝에 준하는 성능**: 적은 매개변수만 조정함에도 불구하고 **전체 파인튜닝과 유사하거나 뛰어난 성능**을 달성할 수 있습니다.
    *   **뛰어난 분포 외 일반화 성능 (Out-of-Distribution Generalization)**: 학습 분포와 다른 테스트 분포에서도 **더 나은 일반화 성능**을 보이는 경향이 있습니다. 이는 일반적인 목적의 사전 학습된 모델 매개변수를 보존하기 때문에 가능합니다.
    *   **최적화 용이성 및 표현력**: 연속적인 매개변수를 사용함으로써 이산적인 프롬프트 탐색 공간의 한계를 극복하고, **매우 표현력이 풍부**하며 **최적화하기 쉬운** 공간에서 모델을 조정할 수 있습니다.
    *   **긴 프롬프트 압축**: 접두사 튜닝의 아이디어는 긴 프롬프트를 **접두사 매개변수 공간으로 매핑**하여 효율적으로 **압축**하는 데도 적용될 수 있습니다. 소스에서는 프롬프트 길이를 **25배 압축**하고도 지시 사항 수행 성능을 유지할 수 있음을 언급합니다.
    *   **PEFT 분야 개척 및 표준화**: 접두사 튜닝은 **매개변수 효율적인 파인튜닝(Parameter Efficient Fine-Tuning, PEFT)** 연구 방향을 개척했으며, LoRA 및 프롬프트 튜닝과 같은 후속 연구에 영감을 주었습니다. 현재 OpenAI, Anthropic, Google, Nvidia 등 여러 회사에서 언어 모델 사용자 정의의 **사실상 표준(de facto way)**으로 널리 사용되고 있습니다.

요약하자면, 접두사 튜닝은 언어 모델의 기존 파인튜닝 및 프롬프팅 방식의 단점을 보완하여, **매개변수 효율성을 극대화**하면서도 **뛰어난 성능과 유연성**을 제공하는 혁신적인 모델 조정 기법입니다.

---

## 접두사 튜닝의 주요 이점

**접두사 튜닝(Prefix Tuning)**의 주요 이점은 다음과 같습니다.

*   **매개변수 효율성 (Parameter Efficiency)**: 접두사 튜닝은 언어 모델을 특정 작업이나 사용 사례에 맞게 **조정(adapt)**하는 데 필요한 매개변수 수를 **크게 줄입니다**. 전체 모델 매개변수는 고정하고, 작업별로 작은 **연속적인 벡터(접두사 매개변수)**만 최적화합니다. 이로 인해 전체 파인튜닝에 비해 **수천 배의 매개변수 효율성 향상**을 달성할 수 있습니다.
*   **비용 및 자원 절감**: 매개변수 효율성이 뛰어나므로, 각 요청에 대해 모델을 전체 파인튜닝하고 저장하는 데 드는 **높은 비용**을 크게 절감할 수 있습니다. 이는 언어 모델 사용자 정의를 **민주화**하여 적은 자원을 가진 사람들도 모델을 쉽게 조정할 수 있게 합니다.
*   **파인튜닝과 유사한 성능 달성**: 적은 매개변수만 조정함에도 불구하고 **전체 파인튜닝과 유사한 성능**을 달성할 수 있습니다.
*   **뛰어난 분포 외 일반화 성능 (Out-of-Distribution Generalization)**: 학습 분포와 다른 테스트 분포에서도 **더 나은 일반화 성능**을 보이는 경향이 있습니다. 이는 일반적인 목적의 사전 학습된 모델 매개변수를 보존하기 때문에 가능한 이점입니다.
*   **최적화 용이성 및 표현력**: 프롬프트의 **이산적인 탐색 공간**의 한계를 극복하기 위해 **연속적인 자유 매개변수**를 도입합니다. 이를 통해 탐색 공간이 **매우 표현력이 풍부**해지고, 그래디언트 디센트와 같은 도구를 활용하여 **최적화하기 매우 쉬워집니다**.
*   **긴 프롬프트 압축에 적용 가능**: 접두사 튜닝의 아이디어는 **프롬프트 압축**에도 적용될 수 있습니다. 긴 프롬프트는 추론 지연 및 계산 비용을 증가시키는데, 프롬프트를 접두사 매개변수 공간으로 매핑하여 **프롬프트 길이를 효과적으로 압축**하고 이러한 문제를 완화할 수 있습니다. 소스에서는 프롬프트를 25배 압축하고도 지시 사항 수행 성능을 유지할 수 있음을 언급합니다.
*   **PEFT 분야 개척 및 표준화**: 접두사 튜닝은 **매개변수 효율적인 파인튜닝(PEFT)** 연구 방향을 개척했습니다. **LoRA** 및 **프롬프트 튜닝**과 같은 후속 연구에 영감을 주었으며, 현재 언어 모델 사용자 정의의 **사실상 표준(de facto way)**이 되어 OpenAI, Anthropic, Google, Nvidia 등 여러 회사에서 널리 사용되고 있습니다.

요약하자면, 접두사 튜닝은 언어 모델을 제어하는 데 있어 **매개변수 효율성, 비용 절감, 파인튜닝에 준하는 성능, 뛰어난 일반화 능력, 쉬운 최적화, 프롬프트 압축 능력** 등의 다양한 이점을 제공하며, PEFT 분야를 선도하고 현재 업계 표준으로 자리 잡는 데 기여했습니다.

---

## 레드팀(Red teaming)의 목적

**레드팀(Red teaming)**의 주요 목적은 다음과 같습니다.

*   **언어 모델 제어의 성공 여부를 평가하기 위해 실패 사례를 탐지하는 것**입니다. 언어 모델을 유용한 제품 및 서비스로 변환하기 위해 제어하는 과정에서 모델이 의도하지 않거나 유해한 방식으로 행동하는 '제어 위반(control violation)'이 발생할 수 있습니다. 레드팀은 이러한 실패를 **찾아내는 방법**입니다.
*   **모델이 원치 않는 행동을 유발하는 입력(프롬프트 X)을 검색하는 것**입니다. 특정 원치 않는 응답(Y)이 주어졌을 때, 그 응답을 유발할 확률이 높은 입력 X를 찾는 문제로 공식화될 수 있습니다 (P(Y|X) 최대화). 이는 "언어 모델을 탈옥(jailbreaking)하는 것"으로도 불립니다.
*   궁극적으로는 레드팀을 통해 **발견된 실패 모드나 전략을 활용하여 언어 모델을 개선하는 것**입니다. 예를 들어, 레드팀을 통해 모델을 무너뜨릴 수 있는 특정 입력 전략이 발견되면, 모델 개발자는 이러한 전략에 대해 모델이 강건해지도록 **데이터를 보강하거나(data augmentation)**, **발견된 오류를 수정(patch errors)**할 수 있습니다.
*   **다양한 실패 모드를 가능한 많이 포괄적으로 발견하는 것**입니다. 기존의 레드팀 방식은 단 하나의 입력 스트링을 찾아내는 데 집중했지만, 소스에서 제시하는 접근 방식은 원치 않는 행동을 유발할 수 있는 **다양한 프롬프트의 분포를 파악**함으로써 보다 포괄적인 실패 사례를 발견하는 것을 목표로 합니다.
*   결과적으로, 모델의 오류를 **선제적으로 탐색**하고 이를 모델 개발에 반영함으로써, **모델 개발을 위한 건전한 생태계를 조성**하는 데 기여하는 것입니다.

요약하자면, 레드팀은 언어 모델이 **안전하고 신뢰할 수 있게 작동하도록 보장**하기 위해, 모델의 **취약점과 실패 사례(특히 유해하거나 의도치 않은 행동을 유발하는 입력)**를 **포괄적으로 탐지**하고, 이를 바탕으로 모델을 **개선**하기 위한 필수적인 과정입니다.

---

## 제어 기준(control criteria)

**제어 기준(control criteria)**은 언어 모델의 생성 출력이 만족해야 하는 **특정 조건, 제약 또는 목표**를 의미합니다.

이 기준들은 언어 모델 자체(텍스트의 유창성을 담당)와는 별개로 존재하며, 모델이 생성하는 텍스트를 **원하는 방향으로 조종(steer)**하거나 **특정 요구사항을 충족**하도록 유도하는 역할을 합니다. 언어 모델을 유용하고 실용적인 제품 및 서비스(예: 코드 자동 완성, 검색 결과 요약 등)로 변환하기 위한 "제어" 과정의 핵심 요소입니다.

소스에서 언급된 제어 기준의 예시는 다양합니다:

*   생성될 텍스트의 **접두사(prefix)** 또는 **접미사(suffix)**에 대한 제약.
    *   예를 들어, 특정 프롬프트에 응답하거나 레드팀(Red teaming)과 같이 특정 "원치 않는 응답 Y" (접미사)를 유발하는 입력 X (접두사)를 찾는 경우.
*   생성될 텍스트의 **형식**에 대한 제약 (예: 정확히 JSON 형식이어야 함).
*   생성될 텍스트가 특정 **논리적 또는 수학적 정확성**을 가져야 한다는 제약 (예: 유효한 수학적 증명).
*   생성될 텍스트의 **구문(syntax)** 또는 **의미(semantics)**적 특성에 대한 제약.
*   모델이 **유해하거나 의도치 않은 행동(unwanted behaviors)**을 하지 않도록 하는 안전성 제약 또한 제어 기준의 일환으로 볼 수 있습니다.

이러한 제어 기준은 특히 "플러그 앤 플레이(plug-and-play)" 방식의 언어 모델(예: Diffusion LM)에서는 핵심 메커니즘으로 작동합니다. 이 프레임워크에서는 제어 기준을 다음과 같은 형태로 모델에 **"플러그인"**할 수 있습니다:

*   **분류기(classifier)**.
*   **검증기(verifier)** (예: 수학 검증기).
*   생성 과정의 중간 표현(예: 연속적인 벡터 공간)에 대해 **미분 가능한 점수 함수(differentiable scoring function)**.

이 점수 함수는 생성 과정(예: Diffusion LM의 반복적 개선 단계)에서 **그래디언트 신호(gradient signal)**로 사용되어 모델의 출력을 제어 기준이 원하는 방향으로 **조종(steer)**합니다. 이러한 방식을 통해 언어 모델의 행동을 더 예측 가능하고 신뢰할 수 있게 만들 수 있습니다.

여러 개의 제어 기준을 **조합(compose)**하여 동시에 적용하는 것도 가능하며, 이때 각 기준은 미분 가능해야 합니다.

결과적으로, 제어 기준은 강력한 사전 학습 언어 모델을 다양한 **실제 응용 프로그램에 적용**하고, **특정 요구사항을 충족**하는 출력을 생성하며, 모델의 **신뢰성과 유용성을 향상**시키기 위한 필수적인 요소입니다. 이는 제어 기준에 **조건화된 사후 분포(posterior distribution)**에서 샘플링하는 것으로 수학적으로 공식화될 수 있습니다.


---

## 확산 언어 모델(Diffusion LM)

**확산 언어 모델(Diffusion LM)**은 텍스트 생성을 위한 새로운 패러다임의 생성 모델입니다.

확산 언어 모델에 대한 주요 내용은 다음과 같습니다.

1.  **정의 및 기본 원리**:
    *   확산 언어 모델은 주로 **연속적인 잠재 공간**에서 작동하는 텍스트 생성 모델입니다.
    *   기존의 대부분의 언어 모델이 왼쪽에서 오른쪽으로 토큰을 하나씩 생성하는 **자기회귀(auto-regressive)** 방식인 것과 달리, 확산 언어 모델은 **비자기회귀(non-order regressive)** 방식입니다. 이는 각 시간 단계에서 전체 텍스트 시퀀스를 나타내는 벡터 표현을 사용한다는 것을 의미합니다.
    *   생성 과정은 **가우시안 노이즈** 시퀀스에서 시작하여, 이를 점진적으로 노이즈 제거하여(denoise) 단어에 해당하는 벡터로 변환하고, 최종적으로 이 벡터들을 어휘에 대한 낮은 엔트로피 분포로 투영하여 단어를 얻습니다.
    *   반복적인 노이즈 제거 과정을 통해, 초기 확산 단계에서는 높은 수준의 의미론이나 구문과 같은 **거친 내용(coarse-grain content)**을 생성하고, 후반 단계에서는 세부적인 단어 선택과 같은 **미세한 내용(finer-grain content)**을 생성합니다.

2.  **작동 방식 및 도전 과제**:
    *   확산 모델은 잠재 변수 모델이며, 단어의 불연속적인 시퀀스를 임베딩 조회 테이블을 사용하여 연속적인 벡터 공간으로 임베딩함으로써 연속적인 잠재 변수를 구성합니다.
    *   이러한 잠재 변수의 계층 구조는 가우시안 확산 과정을 따릅니다.
    *   모델은 노이즈가 있는 단계($$X_T$$)를 입력받아 이전 단계($$X_{T-1}$$) 또는 최종 노이즈가 제거된 단계($$X_n$$)를 예측하는 **노이즈 제거 모델($$\mu \theta$$)**을 훈련합니다.
    *   생성 시에는 가우시안 노이즈에서 시작하여 mu theta 모델을 반복적으로 적용하여 벡터의 노이즈를 제거하고, 최종적으로 얻은 연속적인 벡터($$X_n$$)를 가장 가까운 단어 임베딩으로 반올림하여(rounding) 불연속적인 단어 시퀀스를 얻습니다.
    *   하지만 언어는 본질적으로 불연속적이기 때문에, 연속 공간에서 텍스트를 모델링하는 것은 높은 정밀도를 요구하며, 예측된 벡터가 두 단어 임베딩 사이에 위치할 때 문제가 발생합니다. 이를 **반올림 오류(rounding error)**라고 부르며, 예를 들어 "break the glass" 대신 "rest the glass"와 같이 임베딩 공간에서는 가깝지만 문맥상 교체 불가능한 단어에서 발생할 수 있습니다. 이러한 사소한 정밀도 오류는 반복 과정에서 누적될 수 있습니다.

3.  **도전 과제 해결 방안**:
    *   반올림 오류 문제를 해결하기 위해 두 가지 방법이 제시됩니다:
        *   **재매개변수화(Reparameterization)**: 모델이 항상 최종 결과인 Xn not을 직접 예측하도록 하여, 출력 공간을 단어 임베딩과 일치시키고 훈련 및 예측의 정밀도를 높입니다.
        *   **디코딩 시 클램핑(Clamping trick at decoding time)**: 예측된 Xn not이 실제 단어 임베딩과 일치하지 않을 경우, 이를 가장 가까운 단어 임베딩으로 **클램핑(강제 조정)**합니다. 이는 정밀도 오류가 누적되는 것을 방지하여 디코딩 안정성을 보장합니다.

4.  **제어 용이성 (Controllable by Design)**:
    *   확산 언어 모델은 설계상 **제어가 용이하다는 장점**이 있습니다. 기존의 자기회귀 모델이 생성 순서의 제약 때문에 제어가 어려운 반면, 확산 언어 모델은 플러그 앤 플레이(plug-and-play) 방식으로 제어 기준을 통합할 수 있습니다.
    *   사용자는 원하는 **제어 기준(control criteria)**을 모델에 **플러그인**할 수 있습니다. 이 기준은 분류기, 검증기 또는 생성 과정의 중간 표현($$X_T$$)에 대해 **미분 가능한 점수 함수(differentiable scoring function)** 형태여야 합니다.
    *   제어는 Long dynamics와 같은 방법을 통해 이루어지며, 각 노이즈 제거 단계에서 모델의 유창성(Diffusion LM)과 제어 기준 만족도(differentiable scoring function)를 모두 개선하는 **그래디언트 방향(gradient direction)**으로 벡터를 업데이트하고, 샘플링을 위한 약간의 노이즈를 추가하여, 모델의 생성을 제어 기준이 원하는 방향으로 **조종(steer)**합니다. 여러 개의 미분 가능한 제어 기준을 **조합(compose)**하여 동시에 적용하는 것도 가능합니다.

5.  **주요 성과 및 영향**:
    *   확산 언어 모델은 구조화된 구문 제어 및 여러 제어 기준의 조합 적용 등 다양한 제어 문제에서 다른 기준선 모델들보다 **우수한 성능**을 보였습니다.
    *   이는 **최초의 연속적 확산 언어 모델**로, DeepMind를 포함한 학계 및 산업계에 영향을 미쳤으며, 이를 기반으로 한 다양한 제어 가능한 텍스트 생성 및 다른 불연속적 양식(예: 단백질, 3D 분자)을 위한 모델 개발이 이루어졌습니다.
    *   또한, 확산 언어 모델은 자기회귀 모델보다 **5배에서 10배 빠른 디코딩 시간**을 제공할 수 있는 잠재력이 있다고 언급됩니다.
    *   확산 모델의 **반복적인 개선 구조**는 언어 모델의 **일관성(consistency)** 문제를 해결하고 모델의 신뢰도를 높이는 데 잠재적으로 기여할 수 있습니다.

요약하자면, 확산 언어 모델은 연속 공간에서의 작동, 비자기회귀 방식, 그리고 제어 기준을 직접 통합할 수 있는 설계 덕분에 기존 모델의 제약에서 벗어나 **보다 유연하고 강력한 제어 기능**을 제공하는 텍스트 생성 모델입니다.

---

## 확산 모델을 언어에 사용하는 이유

**확산 모델을 언어에 사용하는 주된 이유는 기존 언어 모델의 구조적인 제약에서 벗어나 모델의 통제(Control)를 더 용이하게 만들기 위함**입니다.

구체적인 이유는 다음과 같습니다.

1.  **내재적인 제어 용이성(Controllable by Design)**:
    *   대부분의 기존 언어 모델은 텍스트를 왼쪽에서 오른쪽으로 토큰 하나씩 생성하는 **자기회귀(auto-regressive)** 방식입니다. 이 방식은 생성 순서가 고정되어 있기 때문에 특정 제약(예: 구문 구조, 특정 단어 포함 등)을 만족하도록 모델의 생성을 통제하기가 어렵습니다. 생성 순서를 깨는 작업은 현재 언어 모델에게 매우 어려운 문제로 언급됩니다.
    *   확산 언어 모델(Diffusion LM)은 이러한 제약을 해결하기 위해 설계된 **비자기회귀(non-order regressive)** 방식의 모델입니다. 각 시간 단계에서 전체 텍스트 시퀀스를 벡터 표현으로 다루며, 모든 토큰을 동시에 생성할 수 있습니다.
    *   이러한 설계 덕분에 확산 언어 모델은 **설계 단계부터 제어가 용이**합니다. 원하는 제어 기준(control criteria)을 모델에 **플러그 앤 플레이(plug-and-play)** 방식으로 통합할 수 있습니다.
    *   이는 미분 가능한 점수 함수 형태의 제어 기준(분류기, 검증기 등)을 사용하여, 각 노이즈 제거 단계에서 모델의 유창성과 제어 기준 만족도 모두를 개선하는 방향(그래디언트 방향)으로 생성을 조종(steer)하는 방식으로 이루어집니다. 여러 제어 기준을 조합하여 동시에 적용하는 것도 가능합니다.
    *   자기회귀 모델 위에 제어 기준을 추가하는 기존 방식보다 구조화된 구문 제어 등 다양한 제어 문제에서 더 우수한 성능을 보여주었습니다.

2.  **두 가지 핵심 아이디어의 결합**:
    *   확산 모델은 언어 모델의 통제력 강화를 위한 두 가지 핵심 아이디어인 **연속적인 완화(continuous relaxation)**와 **반복적인 개선(iterative refinement)**을 자연스럽게 결합합니다.
    *   **연속적인 완화**는 프롬프트 튜닝(prefix tuning)에서 아이디어를 얻었으며, 연속적인 매개변수 공간이 이산적인 공간보다 표현력이 풍부하고 최적화하기 쉽다는 장점이 있습니다. 확산 모델은 텍스트를 임베딩을 통해 연속적인 잠재 공간으로 매핑하여 작동합니다.
    *   **반복적인 개선**은 통제 실패 감지를 위한 레드팀 기법에서 아이디어를 얻었으며, 여러 실패 모드를 포괄적으로 탐색하는 데 유용합니다. 확산 모델은 노이즈를 점진적으로 제거하는 반복 과정을 통해 텍스트를 생성합니다.
    *   이 두 가지 기둥 위에 구축된 확산 언어 모델은 모델을 더 제어하기 쉽게 만들고(steering advantage) 더 다양한 모드를 표현할 수 있게 합니다(iterative advantage).

3.  **성능 및 효율성의 잠재력**:
    *   확산 언어 모델은 구조화된 구문 제어 및 제어 기준 조합 적용 등에서 기존 자기회귀 모델 기반 기준선보다 **우수한 성능**을 달성했습니다.
    *   또한, 확산 언어 모델은 자기회귀 모델보다 **5배에서 10배 빠른 디코딩 시간**을 제공할 수 있는 잠재력이 있다고 언급됩니다.
    *   확산 모델의 반복적인 개선 구조는 언어 모델의 **일관성(consistency)** 문제를 해결하고 신뢰도를 높이는 데 잠재적으로 기여할 수 있다고 제시됩니다.

확산 모델을 언어에 적용하는 데에는 언어가 본질적으로 불연속적이라는 점 때문에 어려움이 있었습니다. 연속 공간에서의 모델링은 높은 정밀도를 요구하며, 예측된 벡터가 실제 단어 임베딩과 정확히 일치하지 않을 때 반올림 오류(rounding error)가 발생할 수 있습니다. 이러한 문제를 해결하기 위해 모델이 항상 최종 노이즈가 제거된 벡터를 예측하도록 재매개변수화하거나, 디코딩 시 예측된 벡터를 가장 가까운 단어 임베딩으로 강제 조정(clamping trick)하는 방법 등이 적용되었습니다.

요약하면, 확산 모델은 기존 언어 모델의 자기회귀적 특성으로 인한 제어의 어려움을 극복하고, 연속적인 공간에서의 작동과 반복적인 개선을 통해 **본질적으로 제어가 용이한 새로운 텍스트 생성 패러다임을 제공**하기 때문에 언어 모델에 사용됩니다.

---

## 통제 실패(control failure) 감지 방법

언어 모델의 **통제 실패(control failure)**를 감지하는 주요 방법은 **레드팀(Red Teaming)**입니다.

통제 실패 감지에 대한 구체적인 내용은 다음과 같습니다.

*   **통제 실패의 정의**: 언어 모델을 특정 사용 사례에 맞게 통제하고 적용한 후, 이 통제가 성공적인지 평가해야 합니다. 통제 평가는 모델이 의도치 않은 행동을 하거나, 잘못되거나 유해한 정보를 생성하는 경우와 같은 **실패 사례(failure cases)**를 탐지하는 것을 목표로 합니다. 예를 들어, 가위 들고 뛰기의 건강상 이점을 물었을 때 위험을 무시하고 면역력 향상 같은 장점만 이야기하거나, 폭탄 제조법 같은 유해한 질문에 상세한 지침을 제공하는 것이 통제 실패에 해당합니다.
*   **레드팀(Red Teaming)**: 이러한 통제 실패를 감지하는 **하나의 방법**입니다. 레드팀의 구체적인 목표는 모델의 **원치 않는 행동(unwanted behaviors)**을 유발하는 프롬프트(입력 X)를 찾는 것입니다. 즉, 주어진 "원치 않는 응답 Y"에 대해, 해당 응답을 생성하게 만드는 입력 프롬프트 X를 검색하는 문제입니다. 이를 모델의 목표 언어 모델 Pθ 하에서 P(Y|X)의 확률을 최대화하는 X를 찾는 것으로 공식화할 수 있습니다.
*   **기존 레드팀 방식의 한계**: 기존의 레드팀은 입력 토큰 공간에서 좌표 상승(coordinate ascent)과 같은 검색 메커니즘을 사용하여 P(Y|X)를 최대화하는 *하나의* 문자열(프롬프트)을 찾으려고 시도했습니다. 그러나 가능한 입력의 검색 공간은 기하급수적으로 크며, 이러한 검색 메커니즘은 Y를 유발할 수 있는 여러 가능한 방식(modes) 중 **하나만 발견**한다는 한계가 있습니다. 포괄적인 평가를 위해서는 더 많은 실패 사례(다양한 X)를 포괄하는 것이 중요합니다.
*   **새로운 통제 실패 감지 방법**: 더 나은 커버리지를 달성하기 위해, 문제를 단일 문자열을 찾는 것에서 **더 많은 지원(support)을 포괄할 수 있는 문자열의 분포를 찾는 것**으로 문제 정의를 변경했습니다.
    *   이를 **사후 추론(posterior inference)** 문제로 공식화했습니다. 목표는 주어진 원치 않는 응답 Y에 대한 입력 X의 사후 분포 P(X|Y)를 추정하는 것입니다.
    *   이 사후 분포 추정은 정규화 상수의 다루기 어려움 등 도전 과제가 있습니다.
    *   이를 해결하기 위해 고전 통계학의 **변분 추론(variational inference)** 기법과 **반복적인 분해(iterative decomposition)** 아이디어(Frank Wolf 알고리즘에서 영감)를 사용했습니다.
    *   반복적인 알고리즘은 각 반복에서 하나의 모드(전략)를 발견하고, 이전 반복에서 발견된 모드에 불이익을 주어 새로운 모드 발견을 장려합니다. 이를 통해 다양한 프롬프트 전략(예: 반복, 이어 쓰기 유도, 고수준 요약 또는 출처 인용 등)을 발견할 수 있습니다.
    *   최종 결과는 각 반복에서 얻은 분포를 혼합한 **분포의 혼합(mixture of distribution)**으로 표현됩니다.
*   **성과**: 이 새로운 방법은 기존의 감독 학습 미세 조정(supervised fine-tuning) 및 강화 학습(reinforcement learning) 기준선보다 더 높은 유도 보상(elicitation reward)을 달성하며 우수한 성능을 보였습니다. 특히, 기존 접근 방식들로 발견된 대다수의 공격 전략을 이 방법으로 복구할 수 있음을 보여주었습니다. 이를 통해 Llama 8b 모델에서 공격 성공률을 2%에서 100%로 향상시켰으며, 발견된 프롬프트가 7TB 모델 및 GPT40, Claude 3.5와 같은 상용 모델에도 일반화됨을 확인했습니다.
*   **활용**: 이러한 실패 사례(원치 않는 행동을 유발하는 프롬프트/전략)를 감지하는 것은 모델 개발자에게 **어떤 오류를 수정해야 하는지 안내**하는 데 사용될 수 있습니다. 발견된 전략을 **데이터 증강(data augmentation)**에 활용하여 모델이 해당 유형의 공격에 대해 더욱 강건해지도록 훈련할 수 있습니다. 또한, 이 기술은 실패를 검색하는 대신 성공(바람직한 응답)을 검색하는 문제에도 적용될 수 있으며, 이는 모델 성능 향상을 위한 최적의 프롬프트 전략을 찾는 데 사용될 수 있습니다. 다양한 유형의 통제 실패(예: 저작권 침해)에 대해서도, 해당 실패를 보상 함수로 정의할 수 있다면 이 방법을 적용하여 관련 전략을 발견할 수 있습니다.

요약하자면, 통제 실패 감지의 핵심 방법은 **레드팀**이며, 소스에서는 다양한 실패 사례를 포괄적으로 탐지하기 위해 **사후 추론 기반의 반복적인 알고리즘**을 활용하는 새로운 접근 방식을 제시하고 있습니다.