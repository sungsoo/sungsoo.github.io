---
layout: post
title: Proximal Policy Optimization (PPO) for LLMs Explained Intuitively
date: 2025-07-27
categories: [artificial intelligence]
tags: [artificial general intelligence]

---

# [Proximal Policy Optimization (PPO) for LLMs Explained Intuitively](https://www.youtube.com/watch?v=8jtAzxUwDj0)


## Abstract

이 영상에서는 강화 학습에 대한 사전 지식 없이도 근접 정책 최적화(PPO)를 기본 원리부터 자세히 살펴봅니다. 영상을 마치면 PPO로 이어지는 핵심 강화 학습 구성 요소를 이해하게 될 것입니다. 여기에는 다음이 포함됩니다.
🔵 정책 경사
🔵 행위자-비판자 모델
🔵 가치 함수
🔵 일반화 이점 추정

LLM(Large Language Model) 환경에서 PPO는 OpenAI의 o1/o3, 그리고 아마도 Claude 3.7, Grok 3 등과 같은 추론 모델을 학습하는 데 사용되었습니다. PPO는 AI 모델을 인간의 선호도에 맞춰 조정하는 데 도움이 되는 인간 피드백 강화 학습(RLHF)과 LLM에 추론 능력을 부여하는 검증 가능한 보상을 통한 강화 학습(RLVR)의 핵심입니다.

### 논문:
* PPO 논문: [https://arxiv.org/pdf/1707.06347](https://arxiv.org/pdf/1707.06347)
* GAE 논문: [https://arxiv.org/pdf/1506.02438](https://arxiv.org/pdf/1506.02438)
* TRPO 논문: [https://arxiv.org/pdf/1502.05477](https://arxiv.org/pdf/1502.05477)


<iframe width="600" height="400" src="https://www.youtube.com/embed/8jtAzxUwDj0?si=I14koDg2SxcHGWEN" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>