---
layout: post
title: A Natural Fit for Mechanistic Interpretability  
date: 2025-05-02
categories: [artificial intelligence]
tags: [protein engineering]

---

### Article Source


* [[Causal Representation Learning; A Natural Fit for Mechanistic Interpretability](https://www.youtube.com/watch?v=1fy9XpSoG3o)

---

# [Causal Representation Learning; A Natural Fit for Mechanistic Interpretability](https://www.youtube.com/watch?v=1fy9XpSoG3o)

* [Safety-Guaranteed LLMs](https://simons.berkeley.edu/workshops/safety-guaranteed-llms)

As the landscape of artificial intelligence evolves, ensuring the safety and alignment of superintelligent language models (LLMs) is paramount. This workshop will delve into the theoretical foundations of LLM safety. This could include topics like the Bayesian view of LLM safety versus the RL view of safety and other theories. 

The flavor of this workshop is futuristic, focusing on how to ensure a superintelligent LLM/AI remains safe and aligned with humans.  This workshop is a joint effort of the Simons Institute and [IVADO](https://ivado.ca/en/).

### Key Topics:

* Bayesian Approaches to LLM Safety
* Reinforcement Learning Perspectives on Safety
* Theoretical Frameworks for Ensuring AI Alignment
* Case Studies and Practical Implications
* Future Directions in LLM Safety Research
    

## Abstract

Steering methods manipulate the representations of large language models (LLMs) to induce responses that have desired properties, e.g., truthfulness, offering a promising approach for LLM alignment without the need for fine-tuning. However, these methods typically require supervision from e.g., contrastive pairs of prompts that vary by a single target concept, which is costly to obtain and limits the speed of steering research. An appealing alternative is to use unsupervised approaches such as sparse autoencoders (SAEs) to map LLM embeddings to sparse representations that capture human-interpretable concepts. However, without further assumptions, SAEs may not be identifiable: they could learn latent dimensions that entangle multiple concepts, leading to unintentional steering of unrelated properties. In this talk, I'll introduce sparse shift autoencoders (SSAEs). These models map the differences between embeddings to sparse representations that capture concept shifts. Crucially, we show that SSAEs are identifiable from paired observations that vary by multiple unknown concepts, leading to accurate steering of single concepts without the need for supervision. We empirically demonstrate accurate steering across semi-synthetic and real-world language datasets using Llama-3.1 embeddings.

## 주요 내용: 희소 변화 오토인코더를 이용한 잠재 공간 분리

이번 세미나는 **희소 변화 오토인코더(Sparse Shift Autoencoders, SSAEs)라는 새로운 비지도 학습 방법론을 제시하여 언어 모델(LLM)의 잠재 공간에서 의미 있는 개념을 분리하고 조작하는 방법**을 탐구합니다. 기존 희소 오토인코더(SAEs)의 한계를 극복하고, 약한 지도 학습 아이디어를 도입하여 더 강력한 분리 성능을 달성하는 것을 목표로 합니다. 주요 핵심 내용은 다음과 같습니다.

### **1. 기존 희소 오토인코더(SAEs)의 한계:**

* SAE는 희소성 정규화를 통해 잠재 공간에서 분리된 특징을 학습하고자 하지만, 개념과 임베딩 차원의 불일치, 데이터 다양성 부족 등의 문제로 인해 실제로 의미 있는 개념을 분리하고 조작하는 데 어려움을 겪습니다.
* 이는 유용한 조작 방향을 학습하거나, 의미 있는 개념을 탐지하는 데 실패하는 결과로 이어집니다.

### **2. 다중 개념 변화 데이터(Multi-Concept Shift Data):**

* SSAE는 단일 개념 변화가 아닌, 여러 개념이 동시에 변화하는 데이터 쌍을 활용합니다.
* 이러한 데이터는 자연 발생적인 변화를 더 잘 반영하며, 희소성 정규화가 효과적으로 작동할 수 있는 충분한 가변성을 제공합니다.
* 변화 벡터는 개념 공간에서는 희소하지만, 임베딩 공간에서는 밀집된 형태로 나타나 LLM이 개념을 얽히게 만드는 방식을 보여줍니다.

### **3. 희소 변화 오토인코더(SSAEs)의 작동 원리:**

* SSAE는 임베딩 공간에서 표현된 변화(shifts)를 입력으로 받아, 이를 개념 공간으로 풀어내는 선형 함수를 학습합니다.
* 각 데이터 쌍에 대해 변화된 개념의 수와 내용을 예측하고, 이를 기반으로 임베딩 공간으로 다시 선형 디코딩하여 재구성 오류를 최소화합니다.
* 핵심 아이디어는 개별 예시의 표현이 희소한 것이 아니라, *변화* 자체가 개념 공간에서 희소하다는 점을 활용하는 것입니다.

### **4. 식별 가능성(Identifiability) 보장:**

* SSAE는 특정 조건 하에서 (순열 및 스케일링 불확정성까지) 실제 개념 변화와 선형 혼합 함수를 복구할 수 있음을 이론적으로 증명합니다.
* 측정 불가능한 개념(임베딩 공간에서 변화를 관찰할 수 없는 개념)은 변화를 고려할 때 상쇄됩니다.
* SSAE는 데이터 세트 내에서 실제로 변화하는 개념들을 분리하는 데 초점을 맞춥니다.

### **5. 실험 결과 및 비교:**

* Llama 3.1 임베딩에 대한 실험에서 SSAE는 단일 개념 변화 예측 성능에서 기존 SAE 대비 우수한 성능을 보이거나 경쟁적인 성능을 나타냅니다.
* 특히 SAE가 제대로 작동하지 못했던 경우에 SSAE는 상당한 개선을 보입니다.
* 이는 SSAE가 LLM의 잠재 공간에서 더 의미 있고 조작 가능한 개념을 학습할 수 있음을 시사합니다.

**결론적으로,** 희소 변화 오토인코더(SSAEs)는 다중 개념 변화 데이터와 희소성 정규화를 활용하여 LLM의 잠재 공간에서 개념을 분리하는 효과적인 새로운 접근 방식입니다. 기존 SAE의 한계를 극복하고, 약한 지도 학습 아이디어를 성공적으로 통합하여 더 강력한 표현 학습 및 조작 가능성을 제시합니다. 향후 LLM의 해석 가능성 및 제어 연구에 중요한 기여를 할 것으로 기대됩니다.

<iframe width="600" height="400" src="https://www.youtube.com/embed/1fy9XpSoG3o?si=nkz-VzxfViKYj1Zr" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>


## 세부 내용

안녕하세요, 여러분. 함께해 주셔서 감사합니다. 다음 발표는 다니엘라르 교수님입니다. 몬트리올 대학교 컴퓨터 과학 조교수이십니다. 저는 교수님과 함께 연구해 왔습니다. MIA에 부임하신 이후로 인과성에 대해 연구하고 계십니다. 이는 현재 최첨단 기술에서 분명히 부족한 부분입니다. 그럼 교수님의 말씀을 들어보시겠습니다.
정말 감사합니다.
강연 시작 전에 박수갈채를 받은 건 처음이네요. 따뜻하게 맞아주셔서 감사합니다. 주최자분들께도 초청해 주셔서 정말 감사합니다. 이곳에 오게 되어 기쁩니다. 사이먼스 연구소는 처음인데, 박사 과정을 이곳에서 멀지 않은 산타크루즈에서 마쳤기 때문에 더욱 좋습니다. 캘리포니아에 다시 오게 되어 정말 기쁩니다. 감사합니다. 네, 오늘 저는 인과 표현 학습이라는 주제로 이야기할 것입니다. 궁금한 점이 있는데, 혹시 청중 여러분 중에 이 용어를 들어보신 분이 계신가요?
네, 꽤 많은 분들이 들어보셨군요. 아주 좋습니다. 기쁩니다. 하지만 그럼에도 불구하고, 저는 꽤 많은 소개 또는 튜토리얼을, 아마도 이해하기 쉽고 직관적인 수준에서 진행할 것입니다. 최근에 제 학생인 슈리(여기 계십니다. 꼭 이야기를 나눠보세요)와 함께 해온 것은 인과성과 머신 러닝의 아이디어를 LLM의 해석 가능성 및 조작과 연결하려는 시도입니다. 처음 이 주제를 접했을 때, 해석 가능성이나 조작과 같은 단어들은 저에게 다소 모호했습니다. 그래서 저는 제가 처음 배우기 시작했던 방식, 즉 매우 구체적이고 간단한 예시로 시작할 것입니다. 해석 가능성에 대해 이야기할 때 제가 무엇을 의미하는지, 또는 조작이라고 말할 때 제가 무엇을 생각하는지 설명하기 위해, 우리는 두 자리 덧셈을 수행하도록 트랜스포머를 훈련시키는 매우 간단한 예시를 들어보겠습니다. 네. 그리고 왼쪽에는 입력 16 + 25가 있는 트랜스포머가 있습니다. 다시 한번 같은 페이지에 있도록 말씀드리면, 이러한 임베딩을 생각할 때 저희가 의미하는 것은 토큰 자체가 트랜스포머에 의해 임베딩되고, 잔차 스트림에 추가되는 다중 헤드 어텐션과 MLP의 여러 레이어에 의해 연속적으로 수정된다는 것입니다. 네. 그리고 그것이 모든 토큰에 대한 최종 레이어의 임베딩을 제공합니다.

네, 여기 컴퓨터 프로그램이나 저희가 이 작업을 해결할 때 아마 사용할 간단한 작은 알고리즘이 있습니다. 네, 그래서 첫 번째 숫자와 그 일의 자리 숫자가 있습니다. 죄송합니다. 십의 자리 숫자와 일의 자리 숫자가 있습니다. 두 번째 숫자와 그 십의 자리 숫자와 일의 자리 숫자가 있습니다. 일반적으로 올림 변수를 계산하고, 이 특정한 방식으로 일의 자리와 십의 자리의 결과를 계산할 것입니다.
그리고 구체적으로 질문해 보겠습니다. 만약 제가 이 특정한 예시에서 올림 변수를 바꾸고 싶다면, 즉 시스템에 개입하여 0으로 설정하고 싶다면, 모델의 행동은 어떻게 변할까요? 분명히 이것은 이 문제에 대한 틀린 답입니다. 그렇죠? 이것은 두 자리 덧셈을 수행하는 이러한 종류의 인과 모델에 따른 이 특정한 개입 하에서의 올바른 행동입니다. 그리고 이제 저는 트랜스포머 내에서 이 정확히 똑같은 행동을 시뮬레이션하기 위해 개입할 수 있는 어떤 곳이 있는지 알고 싶을 수 있습니다.
그래서 이것은, 아시다시피, 제가 31 대신 41을 출력하도록 정확히 똑같은 행동 s를 얻기 위해 편집하거나 편집할 수 있는 어떤 곳이 있는지 묻는 것과 같습니다.
네, 이러한 종류의 개념, 인과 모델에서 사용할 수 있는 이러한 잠재 요인들은 일반적으로 임베딩의 차원과 NLP의 뉴런 전체에 걸쳐 분포될 것이기 때문에 전혀 명확하지 않습니다. 네, 따라서 어떤 종류의 개념에 대해 트랜스포머에서 축 정렬 섭동을 수행할 수 있어야 할 이유는 없습니다.
따라서 비지도 해석 가능성 파이프라인 또는 때로는 비지도 특징 발견이라고 불리는 것의 목표는 다른 기준을 배우는 것입니다. 그렇죠? 저희는 토큰 임베딩을 가져와서 이러한 기본 개념들이 실제로 분리되는 다른 기준을 찾고 싶습니다. 따라서 여기 빨간색으로 표시된 저희의 새로운 기준의 한 차원은 이 올림 변수를 나타낼 수 있습니다. 다른 차원, 아마도 차원 번호 1은 첫 번째 숫자의 십의 자리 숫자를 나타낼 수 있습니다. 그리고 트랜스포머 임베딩 공간으로 다시 매핑할 수 있기를 희망할 수 있습니다. 네, 공간과 특징뿐만이 아닙니다.
여기서는 구어체로 말하고 있지만, 분리된 다른 특징 세트를 찾으려고 노력한다는 의미입니다.
좋아요.
좋습니다. 그렇다면 이것이 저희가 처음에 제기했던 문제를 해결하는 데 어떻게 도움이 될까요?
이 분리된 특징 공간에서 개입을 수행해 볼 수 있습니다. 디코딩 함수를 사용하여 토큰 임베딩 공간에서 해당 개입이 어떻게 보일지 확인할 수 있습니다. 그리고 이제 저희는 그것을 마지막 토큰 마지막 레이어 임베딩으로 복사하면 의도한 동작을 얻을 수 있기를 바랍니다. 그렇죠? 이제 상위 수준 모델처럼 하위 수준 모델인 신경망, 즉 트랜스포머도 출력 31을 생성합니다. 좋습니다. 그렇다면 이것이 현재 어떻게 구현되고 있습니까? 아니면 몇 달 전에는 어떻게 구현되었습니까?
음, 저희는 실제로 압축 센싱에서 아이디어를 빌려왔습니다. 밀도 높은 신호를 희소한 신호들의 합으로 분해하는 사전 학습이라는 매우 오래된 아이디어입니다. 좋아요.
희소한 신호들의 합. 저는 그것을 의미합니다. 왜냐하면 이것들은, 네, 이러한 종류의 방향 자체가 희소할 필요는 없지만, 저희는 그것들의 희소한 조합만을 사용하고 싶습니다. 좋아요. 그래서 이 아이디어는 사전 학습과 압축 센싱에서 비롯되었고, 해석 가능성 연구에서 어떻게 구현되었냐면 오토인코더를 학습하려고 노력하는 것이었습니다. 그래서 저희는 임베딩을 가져와서 일종의 희소한, 다시 한번 희소한 기준이라고 말할 텐데, 희소한 기준으로 표현하고, 선형 디코더를 사용하여 원래 입력을 재구성하려고 노력하는 일종의 선형적인 인코딩 함수를 학습하려고 했습니다. 하지만 희소 사전 학습의 희소한 측면을 포착하기 위한 매우 중요한 제약 조건이 있습니다. 즉, 주어진 신호를 표현하는 데 사용해야 하는 개념의 수는 희소해야 한다는 것입니다.
좋아요. 좋아요. 그래서 이것이 구체적인 목표이고, 이것은 2023년경에 해석 가능성 커뮤니티에서 널리 채택되었습니다.
음, 그리고 개념이 실제로 임베딩에서 선형적으로 표현될 때 이 모든 것이 이치에 맞다는 점에 주목하세요. 왜 이것이 사실일까요?
음, 이 가설을 뒷받침하는 오래된 증거와 새로운 증거가 많이 있습니다. 2013년에 저희가 여전히 워드 임베딩을 사용했을 때, 그러한 종류의 모델이 임베딩 공간의 선형 방향으로 개념을 표현한다는 실증적 증거가 있었습니다. 그리고 최근에는 다음 토큰 예측 모델조차도 마지막 레이어에서 개념을 선형적으로 표현한다는 이론적인 연구가 있습니다. 좋아요, 나중에 더 자세히 이야기할 수 있습니다. 이 발표에서는 다루지 않겠습니다. 음, 하지만 핵심은, 그렇죠, 개념이 이 주어진 모델에서 실제로 분리되어 있다면, 디코딩 함수의 각 열은 고유한 조작 방향을 제공한다는 것입니다. 그렇죠? 디코더 열 번호 0, 1, 2 등에 어떤 개념이 해당하는지는 모르지만, 개념이 진정으로 분리되어 있다면 단일 개념이 조작됩니다.
네. 개념이 분리되었다는 것의 정의와 거의 같나요? 그것이 분리되었다고 말하는 것의 의미인가요? 음, 분리되었다는 의미를 조금 더 정확하게 정의해 보겠습니다. 하지만 지금은 더 함축적인 의미로 생각할 수 있습니다.
좋아요. 좋아요. 그렇다면 이것은 어떻게 작동해 왔습니까? 그렇죠. 음, 앞서 말씀드린 것처럼 희소 오토인코더는 희소 사전 학습에서 영감을 받아 ML 논문에 나타나기 시작했고, 앤트로픽에서 인기를 얻었고, 예를 들어 구글 딥마인드가 제마를 위한 매우 큰 사전 훈련된 희소 오토인코더를 공개하고 오픈 소스로 만들면서 더욱 인기를 얻었습니다. 그것은 꽤 많은 인기를 얻기 시작했습니다. 그러다가 더 많은 연구 논문들이 나왔습니다.
음, 희소 오토인코더에 대한 광범위하고 엄격한 평가를 살펴보았고, 유용한 조작이나 유용한 개념 감지로 이어질 방식으로 개념을 발견하거나 분리하지 못하는 것 같다는 증거가 늘어나기 시작했습니다. 그리고 구글 딥마인드가 이제 SAE에서 벗어나고 있다고 발표할 정도가 되었습니다.
그리고 저는 해석 가능성 연구에서 다음 번 과대 광고로 넘어갈 것이라고 생각하지 않을 수 없습니다. 그리고 저는, 아시다시피, 해석 가능성 및 조작 연구에서 저희가 계속 겪고 있는 이러한 부침 주기가 있으며, 더 많은 보장을 가진 비지도 방법을 고려함으로써 어느 정도 방지할 수 있다고 주장할 것입니다. 그렇죠? 좋아요. 그리고 제가 보장이라고 말하는 것이 무엇을 의미하는지 말씀드리겠습니다. 느슨한 용어로, 저희는 증명 가능하게 개념을 분리할 것입니다. 좋아요. 손이 올라간 것을 보았습니다. 마음이 그 곡선을 계속 잘못 맞춥니다.
네, 예측 모델도 만들 수 있을 것 같습니다. 음, 좋아요. 그렇다면 분리하고 증명 가능하게, 어떻게 해야 할까요? 어떻게 해야 할까요? 좋은 점은 인과 표현 학습이라고 불리는 오랜 분야가 있으며, 이는 표현에서 발견하는 특징들이 일종의 개입 가능한 의미론을 갖도록 딥러닝을 수행하는 것에 관한 모든 것입니다. 이러한 특징들은 일반적으로 제가 고립적으로 개입하고 싶은 종류의 개념이나 요인에 해당합니다.
좋아요. 그리고 이 모든 것을 설정하겠습니다. 음, 아시다시피, 제가 간단한 예시를 통해 설명하고 좀 더 정확하게 설명할 때까지 참아주세요. 질문을 받겠지만, 명확성을 위한 질문만 받겠습니다. 열린 질문이라면 끝까지 기다리겠습니다. 음, 제가 이러한 개념들을 어떻게 생각하는 것이 올바른지 명확히 하고 싶습니다. 왜냐하면 이 덧셈 문제에서조차도 제가 개념이라고 생각할 수 있는 것들이 너무 많기 때문입니다. 음, 그래서 이것을 좀 더 정확하게 만들면 요인이나 개념에 대한 생성 모델 해석을 생각해낼 수 있을 것이라고 생각하기 때문에, 잠시만 기다려 달라고 요청하겠습니다. 하지만 강연의 나머지 부분에서 명확하지 않다면 끝에 다시 돌아오겠습니다. 좋아요.
좋아요. 그럼 실제로 더 간단한 문제, 아니 어쩌면 더 간단한 문제를 생각해 보겠습니다. 이미지를 고려하고 이미지의 인코딩 또는 임베딩을 학습하는 것을 고려할 것입니다. 그리고 이것들은 매우 매우 간단한 이미지가 될 것입니다. 저는 공을 가질 것입니다. 이러한 이미지들이 변하는 유일한 종류의 차원은 공들이 x축의 다른 위치에 있을 것이고, 색깔이 다를 것이라는 것입니다. 좋아요. 따라서 실제로 여기에는 두 가지 변동 요인만 있습니다. 그리고 음, 텍스트에서 감독을 받는다고 가정해 보겠습니다. 그래서 저는 이미지 속 공의 위치를 알려주는 작은 캡션을 가질 것입니다. 그리고 구체적인 예시를 위해, 저는 일종의 클립 스타일 목표를 고려할 것입니다. 좋아요. 하지만 여기서 매우 매우 분명히 말씀드리고 싶은 것은, 제가 이야기할 모든 것이 오토인코딩, BAE 스타일 모델링, 관찰의 로그 우도와 같은 것을 포함한 매우 다양한 표현 학습 목표에 적용된다는 것입니다. 따라서 이것은 매우 구체적인 예시일 뿐입니다. 따라서 클립 스타일과 같은 것을 수행한다고 가정해 보겠습니다. 예를 들어 캡션을 인코딩하는 방법이 있다고 가정해 보겠습니다. 그리고 지금은 고정되어 있다고 가정해 보겠습니다. 그리고 저는 인코딩 함수를 학습하고 싶습니다. 저는 제 객체의 임베딩을 학습하고 싶습니다. 저는 텍스트 인코딩으로 선형적으로 다시 매핑할 수 있는 일부 특징 세트를 학습하고 싶습니다. 좋아요. 그것이 저희가 목표로 설정할 것입니다. 질문 있으신가요? 네, 좋습니다. 따라서 저는 여기서 작동할 한 가지 해결책을 제시했습니다. 그렇죠? 이 특정 축들은 이 특징 세트의 회전일 뿐입니다. 따라서 그것들은 선형적으로 관련될 것이고, 그것은 매우 좋은 기준 선택이었습니다. 왜일까요? 음, 이 기준의 축, 즉 축들은 저희가 고립적으로 개입하고 싶은 종류의 것들과 일치합니다. 그렇죠? 따라서 예를 들어 제 이미지에서 새로운 이미지를 얻을 때마다 공의 색깔과 위치가 모두 변하더라도, 모델을 훈련시킨 후에 제가 하고 싶은 것은 이러한 개념들을 독립적으로 고립적으로 조작하는 것일 수 있습니다. 그리고 이 특정 좋은 기준에서, 제가 해야 할 일은 한 축을 따라서만 이동하는 것입니다. 그리고 그렇게 하면, 한 가지 일만 변할 것입니다. 이 경우, 색깔입니다. 좋아요. 질문은, 제 목표로 그 매우 좋은 기준을 학습하도록 보장받을 수 있을까요? 그리고 이미 많은 분들이 이것을 보고 계실지도 모르지만, 아닙니다. 그렇죠? 왜냐하면 어떤 회전이라도 수행할 수 있고, 손실을 똑같이 잘 최소화할 수 있지만, 이 특정 기준은, 이 회전의 뒤쪽에 있는 좋은 색깔들을 잘 보이지 않게 했다고 생각하지만, 이제 제가 공의 색깔만 고립적으로 바꾸고 싶다면, 실제로 제 새로운 기준에서 이 대각선 방향으로 이동해야 할 것입니다. 그렇죠? 이전에는 단일 축만 섭동하면 되었지만, 이제 색깔을 바꿀 수 있는 이 특별한 벡터를 찾아야 할 것입니다. 따라서 이것을 좀 더 명확하게 볼 수 있습니다. 이제 제가 어떤 종류의 축 정렬 변경을 했다면, 이것이 이미지에서 매우 최소한의 개념만 변경하기를 희망했지만, 여러 가지가 변경될 것입니다. 공은 회색이 되고 동시에 중앙으로 이동할 것입니다. 이것이 그렇게 나쁜가요? 이론적으로는 레이블이 지정된 예시를 사용하여 해당 변경 방향을 학습할 수 있지만, 이것이 핵심입니다. 그렇죠? 이제 갑자기 어떤 종류의 조작을 수행하기 위해 더 많은 레이블이 지정된 데이터가 필요한 문제로 돌아왔습니다. 그렇다면 비지도 학습만으로 무엇을 할 수 있을까요? 그 좋은 분리된 개념들을 학습할 수 있을 희망이 있을까요? 좋아요, 거기에... 저는 많은 참조 문헌으로 압도하려고 이 슬라이드에서 매우 고의적으로 장난스럽게 행동하고 있습니다. 여러분이 그것들을 모두 읽으려고 하는 것을 의도하지는 않지만, 요점은 이것이 실제로 1990년대 초반의 선형 ICA부터 시작된 매우 매우 매우 오랜 문헌 분야라는 것입니다. 불행히도 제가 가진 첫 번째 인용은 2016년 것입니다. 하지만 이것은 식별 가능한 목표와 표현 학습을 설계하는 방법에 대한 매우 오랜 연구입니다. 즉, 여러분이 그것들을 최소화하면, 최소값 집합의 일부인 모든 해들이 매우 좋은 해석 가능한 속성을 가질 것이라고 보장할 수 있습니다. 그리고 저는 매우 높은 수준에서 단지 그 중 하나를 설명하여 이러한 방법들이 어떻게 작동하고 어떻게 진행되는지 보여드리려고 합니다. 좋아요. 그리고 저희 논문 중 하나가 이제 이 아이디어를 다시 해석 가능성으로 매핑하는 것을 볼 것입니다. 좋아요. 이 CRL의 특정 아이디어는 매우 높은 수준에서 약한 감독을 사용하는 것에 관한 모든 것입니다. 좋아요. 그리고 약한 감독이란 무엇일까요? 저희는 단일 관찰이 아닌 쌍으로 된 샘플을 가질 것입니다. 그리고 이 특정 예시에서, 여러분은 여러 개의 공을 가지고 있습니다. 그리고 아시다시피, 이러한 이미지를 표현하는 좋은 방법은 각 공의 XY 위치와 그 색조를 인코딩하는 것일 수 있습니다. 그리고 여기에는 여러분이 할 수 있는 여러 가지 다른 섭동이 있습니다. 좋아요? 다양한 수준의 희소성을 가지고 있습니다. 따라서 아마도 섭동 번호 1은 녹색 공만 이동시킬 것입니다. 섭동 번호 2는 파란색 공을 이동시킬 것입니다. 등등. 여러분은 x축과 y축 모두를 따라 객체들을 이동시키기 시작하는 더 밀도 높은 섭동을 가지고 있습니다. 여러분은 동시에 여러 개의 공들을 양쪽 축을 따라 이동시키는 훨씬 더 밀도 높은 섭동을 가질 수도 있습니다. 그렇죠? 따라서 희소하거나 밀도 있는 정도가 다양한 이러한 종류의 풍부한 섭동 집합을 가질 수 있습니다. 그리고 이러한 종류의 쌍으로 된 관찰이 있다면, 여기서 핵심은 여러분이 아시다시피 반사실적 샘플을 가지고 있다는 것입니다. 즉, 원래 입력과 그것의 섭동된 버전을 가지고 있다는 것입니다. 저희는 이것을 사용하여 인코더를 훈련시킬 수 있습니다. 특히, 만약 여러분이 실제로 희소 섭동을 가지고 있다면, 이미지와 그것의 반사실적 쌍의 인코딩이 너무 많은 차원에서 변하지 않도록 인코더를 훈련시킬 수 있습니다. 좋아요. 따라서 f(x)와 f(x 틸다)가 너무 많은 차원에서 변하지 않도록 할 수 있습니다. 여기서 f(x 틸다)는 반사실적 이미지의 임베딩입니다. 여러분은 이 두 임베딩의 차이가 희소하도록 강제할 수 있습니다. 좋아요. 왜일까요? 그것은 무엇을 할까요? 차이에서의 희소성 정규화는 무엇을 할까요? 이것이 무엇을 하는지 기억하세요? 음, 저희는 이론적으로, 특히 이동이 1-희소할 때, 이 특정 목표에 의해 학습된 모든 해, 즉 최소값들이 순열 및 스케일링까지의 지상 진실과 관련된 zhat hat(x의 fhat)이 될 것이라는 것을 실제로 증명할 수 있습니다. 좋아요. 다시 말해, 여러분은 지상 진실 표현(지상 진실이라고 말할 때, 모든 개념이 독립적으로 개입 가능한 그 좋은 기준에 대해 이야기하고 있었습니다)과 이 특정 목표에 의해 발견된 해 사이에 일대일 정렬이 있다고 생각할 수 있습니다. 좋아요. 이것이 저희가 식별 가능성이라고 구체적으로 의미하는 것입니다. 즉, 남아 있는 이러한 종류의 순열 스케일링 불확정성이 있지만, 그것이 전부입니다. 잠시 멈추겠습니다. 네.
약한 지도 학습이지만, 환경 내의 개별 객체에 대해 "이 객체를 이동시켰다"고 말하는 것이기 때문에 그것보다 약간 더 강력할 수 있습니다.
네, 말씀하신 것이 완전히 타당합니다. 제 생각에는, 아시다시피, 명명 규칙이 매우 오해를 불러일으킬 수 있는 것 중 하나이고, 약한 지도 학습이 논쟁의 여지가 많다는 점도 맞습니다. 왜냐하면 직접적으로 감독하지 않고, 예를 들어 "이 이미지에 대해, 이것, 이것, 이것, 그리고 이 측정값으로 표현된다"고 말하는 것이 아니라, 여러분이 감독하는 것은 어떤 종류의 축 정렬 변화가 있는지에 관한 것이기 때문입니다. 하지만 정말 좋은 지적이라고 생각합니다. 네. 그렇다면 학생 스포츠에서 텍스트 표현에서 동일한 종류의 정보를 얻는 것은 어떻습니까? 색깔 변화는 어떻습니까?
그것은 정말 환상적인 질문입니다. 아마도 사람들이 이 문제를 연구하고 있을 것이라고 생각합니다. MA 주변에서 사람들이 희소 클립 같은 것을 시도하고 있다는 소문을 들었습니다. 따라서 이것이 일어나고 있어도 놀라지 않을 것입니다.
좋아요, 그것이 일종의 아이디어입니다. 그렇죠? 저희는 그것을 해석 가능성으로 옮길 것입니다. 다시 말해, 저희가 무엇을 하고 있었습니까? 저희는 이미지의 회전된 임베딩을 가져와서 그것들을 풀려고 노력하고 있었습니다. 그래서 이제 제 축들은 어쨌든 제가 자연스럽게 만들고 싶은 종류의 개입에 해당할 것입니다.
저는 이제 토큰 임베딩에 대해서도 똑같은 것을 시도할 것입니다. 그렇죠? 제가 학습하는 잠재 특징들이 제가 조작하고 싶은 종류의 개념들이 될 표현을 찾으려고 노력할 것입니다.
좋아요, 최근에 희소 오토인코더가 반드시 약속을 지키지 못했다는 실증적 발견에 대해 말씀드렸습니다. 음, 그 논문은 매우 좋습니다. 액스벤치라고 불리며, 그들은 인간 평가를 포함한 꽤 광범위한 평가 세트를 수행했습니다. 인간 평가에서는 그들이 조작을 하고, 완성을 생성한 다음, 인간이 특정 방식으로 점수를 매기도록 했습니다. 그리고 그들은 모델이 실제로 분리되었다면 이쯤에 있어야 한다는 것을 보여주는 다른 종류의 평가도 많이 했습니다. 좋아요. 논문에 들어가지는 않겠지만, 그들이 수행한 평가에는 분리의 검증 가능한 함축이라는 좋은 속성이 있었다고 말하겠습니다. 분리가 되어 있다면, 그들이 설정한 작업에서 잘 수행해야 했습니다. 그리고 저희는 SAE가 약간의 감독을 사용했기 때문에 이것을 무시한다는 것을 알 수 있습니다. 그것은 좋지 않은 이쯤에 있습니다.
그래서 그것은 아시다시피, 아마 에세이가 실제로 개념을 분리하지 못할 수도 있다는 이러한 관찰로 이어집니다. 그래서 질문은 저희가 실제로 이전의 회전된 기준처럼 얽힌 특징으로 끝나는가입니다. 그렇죠? 그것이 SAE에서 일어나고 있는 일입니까?
음, 이 실험들은 나중에 다시 말씀드리겠지만, 저희도 다른 작업 세트, 즉 몇 가지 자연어 개념이 변하는 다른 자연어 데이터 세트를 사용하여 자체 연구를 수행했습니다. 그리고 저희는 이러한 예시 zed와 zed tilda를 살펴보았습니다. 다시 말하지만, 이것들은 임베딩입니다. zed tilda는 프롬프트 자체에서 단일 개념 변화에서 파생된 임베딩입니다. 그리고 저희는 라마 스코프 오픈 소스 디코더의 각 열을 사용하여 zed를 조작하고 z tilda에 상당히 가까워지는지 확인하려고 시도합니다. 음, 순열 및 불확정성이 여전히 있어서 저희가 아무것도 할 수 없기 때문에, 모든 열을 검색해야 한다는 것을 기억하세요. 그리고 저희의 발견은 약간 더 유망하거나 호의적이었습니다. 일부 데이터 세트에서는 코사인 유사성이 나쁘지 않다는 것을 발견했지만, 실패하면 때로는 재앙적으로 실패하기도 합니다. 이 특정 데이터 세트는 언어였던 개념들을 포함했습니다. 따라서 아시다시피, 여기에는 흥미로운 점이 있습니다. 음, 이는 실제로 SAE에 희소성 정규화가 있음에도 불구하고 개념을 분리할 수 없게 만드는 무언가가 있을지도 모른다는 추가 가설로 이어집니다. 그리고 이것은 약간 이상합니다. 왜냐하면 앞서 보았듯이 희소성 정규화는 일반적으로 직관적으로 분리된 특징으로 이어졌기 때문에 무언가가 잘못되고 있습니다. 무엇이 문제일까요?
그래서 문제 번호 1, 그렇죠? 이것은 모두 세부 사항에 악마가 있다는 것입니다. 문제 번호 1, 개념, 일부 개념은 전혀 학습할 수 없을 수도 있습니다. 그게 무슨 뜻일까요? 좋아요, 임베딩의 차원보다 훨씬 더 많은 개념이 있습니다. 따라서 아마도 일종의 활성화 C1, 다른 활성화 C2가 있는 예시가 있을 수 있습니다. 그렇죠? 따라서 이것들은 텍스트에 있을 수 있는 종류의 개념을 나타내는 두 개의 다른 잠재 코드이며, 정확히 동일한 임베딩, 예를 들어 정확히 동일한 라마 임베딩에 매핑될 수 있습니다. 이제 저희는 실제로 데이터 생성 프로세스를 역전시키려고 노력하고 있습니다. 그렇죠? 저희는 zed에서 C로 가려고 노력하고 있습니다. 이것은 실제로 단사 함수의 실패입니다. 그렇죠? 두 개의 다른 입력이 있고 동일한 출력에 매핑됩니다. 따라서 이제 출력에서 입력으로 다시 가려고 하면, 단사 함수가 부족하기 때문에 여기서 어떻게 해야 할지조차 모를 것입니다. 따라서 분리는 잘 정의된 문제가 아닙니다. 그것이 문제 번호 1입니다. 문제 번호 2는 데이터 다양성 부족과 관련이 있습니다. 구체적인 예시를 들어보겠습니다.
따라서 단일 개념만 포착하는 예시가 부족하면, 예를 들어 가지고 있는 모든 예시가 실제로 매우 많은 개념에 의해 활성화된다면, 많은 가짜 해답, 정규화기를 최소화하는 많은 얽힌 인코딩 함수가 있을 것입니다. 그렇죠? 그것은 저희가 본질적으로 잃는 것입니다. 저희는 L1 페널티에서 무엇보다도 분리된 해답을 선호하는 힘을 빼앗습니다. 그리고 이것은 실제로 놀라운 일이 아닙니다. 저희는 희소 디코딩에 대한 저희 논문에서 이것을 발견했습니다. 그리고 이것은 희소 사전 학습과 더 유사한 종류의 알고리즘에도 잘 알려진 아이디어입니다. 이것이 작동하려면 단일 개념으로만 설명해야 하는 일부 앵커(저희가 그렇게 부릅니다)가 필요합니다. 표현에 이러한 종류의 실제 희소성이 있다면, L1 정규화기는 무엇보다도 분리된 표현을 실제로 선택할 것입니다. 네. 여전히 이 프로비전 예시 설정에 있습니까? 단일 개념 변이가 들어오는 곳이 바로 그곳입니까?
그래서 저희는 그것을 뒤로하고 잠시 후에 다시 돌아올 것입니다. 하지만 지금은 약한 지도 학습이 없는 표준 VA, 죄송합니다, 희소 오토인코더에 대한 것입니다. 그래서 저희는 이것을 가지고 있습니다. 그렇죠? 저희는 매우 큰 토큰 임베딩 코퍼스에 대한 토큰 임베딩을 가져와서 각 토큰 임베딩을 희소하게 표현하는 일부 인코딩 함수를 학습하려고 노력할 것입니다. 공 대 위치와 같은 것을 버려야 합니다. 왜냐하면 거기서는 단일 개념이 공 또는 위치일 뿐이고 다른 것은 아니기 때문입니다.
정확합니다. 따라서 그 예시로 다시 돌아오겠습니다. 아니면 약한 지도 학습 사례로 다시 돌아오겠지만, 지금은 잠시 SA로 돌아왔습니다. 네, 그 질문과 매우 관련이 있습니다. 쌍이 없다면, 어떻게 단일 개념만 가질 수 있을까요?
그것은 철학적인 질문이지만, 제 생각에는 그것이 바로 요점입니다. 그렇죠? 저희는 식별 가능한 표현을 얻기 위한 목표로 희소 디코딩 또는 희소 인코딩을 실제로 사용하고 싶다면, 단일 개념에 의존하는 이러한 앵커 특징이 필요하다는 것을 보여주었습니다. 이것이 없다면 식별 가능성을 얻을 수 없습니다. 데이터에 대해 타당하다고 생각하는 가정이 있다면, 사용하지 않을 이유가 없습니다. 하지만 그것이 방어할 수 없다고 주장할 수도 있습니다. 그런 경우에는 실제로 다른 것이 필요합니다. 좋아요. 그래서 저희는 희소 오토인코더의 이러한 실패 또는 희소 오토인코더의 이러한 명백한 실패에 동기를 부여받았고, 이제 표현 자체가 아닌 희소한 변화가 될 약한 지도 학습 아이디어로 돌아갈 것입니다. 그리고 저희는 다중 개념 변화 데이터를 사용하는 희소 변화 오토인코더라고 불리는 방법을 가지고 있습니다. 좋아요. 그리고 그것이 여기서의 기여 중 하나이며, 이것이 무엇인지 자세히 설명하겠습니다. 이것은 여기 계신 저의 환상적인 박사 과정 학생인 슈리가 주도했습니다. 음, 그녀에게 가서 인사하고 여러분의 어려운 질문을 모두 하도록 격려합니다. 음, 이것은 아카이브에 있습니다. 좋아요. 그럼 다중 개념 변화가 무엇인지부터 시작해 보겠습니다. 좋아요. 다중 개념 변화 데이터 세트를 얻기 위해 저희가 거칠 작은 종류의 데이터 생성 프로세스가 있습니다. 저희가 할 것은 먼저 일부 개념 활성화 세트, 개념 측면에서 일부 잠재 코드를 샘플링하는 것입니다. 좋아요, 그것이 여기 P(C)에서 추출된 것입니다. 그리고 이 벡터와 관련된 여러 인덱스가 있다는 점에 주목하세요. 그런 다음 일부 위치의 하위 집합을 샘플링할 것입니다.
좋아요, 저는 P(S)에서 위치 1과 위치 4를 샘플링할 수 있습니다. 여기서 S는 하위 집합으로 인덱싱될 것입니다. 이제 제가 선택한 하위 집합이 주어지면, 일부 벡터, 일부 변화 벡터 델타를 샘플링할 것입니다. 좋아요, 이것은 개념 번호 1을 얼마나 많이 변경하고 개념 번호 4를 얼마나 많이 변경하고 싶은지 알려줍니다. 다른 모든 것은 그대로 유지될 것입니다.
저는 제가 그린 개념 코드에 델타 C를 적용하고 C 틸다를 얻을 것입니다. 이것을 완료한 후, 이러한 개념에서 텍스트를 제공하는 일부 알려지지 않은 혼합 함수가 있습니다. 좋아요, 저는 이러한 생성 함수에 대해 아무것도 가정하지 않을 것입니다. 저는 그것을 무시할 것입니다. 제가 아는 것은 C와 C 틸다에서 각각 파생된 X와 일부 X 틸다를 얻을 것이라는 것입니다. 그런 다음 저는 그것들의 임베딩을 가져올 것입니다. 예를 들어, 마지막 레이어의 마지막 토큰 위의 토큰 임베딩으로 갈 수 있습니다. 그렇죠? 그것이 X에 대한 제 임베딩 개념이 될 수 있습니다. 그리고 저는 zed와 그에 해당하는 zed 틸다에 대한 임베딩을 얻을 것입니다. 그렇죠? 이제 저는 이 두 텍스트 간의 차이를 임베딩 공간에서 표현했고, 그런 다음 저는 그것들의 차이를 계산할 것입니다. 그리고 저는 이것을 델타 z라고 부를 것입니다. 그리고 제 개념 표현 공간에서 차이가 희소했다는 점에 주목하세요. 예를 들어 여기 델타 c는 많은 0을 가지고 있고, 예를 들어 좌표 번호 1과 4에서만 활성화됩니다. 왜냐하면 그것들이 저희가 변경하는 개념이기 때문입니다. 개념 공간에서 희소한 것은 표현 공간에서 희소할 필요가 없습니다. 여기에는 매우 밀도 높은 차이 벡터가 있습니다. 좋아요. 따라서 저희는 좋은 개념 공간에서 LLM 공간으로 이동했을 때 그 좋은 희소성을 잃었습니다. 왜냐하면 이러한 개념들은 LLM에 의해 얽혀 있기 때문입니다. 그것이 바로 저희가 해결해야 할 문제입니다. 그렇죠? 다중 개념 변화가 무엇인지에 대한 질문이 있으시면 잠시 멈추겠습니다. 좋아요, 그리고 저희가 이것을 수행하는 동안 모든 가변성이 어디에 있는지 말씀드리겠습니다. 그렇죠? 여러분은 변경하고 싶은 다양한 개념 세트를 샘플링할 수 있습니다. 모든 단일 예시마다, 여러분은 변경하고 싶은 다른 개념 세트를 샘플링할 것입니다. 이는 일부 변화가 자연스럽게, 일부 자연 발생 예시가 단일 개념으로 달라질 것이라는 것을 의미합니다. 일부는 다섯 개로 달라질 것입니다. 그렇죠? 따라서 여러분은 희소에서 밀도 있는 변화 측면에서 이러한 가변성을 얻을 것입니다. 네. 각 개념은 밀도 벡터와 관련이 있습니다.
각 개념은 베니 공간에서 밀도 벡터와 연관될 것입니다. 네. 그리고 그래서, 그리고 나서 저희는 그것들을 더해서 집합의 개념의 혼합 개념으로 표현될 것을 얻습니다. 좋아요. 그래서, 그래서 아마 한 번 더 말씀드리겠습니다. 그래서, 저희가 정의하지 않을 혼합 함수가 있어서, 개념에서 이러한 개념들의 일부 혼합으로 갈 수 있게 해줍니다. 그렇죠? 아니, 오히려, 저희는 이러한 모든 다른 개념들이 섞인 일부 임베딩을 관찰할 것입니다. 그리고 아이디어는, 저희가 일부 혼합된 개념 집합을 표현할 임베딩 함수 f(x)를 가지고 있고, 이제 그것들을 풀어내고 싶다는 것입니다.
음, 질문.
네, 왼쪽의 내용이 무엇인지 놓친 것 같습니다. 네트워크의 초기 활성화인가요? 아니요, 이것들은 미리 지정된 것이 아닙니다. 아니요, 이것은 데이터 생성 프로세스입니다. 데이터가 어떻게 생성되는지, 텍스트가 어떻게 저희에게 오는지에 대한 것입니다. 좋아요, 좋습니다. 그리고 그것에 대해 아무것도 모릅니다. 네, 그리고 이것은 아시다시피, 이것은 CRL이 확률적 모델링이나 변분 오토인코더로 작업한다면 익숙할 이러한 종류의 잠재 변수 모델링 관점을 매우 많이 채택하는 것입니다. 이것을 표준 CRL과 다르게 만드는 핵심은 많은 표현 학습이 생성 프로세스, 생성 함수에 대해 많은 가정을 한다는 것입니다. 여기서 저희는 너무 많은 가정이나 실제로 어떤 가정도 하지 않을 것입니다. 네, 첫 번째 벡터는 왜 다른 C들과 같은 수의 차원을 가지고 있습니까? 음, 왜냐하면 이것을 생각하는 또 다른 방법은 C와 C 틸다가 있고, 이것이 그 두 벡터 간의 차이를 포착하기 때문입니다. 따라서 이것은 다른 두 벡터와 같은 공간에 존재합니다.
좋아요.
좋아요. 이 단계에서 매우 정확하게 하고 싶은 몇 가지 주의 사항이 있습니다. 음, 사실 번호 1, 그렇죠? 이것들은 단일 개념 변화 데이터가 아닙니다. 이것들은 사람들이 일반적으로 조작 벡터를 학습하는 데 사용하는 대조 쌍이 아닙니다. 그렇죠? 단일 개념으로만 변하는 쌍이 있다면, 그 개념이 무엇인지 알 것입니다. 이러한 대조 쌍을 간단히 사용하여 임베딩을 비교하고 그 개념에 대한 조작 방향을 얻을 수 있습니다. 그렇죠? 그것이 일반적으로 지도 조작에서 수행되는 것입니다. 이것은 그 데이터가 아닙니다. 그렇죠? 여러분은 단일 개념으로 변하는 일부 쌍을 가질 것이라는 의미에서 엄격히 더 일반적입니다. 그 단일 개념이 무엇인지 알지 못합니다. 여러분은 많은 개념으로 변하는 일부 쌍을 가질 것입니다. 여러분은 대조 쌍 간의 변화량에 걸쳐 이러한 종류의 가변성을 가지고 있습니다. 좋아요. 따라서 저희가 제시하는 한 가지 예는, 예를 들어 LLM을 통해 생성된 합성 대조 쌍과 같은 것에 이 시간 방법을 사용할 수 있다는 것입니다. 예를 들어 LLM에게 프롬프트를 가져와서 모든 것을 고정하고 해당 프롬프트의 단일 측면만 변경하도록 요청할 때입니다. 제 생각에는 완벽한 반사실적 사례를 생성할 수도 있지만, 아마도 그렇지 않을 것입니다. 여러 가지를 변경할 수도 있습니다. 따라서 예를 들어 이 다중 개념 변화 패러다임을 사용하면, 완벽하고 매우 깨끗한 단일 개념 변화 데이터 세트를 가질 필요 없이, 이러한 종류의 데이터조차 수용할 수 있을 것입니다. 저희는 쌍 간에 변화하는 개념의 수가 다양한 훨씬 더 저렴하고 얻기 쉬운 데이터 세트를 활용하기 시작할 수 있습니다. 질문 있으신가요?
어떤 종류의 데이터가 있는지에 대해. 네.
마지막 질문과 관련된 명확화입니다. 이러한 부모를 생성할 수 있게 해줄 것이기 때문에, 학습하고 싶은 개념을 미리 지정해야 할 것 같습니다. 음...
아니요, 아니요, 제 생각에는 여기에 일종의 간극이 있습니다. 그렇죠?
데이터가 있습니다. 그렇죠?
만약 당신이 가진 데이터를 설명하기에 이것이 적절한 데이터 생성 프로세스라고 기꺼이 받아들인다면, 저희 방법의 보장을 신뢰하고 진행해야 합니다. 이 메커니즘 자체를 사용하여 데이터를 생성할 필요는 없습니다.
잠시만요, 다른 분들은 이 단계에서 일반적으로 잘 이해하고 계신가요? 좋아요, 강연 후에 다시 돌아올 수 있지만, 좋아요, 좋습니다. 네, 아마도 C10과 같은 데이터 세트가 C10에서 대조 쌍이 있다고 주장할 수 있는 범주에 속하는지 여쭤봅니다.
그렇지 않다고 생각합니다. 그렇죠?
아마 아닐 겁니다. 음, 저는 여기서 텍스트 기반의 대조 쌍을 더 생각하고 있었습니다. 음, 저희가 제시한 이 예시에서, 저는 어떤 것을 상상할 것입니다. 아마 여러분은 여기서 비전 언어 모델을 사용할 수도 있을 것입니다. 하지만, 예를 들어 안정적인 확산에게 CR10 이미지를 가져와서, 객체의 의미 범주는 정확히 동일하지만 스타일이 약간 다른 이미지를 제공하도록 요청할 수 있을 것입니다.
네, 연속 프레임을 사용할 수 있을 것 같습니다. 감사합니다. 음, 완벽합니다. 그것이 제가 앞서 CRL에서 약한 섭동과 관련하여 이야기했던 내용입니다. 그것이 바로 그들이 사용해야 한다고 주장하는 것입니다. 비디오의 연속 프레임처럼요. 왜냐하면 자연 발생적인 변화는 일반적으로 희소하기 때문입니다. 네.
좋아요. 그렇다면 이 다중 개념 변화 데이터 세트는 무엇을 해결할까요? 음, 저희는 논문에서 인덱스를 샘플링하는 방식과 변화 벡터 델타를 샘플링하는 방식에 대한 조건을 공식화하여 식별 가능성을 위한 적절한 목적 함수가 되기 위해 희소성 정규화에 필요한 충분한 가변성을 얻습니다. 그리고 측정 불가능한 개념, 즉 단사 함수가 아닌 개념의 질문을 어떻게 해결할까요? 음, 어쨌든 임베딩 공간에서 차이를 측정할 수 없는 이러한 것들, 이러한 개념들은 변화를 고려할 때 상쇄될 것입니다. 좋아요, 그래서 측정 불가능한 개념들, 즉 어쨌든 임베딩에서 관찰 가능한 영향을 미칠 수 없는 것들은 변화를 고려할 때 상쇄될 것입니다. 그리고 여기서 핵심적인 세부 사항은 저희가 스스로를 제한할 것이라는 것입니다. 저희는 모든 단일 개념을 복구하거나 모든 단일 개념을 분리하는 것에 대해 아무것도 주장하지 않을 것입니다. 저희는 스스로를 제한할 것입니다. 특정 다중 개념 변화 데이터 세트 내에서 변하는 개념을 분리하는 데만 관심을 가질 것입니다. 좋아요, 그럼 희소 변화 오토인코더는 무엇일까요? 음, 여기서 생성 프로세스에 대해 이야기했습니다. 희소 오토인코더와 매우 유사한 아이디어이지만, 임베딩 공간에서 표현된 이러한 변화를 입력으로 사용할 것입니다.
저희는 이러한 변화를 개념 공간으로 풀어내는 선형 함수를 학습할 것입니다. 따라서 모든 단일 예시 쌍에 대해 예측하려고 노력할 것입니다. 어떤 개념이 바뀌었는지 추측하려고 노력할 것입니다. 정말로, 얼마나 많은 개념이 바뀌었는지 추측하려고 노력할 것입니다. 그리고 이 추측을 바탕으로, 선형적으로 디코딩하고 임베딩 공간으로 다시 돌아가려고 노력할 것입니다. 다시 말하지만, 앞서 말씀드렸듯이, 여기서 핵심은 저희가 SAE처럼 기준을 찾으려고 노력하는 것이 아니라는 것입니다. 특정 예시에 사용된 개념이 희소한 기준을 찾으려고 노력하는 것이 아니라, 개입이 희소한 기준을 찾으려고 노력하는 것입니다. CRL의 약한 지도 학습 예시로 돌아가서요. 따라서 여기서 목표는 구체적으로 하는 것입니다. 따라서 저희는 SAE도 하는 것처럼 이전과 같이 재구성 오류를 최소화할 것이고, 여기서 L0 노름을 보시겠지만, 실제로 이 제약 최적화를 수행할 때 L1을 사용할 것입니다. 저희는 모든 단일 대조 쌍에 사용된 개념의 수를 추측할 때, 저희의 변화를 추측할 때 추측할 희소성을 페널티를 부과하거나 오히려 제약할 것입니다. 저희는 그 L0 노름이 어떤 값 베타보다 작도록 제약할 것입니다. 좋아요. 그리고 베타가 정확히 무엇인지는 약간 특별하며, 휴식 시간 동안 강연 후에 이것에 대해 더 자세히 말씀드릴 수 있지만, 아이디어는 저희가 수행하는 일종의 희소성 정규화가 있고, 제가 언급한 충분한 가변성과 실제로 변하는 개념으로 스스로를 제한한다는 사실 때문에, 제가 원했던 그 순열 및 스케일링 불확정성까지 개념 변화 델타와 선형 혼합 함수 D를 추정할 수 있다는 것입니다. 네. 따라서 정리 명제는 L1 또는 L0에 관한 것입니다. 그것은 L0에 관한 것이지만, 실제로 저희는 L1을 사용합니다. 네.
좋아요. 그럼 다시 희소 오토인코더처럼,
그렇다면 이 시나리오에서 어떻게 할 수 있을까요? 왜냐하면 다중 가설 검정의 경우가 있기 때문입니다. 행렬의 열에 대한 다중 가설이 있습니다. 찾은 방향이 연속적으로 유의미하다는 것을 어떻게 확인할 수 있습니까?
그럼 실험이 무엇인지, 실험 프로토콜이 무엇인지 설명드리겠습니다. 하지만 제 질문을 더 넓게 해석하자면, 순열 후에도 여전히 불확정성이 남아 있는데, 디코더의 어떤 열이 어떤 개념에 해당하는지 어떻게 알 수 있느냐는 질문으로 이해하겠습니다. 그렇게 해석하고 싶으신가요? 또한 어떤 열이 해당 열과 관련하여 통계적으로 유의미한지 알고 싶습니다. 왜냐하면 방향적으로 표현되지만 유의미하지 않은 일부 열을 찾을 수도 있기 때문입니다.
알겠습니다. 그럼 이렇게 말씀드리겠습니다. 스케일링 불확정성이 있습니다. 따라서 저희가 말할 수 있는 것은 디코딩 행렬의 열들이 조작 방향과 평행할 것이라는 것뿐입니다. 그렇죠? 즉, 실제로 개념을 이동시키고 싶다면, 매우 매우 큰 크기로 조작 벡터를 적용해야 할 수도 있습니다. 그것은 저희가 평가 시에 여전히 실험하고 파악해야 할 것입니다. 하지만 이 분리가 유지되고 이제 분리된다는 것이 무엇을 의미하는지 정확히 밝혔습니다. 순열 및 스케일링까지의 지상 진실과의 이러한 동등성을 가져야 합니다. 저희가 말할 수 있는 것은 디코딩 행렬의 열들이 평행할 것이라는 것입니다. 각 열은 하나의 조작 방향에 해당하거나 하나의 조작 방향과 평행할 것입니다.
네. 정리는 L0 문제의 전역 최적값에 관한 것인가요, 아니면 실제로 찾는 지역 문제의 해에 관한 것인가요? 전역 최적값에 관한 것입니다. 네. 그래서 이것은 다시 훌륭한 질문입니다. 왜냐하면 식별 가능성 결과의 특징은 무한 데이터 상황에서 일어나는 일에 관한 것이기 때문입니다. 그것은 정말로, 아시다시피, 여러분이 접근할 수 있는 일부 분포가 여기 있습니다. 이 분포에서 원칙적으로 실제로 어떤 추론을 할 수 있습니까? 실제로 여러분의 실제 최적화기가 매우 다른 해를 찾을 수도 있지만, 어떤 의미에서 인과성은 이러한 종류의 결과를 정말 좋아합니다. 왜냐하면 이 분포가 여러분이 원하는 답을 얻는 것을 허용하지 않는다면, 추정을 계속해서는 안 된다고 알려주기 때문입니다. 그것은 절망적입니다. 좋아요, 저희는 라마 3.1 임베딩으로 몇 가지 평가를 수행했습니다.
그리고 저희는 무엇을 했습니까? 그것은 앞서 말씀드린 것과 같습니다. 그렇죠? 그것은 저희가 테스트 시간에 단일 개념 변화에 접근할 수 있고, 저희 디코더의 열 중 하나가 조작된 임베딩을 얼마나 잘 예측할 수 있는지 볼 것입니다. 좋아요. 따라서 저희는 여전히 검색을 해야 하고, 여기 아래에 있는 여러 데이터 세트가 있습니다. 휴식 시간 동안 저나 시가 여러분과 이야기할 수 있습니다. 하지만 주목해야 할 핵심은 희소 오토인코더 또는 희소 변화 오토인코더(죄송합니다)의 훈련 시간에 저희는 다중 개념 변화 데이터 세트를 가지고 있으며, 단일 개념 변화로 훈련하지 않고 단일 개념 변화로 평가한다는 것입니다. 여기서 저희가 발견한 것은, SSAE가 훨씬 더 잘 수행하는 많은 다른 방법들이 있다는 것입니다. 조작 방향을 학습하기 위해 대조 쌍과 같은 훈련 데이터를 사용하거나 희소성 정규화를 하지 않는 이러한 방법들이 많습니다. SAE와 관련하여 저희가 발견한 것은 저희 방법이 SAE와 매우 유사하게, 매우 경쟁적으로 수행하고, SAE가 이전에는 매우 나쁘게 수행했던 곳에서 저희가 훨씬 더 잘 수행한다는 것입니다. 저희는 이점을 유지합니다. 음, 평가, 즉 이것을 확장하고, 액스벤치와 같은 벤치마크에서 평가하고, 실제로 일부 완성을 생성하여 완성이 얼마나 잘 보이는지 확인하는 측면에서 저희가 해야 할 일이 여전히 많습니다. 하지만 현재로서는 핵심은 그것들이 SAE와 경쟁적이고, 이전에는 SAE가 작동하지 않았던 곳에서 훨씬 더 낫다는 것입니다. 좋아요. 음, 거의 다 끝났다고 말씀드리게 되어 매우 기쁩니다. 따라서 질문에 대한 시간이 충분히 있을 것입니다. 음, 저는 어떤 뜨거운 논쟁으로 끝내지 않을 것입니다. 오히려 저희의 모든 불안한 시스템을 진정시키고 싶고, 저희는 숲으로 끝낼 것입니다. 매우 아름다운 숲입니다. 제가 직접 찍은 사진입니다. 음, 왜일까요? 잠시 동안 나무가 아닌 숲을 보고 싶기 때문입니다. 그렇죠? 음, 이 강연의 요약은 무엇일까요?
저는 제가 연구하거나 오랫동안 연구해 온 인과 표현 학습과 LLM의 해석 가능성 및 조작에서 발생하는 문제(이는 인간 목표와 일치하도록 그들의 행동을 제어하는 것과 분명히 관련이 있습니다) 사이의 연결 고리를 만들고 싶습니다. CRL은 좋은 표현의 증명 가능한 복구를 위한 많은 다른 전략들을 통합하는 분야입니다. 여기서 좋다는 것은 독립적인 구성 요소 자체가 고립적으로 개입 가능하다는 것을 의미합니다. 오늘 제가 이야기하지 않았지만, CRL에는 이미 이러한 인과적 특징, 이러한 인과적 변수 및 그것들에 대한 인과적 모델을 동시에 학습하는 것에 대한 많은 연구가 있습니다. 어느 정도까지는, 지금은 느슨하게 말해서, 그다지 나쁘지 않은 불확정성까지요. 그리고 지난 며칠 동안, 조작뿐만 아니라 계획 및 추론에도 도움이 되는 인과적 모델을 발견하는 것에 대한 많은 관심이 있다는 것을 듣게 되어 매우 기뻤습니다. 그리고 저는 매우 기쁘고, CRL이 다른 사람들이 논의한 이러한 문제들에 매우 좋은 출발점을 실제로 제공할 수 있는 방법에 대해 더 많이 이야기하고 싶습니다. 그럼, 여러분의 인내심과 경청에 감사드리며, 질문이 있으시면 기꺼이 답변하겠습니다.
[박수갈채]
네, 정말 좋은 강연이었습니다. 정말 감사합니다. 음, 원래 SAE의 많은 용도들은 완전히 비지도 방식으로 이루어졌던 것 같은데, 여기서는 승리를 거두었지만 이 희소 변화 데이터가 있어야 합니다. 그렇죠? 그렇다면 그것이 얼마나 큰 제한이라고 생각하시나요? 어떤 사용 사례를 쉽게 다룰 수 있을 것이라고 보시나요? 어떤 것들이 더 멀리 떨어져 있나요? 음, 제 생각에는 훌륭한 질문입니다. 제 생각에는 식별 가능성에 대해서는 공짜 점심은 없다는 일종의 것이 있다고 생각합니다. 어떤 의미에서는 약간의 대가를 치르고 식별 가능성을 얻는 것입니다. 그리고 생성 프로세스 자체에 귀납적 편향이나 제약을 추가할 수도 있습니다. 이는 때때로 수행되는 것입니다. 혼합 함수를 제한할 수도 있습니다. 이것은 이미 결과를 제공합니다. 또는 비ID 데이터를 얻을 수도 있습니다. 음, 제가 아마도 희망하는 것은 조작을 위해 매우 매우 좋은 대조 쌍을 사용하는 데 정말로 초점을 맞춘 많은 연구가 있었다는 것입니다. 따라서 저는 이것을 계속 밀고 나가고 싶습니다. LLM을 사용하여 일종의 합성 반사실적 예시 생성을 할 수 있다면 어떨까요? 이전에는 충실한 반사실적 사례를 생성하지 않았기 때문에 말이 안 되었지만, 이제는 그럴 필요가 없습니다. 실제로 하나 이상의 개념이 섭동되고 있다는 것을 어느 정도 수용할 수 있습니다. 따라서 그것이 아마도 제가 할 만한 곳일 것입니다.
양손이 동시에 올라갔네요. 알겠습니다. 음, 네, 제 질문은 문제 1에 관한 것입니다. 당신은 희소 오토인코더에게 단사 함수가 문제가 되었다고 말했습니다.
그렇다면 단사 함수 문제도 있는 것인가요? 아니요, 아니요, 그래서, 무슨 일이 일어나냐면, 토큰 임베딩과 관련하여 단사 함수가 아니었던 그 모든 개념들은 변화를 고려할 때 상쇄됩니다. 따라서 실제로 저희의 실제 이론에서 저희가 해야 할 것은, 이제 데이터 세트에서 실제로 변화하는 개념에서 토큰 임베딩으로의 매핑이 단사 함수라는 가정을 해야 한다는 것입니다. 하지만 요점은 저희가 그것이 훨씬 더, 아시다시피, 정당화할 수 있는 입장에 있다고 생각한다는 것입니다. 음, 시바, 그리고 제프리. 네, 훌륭한 연구입니다.
네.
그래서 이것은 주 네트워크와는 별개의 구성 요소처럼 보입니다. 그렇죠? 여러분은 여러분의 트랜스포머 언어 모델을 가지고 있습니다. 무엇이든 만들 수 있습니다. 네. 그런 다음, 이러한 인과 관계를 밝혀낼 수 있는 별도의 모델을 구축하고 있습니다. 하지만 이 두 가지를 결합하여 실제로 100%를 달성할 수 있다고 생각하시나요? 훌륭한 질문입니다. 음...
따라서 SAE를 사용하여 희소 특징을 학습하는 매우 훌륭한 최근 논문이 있었고, 그들은 일종의 회로 발견을 수행했으며, 이전보다 훨씬 더 희소하고 해석 가능한 회로를 발견했습니다. 따라서 이러한 두 가지 패러다임을 결합할 수 있는 기회가 있습니다. 즉, 트랜스포머에 의해 학습된 개념 또는 임베딩을 풀어내고, 그 풀어낸 공간은 훨씬 더 해석 가능합니다. 섭동 및 개입을 시뮬레이션하거나, 예를 들어 특징을 학습한 다음, 디코딩 함수를 사용하여 트랜스포머로 다시 매핑하는 것과 같은 작업을 수행할 수 있습니다.
네. L1 버전의 정리가 얼마나 멀리 떨어져 있나요? 원래 압축 센싱 결과는 L1에 대한 것이고, 나머지 부분은 볼록합니다. 따라서 그것을 따라갈 수 있을까요?

음, 네, 맞아요. 압축 센싱에 대해서는 완전히 맞아요. 음, 정말 정말 좋은 질문입니다. 음, 지금 당장은 답이 없습니다. 생각해 봐야 할 것 같아요. 아마 시도 몇 가지 생각을 가지고 있을 거예요. 음, 네, 조슈아, 제 질문은 약간 벗어나 있고 철학적입니다.
그래서 왜 저희가 이걸 하려고 하는 걸까요? 그리고 저는 앤트로픽의 동기는 저희가 언어 공간에서 그 니츠를 신뢰하지 않기 때문이라고 생각합니다.
음, 그래서 저희는 출력 측면에서 보이지 않는 것을 읽으려고 노력하고 있습니다. 네.
하지만 개념을 분리하는 것에 대해 질문하신 내용에 집중한다면, 이미 그를 위한 공간이 있습니다. 언어를 위한 공간입니다. 네. 네.
그래서 제 생각에는 분리가 되어 있는 것 같습니다. 문제는 언어가 저희가 예상하는 의미와 일치하지 않는 방식으로 사용될 수 있다는 기만 또는 사실입니다.
완전히 동의합니다. 하지만 한 가지 생각은, 언어의 문제를 잠재 변수로 매우 진지하게 받아들이고, 그럼에도 불구하고 저희가 직면해야 할 수도 있는 어떤 불확정성이 있는지 묻는 것이 흥미로울 것이라고 생각합니다. 아마도 모든 토큰의 의미를 분리하고 싶어하는 것이 아니라, 말씀하신 것처럼 이미 분리되어 있습니다. 하지만 식별 가능성 이론의 도구가, 작은 문장으로 표현된 여러 후보 이론이 있을 때조차도, 이 데이터를 가장 잘 설명하는 최상의 이론을 얻기 위해 저희가 적용할 수 있는 어떤 종류의 귀납적 편향이 있는지 묻는 데 도움이 될 수 있을까요? 제 생각에는, 아시다시피, 단순성이 하나이지만, 희소한 변화와 이러한 종류의 아이디어 측면에서 다른 것들이 있을 수 있습니다. 네. 네. 음, 희소 오토인코더와 이것을 잘 대조해 주셨습니다. 네. 하지만 개념 병목 모델과 같은 더 지도적인 방법과 이것을 대조하는 데 도움을 주실 수 있는지 궁금했습니다. 음, 개념 병목 모델에는 익숙하지 않습니다.
그래서 음, 논문을 읽고 다시 연락드리겠지만, 음, 그것은 아마도 연구 분야 전체일 것입니다. 제 생각에는, 음, 지도적인 방법, 죄송합니다, 그것은 연구 분야 전체이지 단일 보상이 아닙니다. 하지만 물론, 음, 그럼 저는 여기 서서 제가 그 연구 분야에 익숙하지 않다는 것을 인정해야 합니다. 하지만 제가 말씀드릴 수 있는 지도적인 방법은, 원하는 종류의 완성의 긍정적인 예시가 있거나, 단일 개념으로 달라지는 프롬프트의 긍정적인 예시가 있다면, 평균 임베딩의 차이와 같은 것을 사용하여 조작 방향을 얻을 수 있다는 것입니다. 음, 여기서 제가 말씀드릴 수 있는 것은 목표가 그러한 방법을 이기는 것이 아니라, 훨씬 저렴하게 얻을 수 있는 데이터를 사용하는 대안을 제시하는 것입니다. 음, 다른 질문은 휴식 시간으로 미루는 것이 좋겠다고 생각합니다. 저희에게는 15분밖에 남지 않았습니다. 음, 그럼 15분 후에 뵙겠습니다. 완벽합니다. 감사합니다.
그래서 물류에 대해 몇 가지 말씀드리겠습니다. 음, 몇 가지 업데이트가 있습니다.
