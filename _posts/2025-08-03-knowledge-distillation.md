---
layout: post
title: Knowledge Distillation - Build Smaller, Faster AI Models
date: 2025-08-03
categories: [artificial intelligence]
tags: [artificial general intelligence]

---

# [AWS AI and Data Conference 2025 â€“ Knowledge Distillation: Build Smaller, Faster AI Models](https://www.youtube.com/watch?v=_5hcukWw2rA)


## Abstract

Knowledge distillation transfers capabilities from large language models to smaller, faster models while maintaining performance. Organizations can achieve dramatic improvements in throughput and cost efficiency. Learn how to implement distillation using Amazon Bedrock or to build a custom solution on Amazon SageMaker. Julien Simon will showcase how Arcee AI uses distillation to develop industry-leading small language models (SLMs) based on open architectures. He will also introduce the open-source DistillKit library and demonstrate several newly distilled SLMs from Arcee AI.

### Speakers:
Laurens van der Maas, Machine Learning Engineer, AWS
Aleksandra Dokic, Senior Data Scientist, AWS
Jean Launay Orlanda, Engagement Manager, AWS

Learn more about AWS events: [https://go.aws/events](https://go.aws/events)


<iframe width="600" height="400" src="https://www.youtube.com/embed/_5hcukWw2rA?si=0uC6_tsnUVgWNmt5" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>