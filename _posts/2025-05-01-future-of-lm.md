---
layout: post
title: The Future of Language Models; A Perspective on Evaluation  
date: 2025-05-01
categories: [artificial intelligence]
tags: [artificial general intelligence]

---

# [The Future of Language Models: A Perspective on Evaluation](https://www.youtube.com/watch?v=wcPRW2YOqkA)

* Swabha Swayamdipta (University of Southern California)

## 주요 내용


이번 세미나는 **대규모 언어 모델(LLM) 평가의 현재 상황과 미래 방향**에 대한 심도 깊은 논의를 다루었습니다. 주요 핵심 내용은 다음과 같습니다.

### **1. 기존 평가 방식의 한계 및 재고찰의 필요성:**

* 현재 LLM 평가는 주로 과거 자연어 처리(NLP) 작업에서 사용되던 분류 기반의 정확도, F1 점수 등에 의존하고 있으며, 이는 LLM의 발전된 능력과 장문 생성 능력 등을 제대로 반영하지 못합니다.
* 기존 벤치마크들은 주로 정답이 명확한 분석적, 논리적 작업에 치우쳐 있으며, 실제 세계의 복잡하고 주관적인 작업을 제대로 평가하지 못합니다.
* 테스트 세트의 공개로 인한 모델 개발자들의 의도적인 또는 무의식적인 편향 가능성이 존재하며, 검증 세트 부족 또한 문제입니다.
* 일부 벤치마크의 주제 편향(STEM 중심) 및 검증의 어려움으로 인해 중요한 비 STEM 영역의 평가가 소홀히 되고 있습니다.

### **2. 실제 세계 작업 기반 평가의 중요성 및 어려움:**

* LLM의 실제 효용성을 평가하기 위해서는 실제 이해관계자(인간)와 실제 위험이 걸린 실제 세계 작업에 직접 적용하여 평가하는 것이 중요합니다.
* 이는 노숙자 관련 트윗 분석, 자살 보고서 분석, 응급 구조 가이드 자동화 등의 사례를 통해 제시되었습니다.
* 실제 세계 작업 기반 평가는 인간 평가자의 개입, 시간 및 자원 소모, 주관성 등의 어려움을 수반합니다.
* 인간 평가자 또한 LLM의 응답에 의해 편향될 수 있으며, 특히 해당 분야 경험이 부족한 경우 더욱 그렇습니다.

### **3. 합성 데이터 및 자동 평가의 활용 가능성:**

* 모든 작업에 실제 세계 기반 평가를 적용하는 것은 현실적으로 어렵기 때문에, 텍스트 단순화와 같이 LLM의 생성 능력을 활용하여 합성 데이터를 구축하고 자동 평가를 적용하는 것이 유효할 수 있습니다.
* 작업의 성격과 중요도, LLM의 해당 작업 능력에 대한 이해를 바탕으로 적절한 평가 방식을 선택해야 합니다.

### **4. 장문 생성 평가의 어려움 및 새로운 접근 방식:**

* 장문 생성의 경우 정답(ground truth)이 존재하지 않아 평가가 더욱 어렵습니다. 인간 평가의 주관성, 전문가 부족 등의 문제가 발생합니다.
* 챗봇 아레나와 같은 플랫폼을 통한 대규모 인간 평가가 이루어지고 있지만, 여전히 어려움이 존재합니다.
* 생성된 텍스트와 인간이 작성한 텍스트 간의 격차 측정, 모델 간 생성 결과의 분리 가능성(separability) 분석 등 새로운 평가 방법론이 제시되었습니다.
* 분리 가능성 개념을 활용하여 인간 평가에 더 적합한 인스턴스를 선별하고, ELO 평점 시스템을 개선하는 방안이 모색되었습니다.
* 모델의 가변성을 활용하여 더 나은 기준선(baseline)을 구축하는 시도가 소개되었습니다.

### **5. 평가 대상의 전환: 모델 내부 구조 기반 평가의 탐색:**

* 기존 텍스트 기반 평가의 어려움을 극복하기 위해 모델의 출력 확률, 출력 점수, 최종 매개변수 등 내부 구조를 활용한 새로운 평가 방식이 탐색되고 있습니다.
* 언어 모델 서명(signature) 분석을 통해 모델의 특징을 파악하고, 이를 평가에 활용하는 아이디어가 제시되었습니다.
* 성능 외에도 공격 취약성과 같은 새로운 비교 축을 제시하고, 모델의 잠재적 표현(skills)이나 임베딩 레이어 속성 등을 활용하는 방안이 논의되었습니다.

**결론적으로,** 세미나는 LLM 평가가 직면한 다양한 문제점을 지적하고, 더 나은 평가를 위해서는 창의적이고 다각적인 접근 방식이 필요함을 강조했습니다. 실제 세계 작업 기반 평가의 중요성과 함께, 합성 데이터 및 자동 평가의 전략적 활용, 그리고 모델 내부 구조를 분석하는 혁신적인 시도들이 미래 LLM 평가의 중요한 방향이 될 수 있음을 시사했습니다.

<iframe width="600" height="400" src="https://www.youtube.com/embed/wcPRW2YOqkA?si=nD5YK6u7rJkXnROD" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>


## 세미나 세부 내용

오후에 두 번의 발표가 더 있습니다. 다음 발표자는 워크숍 공동 주최자인 수아바입니다. 그녀는 언어 모델과 트랜스포머의 미래를 위한 평가에 대한 관점을 이야기할 것입니다.

사샤에게 감사드리고, 늦게까지 남아주신 모든 분들께 감사드립니다. 네, 저는 평가에 대해 이야기할 것입니다. 이야기 하나로 시작해 보겠습니다. 아주 먼 옛날은 아니지만, 자연어 처리라는 분야가 있었습니다. 기계 번역, 텍스트 요약, 의미적 동등성, 공지시어 해결, 질의 응답과 같은 작업들이 있었죠. 그러다 거대 언어 모델이 등장하면서 이 모든 작업들을 포괄하게 되었고, 자연어 처리 분야는 오늘날까지 이어지는 존재론적 위기에 빠졌습니다. 매우 강력한 모델들이 등장했지만, 당시의 파이프라인과 현재의 파이프라인을 비교해 보겠습니다. 과거에는 제한된 데이터로 작은 모델들을 학습시키고, 작은 테스트 세트에서 모델의 예측을 평가했습니다. 하지만 현재는 엄청난 양의 데이터로 매우 큰 언어 모델들을 학습시키고, 이 모델들은 긴 형식의 텍스트를 생성할 수 있습니다. 과거에는 매우 성공적으로 수행할 수 없었던 작업이죠. 하지만 평가에 있어서는 거의 동일한 방식으로 이루어집니다. 모델이 일부 테스트 예시에 할당한 레이블을 보고, 정확도나 F1과 같은 평가 지표를 사용합니다.

이러한 분류 스타일의 평가 방식은, 이 발표를 통해 주장하고 싶은 점인데, 평가의 마지막 구성 요소가 실제로 매우 중요합니다. 흔히들 "측정할 수 없는 것은 개선할 수 없다"고 말합니다. 따라서 평가 또는 측정 방식을 개선하지 않으면 언어 모델을 실제로 개선할 수 없습니다. 이 인용구는 켈빈 경의 말에서 비롯되었지만, 그의 정확한 인용은 훨씬 더 복잡합니다. "측정할 수 없을 때, 당신의 지식은 빈약하고 불만족스러운 종류의 것입니다. 그것은 지식의 시작일 수 있지만, 당신은 거의..." 와 같이 훨씬 더 미묘한 의미를 담고 있습니다. 또 제가 가장 좋아하는 인용구 중 하나인데, 직접적인 인용은 아니지만 클레어 카디 코넬대 교수님이 EMNLP 2020 기조연설에서 하신 말씀입니다. 90년대에는 정보 추출이나 문서 이해와 같은 언어 문제가 모델에게 매우 어려웠다고 합니다. 그래서 우리는 이러한 문제들을 텍스트 요약이나 의미적 동등성과 같은 작은 작업들로 나누고, 이러한 작업들을 쉽게 평가할 수 있도록 평가 지표를 만들었습니다. 하지만 현재 NLP 모델은 매우 강력함에도 불구하고, 여전히 이러한 더 간단한 작업들로 평가하고 있습니다. 그리고 명심하세요, 이는 모델이 지금처럼 크고 강력하지 않았던 2020년의 이야기입니다. 따라서 이러한 언어 모델이 훨씬 더 많은 능력을 가지고 있음에도 불구하고, 여전히 동일한 평가 방식을 따르고 있다는 점이 흥미롭습니다.

언어 모델 평가에 대한 제 연구를 대략적으로 분류하면 다음과 같습니다. 평가하는 데이터, 평가자, 모델을 평가하는 작업 또는 기술, 그리고 평가에 사용하는 기준 또는 지표를 고려해야 합니다. 네.

네, 벤치마크나 논문 벤치마크처럼 훨씬 더 복잡한 평가, 예를 들어 자율적으로 코드를 작성하고 유닛 테스트를 실행하는 것과 같은 평가를 하지 않는 이유를 설명해 주시겠어요? 네, 저는 우리가 맞추려고 하는 일종의 예상되는 행동이 있는 평가 부분, 즉 이 예상되는 행동이 이 유닛 테스트를 통과해야 하는 부분을 이야기하고 있습니다. 모델은 우리가 예상하는 특정 최종 목표에 도달하기 위해 많은 복잡한 작업을 수행할 것이고, 우리는 그것이 얼마나 잘 일치하는지 보려고 합니다. 저는 예상되는 행동과 일치시키는 마지막 구성 요소에 대해서만 이야기하고 있습니다. 그 목표에 도달하기 위해 따르는 과정이 매우 다르다는 점은 전적으로 옳습니다. 그리고 이 발표 전체에서 제 주장은 우리가 그 과정을 평가하지 않고, 그 마지막 부분만 평가하고 있다는 것이며, 이는 문제가 될 수 있습니다. 알겠습니다. 평가 데이터에 관해서는, 데이터가 어디에서 왔는지, 합성 데이터인지, 아니면 인간이 수집하고 특정 수준의 어려움을 유지할 수 있도록 인간이 큐레이션한 데이터인지와 같은 질문들이 있습니다. 테스트 세트의 크기는 어느 정도인지, 테스트 세트의 품질은 어떤지, 학습 세트와 얼마나 겹치는지 등 고려해야 할 질문들이 많습니다. 그리고 이 워크숍의 많은 발표에서 이러한 데이터 관련 질문의 일부를 학습 측면에서 다루었습니다. 제 연구 중 일부는 데이터를 평가 측면에서 살펴봅니다. 평가자는 우리가 그다지 자세히 논의하지 않은 부분인데, 누가 평가를 수행하는가 하는 문제입니다. LLM 심판인지, 아니면 전문가인 인간인지. 예를 들어 MMLU 벤치마크와 같이 모델이 맞추려고 하는 예상되는 행동이 있는 대부분의 경우 평가자의 특별한 필요는 없습니다. 하지만 개방형 생성에서는 평가자가 필요합니다. 그리고 작업 또는 기술에 관해서는, 정답이 없는 경우, 가능한 답이 많은 경우, 또는 예상되는 정답과 비교할 특정 변수를 최종적으로 예측해야 하는 긴 형식의 응답을 요구하는 경우가 있습니다. 그리고 기준 지표도 여기에 해당합니다. 언어 모델 평가에서 매우 드문 것 중 하나는 적절한 기준선이 존재하지 않는다는 것입니다. 분류 작업에서는 좋은 랜덤 기준선이나 최빈 클래스 기준선이 있었지만, 생성된 언어의 경우에는 이러한 기준선을 구축하기 어려울 수 있습니다. 따라서 이를 고려해야 하며, 발표의 첫 번째 부분에서는 이러한 작업과 평가자를 살펴볼 것입니다. 두 번째 부분에서는 데이터와 기준선을 살펴볼 것입니다. 그리고 발표의 마지막 부분은 몇 장의 슬라이드로 구성될 텐데, 모델 내부 구조를 기반으로 언어 모델 평가를 수행하는 매우 엉뚱한 아이디어에 대한 탐구를 여러분과 논의해 보려고 합니다. 이는 매우 탐색적인 연구입니다. 좋습니다. 넘어가기 전에 질문 있으신가요? 좋습니다. 첫 번째 부분은 평가 벤치마크 재고찰입니다.

대규모 언어 모델 평가에 대한 설문조사를 보면, 2023년 후반의 매우 유명한 설문조사인데, 대부분의 내용이 벤치마크에 관한 것입니다. 이 모든 것들은 평가를 수행하기 위해 사람들이 어떤 식으로든 수집한 벤치마크의 예입니다. 그리고 이 데이터들을 큐레이션하고 이러한 벤치마크를 구축하는 데 많은 시간과 노력이 필요하기 때문에 이는 매우 중요한 작업입니다. 일부는 합성 데이터이고, 일부는 기존 자원에서 큐레이션되었습니다. 그리고 언어 모델 평가라고 생각할 때 우리는 새로운 벤치마크를 만들고, 이 워크숍 발표에서 백만 번이나 보셨을 MMLU, Alpaca, GPQA, Big Bench, GSM 8K 등과 같은 현재 사용되는 벤치마크를 생각합니다. 이러한 벤치마크의 대부분 또는 거의 모든 벤치마크는 언어 모델이 맞춰야 할 예상되는 정답을 가지고 있습니다. 매우 유명한 것 중 하나는 "인류 최후의 시험"인데, 언어 모델이 잘하지 못할 가장 어려운 것으로 생각하여 만들어졌습니다. 그리고 인상적인 두 페이지 분량의 저자 목록을 가지고 있으며, 주제를 보면 대부분 수학입니다. 일부는 물리학, 생물학, 의학, 컴퓨터 과학, 공학, 화학, 인문학, 사회 과학이며, 기타는 전체 데이터의 약 18%를 차지합니다. 이러한 분포를 보면, 여기에 일종의 선택 편향이 있는 것은 아닌지 의문을 가져야 할 수도 있습니다. 우리는 모두 컴퓨터 과학 분야의 사람들이고, 이러한 STEM 주제에 많은 초점을 맞추고 있을지도 모릅니다. 왜 STEM이 아닌 다른 주제들은 그렇게 작은 부분을 차지해야 할까요? 간단히 말해서, STEM이 아닌 이러한 작업들은 검증하기 어렵기 때문에 일반적으로 고려하지 않습니다. 그리고 이것들은 오늘날 가장 유명한 언어 모델에 대한 논문에 제시된 표의 예입니다. Llama, OpenAI 모델 (어떤 것인지는 잊었습니다), 그리고 DeepSeek가 있으며, 이들 대부분은 이전 슬라이드에서 본 것과 동일한 벤치마크를 사용합니다. 즉, 주로 코딩과 수학입니다. 아마도 이러한 작업들은 많은 분석 작업의 가장 중요한 구성 요소이며, 종종 "모델이 이러한 작업에서 잘 수행되면 다른 모든 작업에서도 매우 잘 수행될 것이다"라는 주장이 제기됩니다. 하지만 이는 우리가 의문을 가져야 할 부분입니다. 이러한 작업들이 충분하다고 말하기 전에, 모델이 다른 모든 작업에서 얼마나 잘 수행하고 있는지에 대한 증거를 찾아야 합니다. 이제 이러한 벤치마크에는 몇 가지 문제가 되는 추세가 있습니다. 대부분의 벤치마크는 테스트 세트만 공개합니다. 이것이 현대적인 방식이죠. 이제 이 테스트 세트는 모든 사람이 사용할 수 있습니다. 따라서 대규모 언어 모델을 구축하는 사람이라면, 그 테스트 세트에서 좋은 성적을 거두도록 장려됩니다. 따라서 그 테스트 세트와 매우 유사한 데이터를 살펴보는 방식으로, 즉 "약간의 부정행위"를 시도할 수 있습니다. 그렇게 하면 알고리즘이나 모델에 특별한 변화를 주지 않고도 이러한 벤치마크에서 점점 더 나은 점수를 얻을 수 있습니다. 이러한 벤치마크 중 상당수는 검증 세트가 없거나, 매우 적은 수의 검증 예시만 제공합니다. 검증 세트는 모델을 학습시키기 전에 더 내재적인 실험을 수행하는 데 사용될 것이고, 그 후에 실제 테스트를 수행할 것입니다. 따라서 사람들은 모델을 구축하는 동안 이러한 테스트 세트를 직접 사용하는 경향이 있습니다. 따라서 이러한 작업에 능숙한 모델에 대한 일종의 편향이 있을 수 있습니다. LM 개발자들은 벤처 투자자들로부터 이러한 벤치마크에서 점점 더 나은 수치를 제공하도록 평가받기 때문에, 언어 모델이 이러한 벤치마크에서 매우 잘 수행된다는 것이 실제로 무엇을 의미하는지 의문을 제기해야 합니다.

다른 시나리오를 상상해 봅시다. 실제 작업을 고려하는 것입니다. 언어 모델이 수학 추론, 코딩 알고리즘 및 많은 논리적 또는 분석적 작업에서 매우 잘 수행되고 있다는 증거가 있습니다. CLA의 영감을 받아, 저는 다음과 같은 질문을 던지고 싶습니다. 대담하게 나아가서 실제 이해 관계자, 실제 인간에게 실제 위험이 걸린 실제 작업으로 직접 테스트하는 것은 어떨까요? 가장 큰 단점은, 아마도 대부분의 사람들이 이 길을 가지 않는 이유인데, 이것은 인간을 필요로 한다는 것입니다. 느리고, 많은 수작업이 필요하며, 훨씬 더 많은 시간과 자원 등이 소요됩니다. 따라서 벤치마크처럼 한 번 구축하고 여러 번 테스트할 수 있는 것이 아닙니다. 이 프로세스는 약간 다를 것입니다. 따라서 발표의 나머지 부분, 다음 슬라이드의 일부 내용은 민감하거나 불쾌감을 줄 수 있는 내용을 포함할 수 있습니다. 미리 경고합니다. 좋습니다. 저는 USC에 있습니다. USC에는 Annenberg 커뮤니케이션 및 저널리즘 스쿨이 있고, 2023년 초에 동료인 린지로부터 AI 도움이 필요하다는 요청을 받았습니다. 그녀는 지역 정치인들이 노숙자에 대한 대중의 태도를 이해하고, 유권자들에게 가장 잘 전달될 수 있는 메시지를 구성하는 데 도움을 주는 문제를 연구하고 있었습니다. 그것이 문제였습니다. 당시 언어 모델의 힘에 취해 있던 저는 "문제없어, 식은 죽 먹기지. 린지, 몇 주 안에 끝내줄게"라고 호언장담했습니다. 그리고 학부생의 도움을 받아 작업을 시작했습니다. 작업은 다음과 같았습니다. 예를 들어 이런 종류의 트윗이 있다고 합시다. 잠시 읽어보세요.

2021년부터 2023년까지 노숙자에 대한 약 300만 또는 500만 건의 트윗을 수집했는데, 그중 250만 건만 사용할 수 있었습니다. 노이즈가 많았기 때문이죠. 하지만 각 트윗을 가져와서 언어 모델을 사용하여 트윗에 나타난 노숙자에 대한 대중의 태도를 나타내는 몇 가지 범주를 만들어내고 싶었습니다. 다시 말하지만, 이상적인 것은 이 트윗들을 가져와서 언어 모델에게 지역 정치인을 위한 메시지를 작성하도록 요청하는 것이었을 겁니다. 하지만 맥락이 너무 길어서 모델에 맞지 않는 문제가 발생했습니다. 그래서 언어 모델이 처리하기에 좀 더 manageable한 문제로 나누고 싶었습니다. 좋습니다, 이것이 과제입니다. 두 주가 지나고 7~8개월 후, 우리는 이 데이터 세트를 구축할 수 있었습니다. 이는 일종의 테스트 세트 역할을 하며, 인간 레이블을 가지고 있었습니다. 사회복지사들과 커뮤니케이션 전문가들, 그리고 제 컴퓨터 과학 학생들과 함께 약 4,000개의 트윗에 주석을 달고, 쉽지 않은 이러한 범주들을 만들어냈습니다. 우리가 본 이러한 레이블들은 쉽지 않았습니다. 이것은 9가지 다중 레이블 분류 작업이 되었습니다. 그 자체로 극도로 고통스럽고, 매우 겸손해지는 경험이었습니다. 좋은 소식은, 일단 이러한 범주들을 얻고 나니, 인간-루프 파이프라인을 구축하여 주석 작업 속도를 높일 수 있었고, 이를 통해 더 많은 트윗에 주석을 달 수 있었습니다. 그리고 이 모든 데이터를 수집한 후, 250만 건의 대규모 트윗 데이터 세트에 대한 예측 실험을 실제로 실행하여, 작은 테스트 세트에서 이러한 모델들이 인간과 비교하여, 또는 매우 작은 언어 모델을 학습시켰을 경우 어떻게 수행되는지 보여줄 수 있었습니다. 이러한 모든 시스템의 성능은 상당히 낮습니다. 매우 어렵고 주관적인 작업입니다. 9가지 다중 레이블 분류는 여전히 어려운 작업입니다. 그리고 그것은 매우 겸손한 경험이었습니다. 하지만 좋은 소식은, 언어 모델이 노숙자에 대한 온라인 태도를 결정하는 이러한 변수를 결정하는 데 도움이 될 수 있다는 것을 확인했다는 것입니다. 따라서 언어 모델은 사용될 수 있지만, 그것이 가장 유용한지는 우리가 더 논의해야 할 문제입니다. 하지만 우리가 또 다른 것을 발견했는데, 모델이 올바른 결정을 내리도록 안내하기 위해서는 인간이 개입해야 한다는 것입니다. 질문 있으신가요?

인간과 언어 모델의 결합. 그래서 언어 모델을 도움으로 사용할 때 발생하는 이러한 편향에 대해 어떤 생각을 가지고 계신가요?

네, 아주 좋은 지적입니다. 언어 모델을 사용할 때, 우리의 설정은 약간 달랐습니다. 언어 모델이 예측의 첫 번째 단계를 수행하고, 인간이 그것을 수정했습니다. 이는 본질적으로 시간을 절약하기 위해 언어 모델이 놓친 레이블을 추가하는 데 많은 노력을 기울이지 않았다는 것을 의미합니다. 우리는 단지 레이블을 수정했습니다. 따라서 정밀도는 높았지만, 재현율이 떨어졌습니다. 그래서 이렇게 하기가 어렵습니다. 하지만 만약 인간이 앉아서 다른 모든 레이블을 생각하고 모델의 예측을 수정했다면, 아마도 더 나은 결과를 얻을 수 있었을 것입니다. 여기서 데이터를 이해하려고 합니다. 이것이 커뮤니케이션 및 공공 정책에 영향을 미칠 수 있는 변수를 결정하기 위한 것이라고 말씀하셨죠? 그렇다면 커뮤니케이션 공공 정책에 영향을 미칠 수 있는 메커니즘이 특정 변수에 의해 매개변수화되어 있고, 그 변수들을 찾으려고 한다고 이해하는 것이 맞을까요? 아니면, 예를 들어 정밀도와 재현율 값, 즉 F1 값을 보여주고 계시는데, 어떤 레이블이 있다고 말씀하신 것인가요? 이해를 도와주시겠어요? 네, 좋은 질문입니다. 우리는 정치인을 위한 이러한 종류의 메시지를 만드는 커뮤니케이션 분야의 사람들과 함께 앉았습니다. 그래서 그들은 우리가 무엇을 찾고 있는지 알고 있었습니다. 예를 들어, 이러한 게시물에 정부 비판이 많다면, 메시지가 정부 비판을 다룰 수 있는 구체적인 방법이 있을 수 있습니다. 처음에는 매우 세분화된 변수들을 예측하려고 했지만, 너무 복잡해졌습니다. 하지만 이러한 레이블, 이러한 변수들은 커뮤니케이션 분야에서 일하는 사람들의 지침에서 나왔습니다. 따라서 이것이 최상의 변수가 아닐 수도 있지만, 우리가 가진 자원으로는 이것이 최선이었습니다. 예를 들어, 변수는 정부 정책 비판과 같은 것일 수 있습니다. 네, 맞습니다. 정확히 그렇습니다. 그것은 확실히 변수입니다. 알겠습니다.

전문가 또는 전문가 및 테스트로부터 주석을 얻는 방법에 대해 이야기했습니다.

그래서 우리의 테스트 세트는 전문가들이 분류 작업을 수행하는 것처럼 순전히 전문가들이 수집한 것입니다.

정확히. 네, 네, 네. 네. 그래서 전문가들이 만든 테스트 세트를 기준으로 측정됩니다. 알겠습니다. 이 컴퓨터 과학자가 사회복지 분야 사람들과 기꺼이 협력하려 한다는 소문이 돌았습니다. 그래서 사회복지대학원의 또 다른 동료인 존 블라즈니히가 연락해 왔습니다. "언어 모델로 몇몇 사람들을 돕고 있다고 들었는데, 매우 중요한 문제가 있습니다. 자살 개입을 위한 새로운 요인에 대한 제 가설을 검증하는 데 도움이 될 자살 보고서를 분석하는 데 도움이 필요합니다."라고 말했습니다. 그리고 네, 이것은 당시에는 언어 모델로 할 수 있을 것 같았고, 사회복지 분야 사람들과 협력한 경험도 있었기 때문에 이 프로젝트에 참여했습니다. 존이 관심을 가졌던 종류의 질문은 피해자들이 사망 전 며칠 동안 법률 전문가와 같은 비임상 인력과 얼마나 자주 상호 작용하는가 하는 것이었습니다. 그리고 이러한 종류의 상호 작용 또는 이러한 종류의 상호 작용 빈도를 아는 것은 존과 같은 사람들이 아마도 법률 전문가를 위한 새로운 개입 방법을 구축하여 자살 예방에 도움이 될 수 있도록 하는 데 도움이 될 수 있습니다. 그리고 그들이 가지고 있던 데이터는 제가 지금까지 본 것 중 가장 어려운 데이터 중 하나였습니다. 그들이 가지고 있던 많은 보고서를 읽었고, 대부분의 정보를 가렸지만, 예를 들어 사망 전 며칠 동안 피해자가 자녀와의 양육권 또는 양육비 소송을 겪었다는 정보 등을 보고 있었습니다. 그리고 이러한 종류의 증거를 보고서에서 얻으면, 일종의 암묵적인 법적 상호 작용이 있었다고 추론할 수 있습니다. 이러한 27만 건 이상의 자살 보고서 중 얼마나 많은 보고서에 법적 상호 작용의 징후가 있는지 확인하고 싶어했습니다. 이는 그러한 개입을 추구할지 여부를 결정하는 데 도움이 될 것입니다.

그래서 다시 말하지만, 수동 코드북 개발, 그리고 코드북은 기본적으로 주석 달기 지침입니다. 그것 자체가 그들이 하는 방식이었고, 이 사람들에 따르면 이 과정 자체가 영원히 걸립니다. 쉬운 과정이 아닙니다. 우리는 약 600개의 내러티브에 대해 이 작업을 수행했고, 저도 이 주석 작업의 일부를 직접 했습니다. 가장 어렵고 매우 어려운 작업이었으며, 데이터를 보기 위해서도 IRB 승인이 필요합니다. 따라서 이 데이터를 수집하는 것은 매우 어려웠지만, 언어 모델을 루프에 포함하여 이전 프로젝트와 매우 유사한 파이프라인을 구축할 수 있었습니다. 이 과정에서 엄청난 속도 향상을 얻었고, 이 파이프라인을 사용하여 새로운 주석을 얻는 데 몇 시간이 걸렸습니다. 그리고 저는 이것이 언어 모델이 살펴보는 것이 괜찮을 수도 있다고 생각합니다. 왜냐하면 인간이 이러한 자살 보고서를 너무 많이 읽어서는 안 되기 때문입니다. 실제 성능에 관해서는 약간의 개선을 보았습니다. 우리는 주석 달기 및 수정의 0단계에서 12단계로 진행되는 반복적인 프로세스를 거쳤습니다. 그리고 언어 모델이 전문가와 상호 작용하는 적절한 단계 수를 거치면, 전문가와 유사한 성능을 얻을 수 있었습니다. 그리고 이를 바탕으로 27만 건의 자살 보고서 중 약 10%에 법적 상호 작용의 징후가 있다는 것을 추론할 수 있었고, 이는 존과 같은 사람에게 매우 도움이 될 것입니다. 그리고 이것은 언어 모델로만 할 수 있었던 일입니다. 그래서 가장 좋은 일 중 하나는 해당 분야 외부의 협력자로부터 받는 피드백을 보는 것입니다. 존의 말을 정확히 인용하는지는 모르겠지만, 그는 그들의 연구가 총알처럼 빠르게 진행되는데, 우리는 제트기처럼 접근했다고 말했습니다. 따라서 그들은 이러한 종류의 도구를 사용하여 더 나은 모델을 구축하는 데 도움을 줄 수 있는 사람들을 정말로 고맙게 생각합니다. 네.

본질적으로 모델입니다. 또한 많은 학습이 모델이... 아, 네, 좋은 지적입니다. 네. 그래서 우리는 Llama와 같은 로컬 언어 모델만 사용할 수 있었습니다. GPD4, Claude 등을 이 작업에 사용할 수 없었습니다. 아뇨, 미세 조정은 전혀 없었습니다. 모두 제로샷이었습니다.

그래서 제로샷이지만, 그 후보 데이터가 벤치마크에 사용되었습니다. 정확히. 우리는 평가 세트를 수동으로 구축했습니다. 네. 그리고 또 다른 질문을 추가하고 싶습니다.

데이터는 로컬 모델에 민감합니다.

그렇죠? 하지만 그 중 많은 부분이 미세 조정되었고, 종종 데이터를 반복할 수 있습니다. 네. 글쎄요, Llama 3 70B에는 강력한 안전 장치가 없다고 생각합니다. 왜냐하면 이 문제를 겪지 않았기 때문입니다. 하지만 당신 말이 맞습니다. 네, 어떤 경우에는 그럴 수도 있지만, 우리는 엄청난 양의 프롬프트 엔지니어링 등을 수행했습니다. 따라서 올바른 프롬프트를 얻는 것조차 간단하지 않았습니다. 따라서 코딩 과정의 일부는, 죄송합니다, 이 파이프라인 과정에서 인간의 역할 중 하나는 언어 모델에 보낼 최적의 프롬프트를 파악하는 것입니다. 네, 그것은 우리가 해야 하는 매우 짜증나는 작업입니다. 네. 명확히 하자면, 이것이 귀납적 코딩 과정 또는 이 귀납적 코딩 과정의 자동화라고 말하는 것이 맞을까요? 네. 완전한 자동화는 아니라고 생각하지만, 일종의 반자동화된 과정이지만, 효율성을 높이는 데 확실히 많은 도움이 됩니다. 따라서 이러한 언어 모델을 평가해야 하는 방법 중 하나는 이러한 실제 작업을 수행하는 효율성을 살펴보는 것입니다. 그리고 네, 동일한 성능을 훨씬 더 빠르게 얻을 수 있는 방법이 무엇일까요? 그리고 확신하건대, 이 점에 대해 곧 말씀하실 테지만, 질문을 미리 던져두고 나중에 답변하셔도 좋습니다. 사회 과학에서 사람들이 귀납적 코딩을 할 때, 긍정적인 점은 물론 해당 영역에 대한 깊은 의미적 이해를 가지고 있다는 것입니다. 부정적인 점은 다른 사람들이 다른 귀납적 코드를 만들어낸다는 것입니다. 따라서 LLM을 통해 이 작업을 수행하면, 아마도 그러한 가변성의 일부가 사라지겠지만, LLM에는 확률적 특성이 있으며, 그것이 생성되는 코드의 고유한 특성을 부과할까요? 좋은 질문입니다. 두 슬라이드만 기다려 주세요. 네. 좋습니다. 또 다른 예입니다. 자세히 설명하지는 않겠습니다. 제 학부생 한 명이 저에게 왔습니다. 그녀는 자원 봉사 응급 구조대원이었습니다. 그녀는 프로세스의 일부를 자동화하기 위해 언어 모델을 사용하고 싶어했습니다. 분명히 응급 구조대원들은 이러한 작업을 많이, 수동으로 많이 수행합니다. 어떤 종류의 응급 상황이 발생할 때마다 올바른 대응을 찾기 위해 이 거대한 안내서를 뒤집니다. 그리고 우리는 여기서 언어 모델을 위해 그것을 세분화했습니다. 전문가인 인간은 없었고, 자원 봉사자인 리샤만 있었습니다. 그리고 우리는 이러한 영역에서 이러한 언어 모델이 얼마나 배포 가능한지 또는 얼마나 배포 준비가 되었는지에 대한 이해를 제공하는 어느 정도의 성능을 얻을 수 있었습니다. 결과적으로 배포 준비가 잘 되어 있지 않았습니다. 따라서 이러한 작업은 직접 평가하기 매우 어렵고, 언어 모델을 위해 세분화해야 합니다. 따라서 여기서 질문으로 돌아가서, 이 과정 전체에서 주석가인 인간 평가자들이 언어 모델의 응답에 약간 편향되는 여러 가지 예를 보았습니다. 아마도 이것이 당신이 지적한 바일 것입니다. 그리고 이것은 특히 주석가들이 해당 분야에서 제한된 경험을 가지고 있을 때 발생합니다. 전문가들은 언어 모델에 더 동의하지 않는 경향이 있습니다. 그러나 전문가들은 여전히 동의하는 경향이 있습니다. 하지만 두 번째 전문가를 데려오면, 그들은 서로의 편향을 약간 더 수정하고, 언어 모델이 제시했을 수 있는 문제점을 더 많이 알아차리는 경향이 있습니다. 따라서 이 모든 것을 말하자면, 이에 대한 경험적 결과는 없지만, 실제로 Microsoft 연구소의 사람들이 AI 사용이 비판적 사고에 영향을 미칠 수 있다는 것을 발견한 관련 연구가 있습니다. 이것은 매혹적인 연구입니다. 하지만 이것은 우리가 매우 조심해야 할 부분입니다. 왜냐하면 이것이 사실이고 우리가 주의를 기울이지 않으면, 인간 평가조차도 매우 편향된 체제로 빠질 수 있기 때문입니다. 좋습니다. 좋습니다. 하지만 이러한 정도의 노력이 항상 필요한가요? 우리는 궁극적으로 2025년에 있습니다. 모든 작업이 이러한 정도의 관심을 받아야 할까요? 그렇게 하면 2주마다 새로운 언어 모델을 출시할 수 없습니다. 따라서 전통적인 NLP 수업 작업에 관해서는, USC의 제 학부생이 "교수님, 항상 제 데이터를 보라고 하시는데, 텍스트 단순화 작업을 수행하면서 벤치마크가 완전히 불만족스럽다는 것을 알았습니다. 그래서 바꾸고 싶었습니다."라고 말했습니다. 네, 이것은 텍스트 단순화 작업과 같습니다. 복잡한 문장을 단순화해야 합니다. 이 두 문장을 읽어보면, 별로 단순화가 이루어지지 않았습니다. 다른 벤치마크는 훨씬 더 나쁩니다. 따라서 이러한 경우, 최소한 어느 정도의 단순화를 수행하는 합성 데이터 세트를 만드는 것이 완전히 괜찮을 수 있습니다. 언어 모델은 언어 생성에 매우 능숙합니다. 그리고 이러한 원래 데이터 세트에 대한 인간의 평가를 보면, 낮은 경향이 있지만, 이 합성 벤치마크에서는 인간의 합의가 약간 더 높은 경향이 있습니다. 그리고 이것은 언어 모델을 최대한 활용할 수 있는 또 다른 곳입니다. 우리는 LLM 레이터를 사용하여 인스턴스가 얼마나 복잡하거나 단순해야 하는지 파악할 수 있습니다. 그리고 일반적으로 그것은 예상되는 동작을 제공합니다. 작은 모델은 큰 모델보다 훨씬 성능이 떨어지는 경향이 있습니다. 그리고 그것이 우리가 예상하는 추세입니다. 따라서 합성 데이터와 자동 평가는 확실히 좋은 도구입니다. 하지만 작업이 무엇인지, 작업의 중요성이 무엇인지, 그리고 이 특정 작업에서 언어 모델의 좋은 기능에 대한 이해가 어느 정도인지 정말로 파악해야 합니다. 그리고 이것은 NLP 텍스트 단순화를 수행하는 사람으로서 제가 편안하게 할 수 있는 일이었습니다. 하지만 이것이 매우 단순한 말이라는 것을 이해합니다. 사람들은 반드시 항상 자신이 무엇을 하고 싶은지 아는 것은 아닙니다. 따라서 벤치마크 작업을 재고하는 것을 빠르게 요약하자면, 이러한 어려운 벤치마크를 구축할 때 작업 난이도에 대한 우리의 개념에는 한계가 있을 수 있습니다. 우리의 벤치마크가 실제로 언어 모델에 어려운지 보장할 수 없습니다. 하지만 이러한 언어 모델을 평가하는 더 나은 방법은 실제 작업에 적용하는 것일 수 있습니다. 정량적 평가는 어려울 수 있습니다. 인간의 전문 지식이 필요하며, 작업 자체를 더 작은 조각으로 나누어야 합니다. 몇 가지 벤치마크에만 과도하게 의존하면 잘못된 진전감을 줄 수 있습니다. 그리고 어떤 작업에 대해 많은 합성 생성, 평가 또는 검증을 수행할 수 있는지, 그리고 어떤 작업에는 그렇게 하지 않아야 하는지를 결정하는 것이 중요합니다. 이것들이 제 주요 결론입니다. 발표의 두 번째 부분으로 넘어가겠습니다. 평가 프레임워크에 대해 약간 이야기할 것입니다. 데이터 분포 및 생성 기준선에 대해 몇 가지 말씀드릴 내용이 있습니다. 여기 GBD2의 왼쪽 생성 예와 바로 어제의 Chad GBT 생성 예가 있습니다. 따라서 굳이 읽을 필요는 없지만, 가장 먼저 말할 수 있는 것은 문법적이라는 것입니다. 좋습니다. 그리고 이것은 어려웠습니다. 불과 몇 년 전만 해도 매우 어려웠습니다. 심지어 초기 버전의 GPT와 같은 것에서도 문법이 문제였고, 반복이 문제였습니다. 많은 문제가 있었지만, 전반적으로 문법적이라고 말할 수 있습니다. 따라서 언어 모델은 언어 또는 유창한 문법적 언어를 생성하는 데 탁월할 수 있지만, 우리는 여전히 고정되고 종종 고유한 답변을 가진 이러한 분류 작업으로 평가합니다. 따라서 긴 형식 생성에는 정답이 존재하지 않습니다. 따라서 생성 결과를 검증할 수 있는 범주가 없을 때 많은 문제가 발생합니다. 인간이 동의하지 않을 수도 있고, 전문가를 구할 수 없을 수도 있습니다. 언어 모델은 일반적으로 이러한 설정에서 두 모델의 생성을 서로 직접 비교합니다. 따라서 여기 Llama 논문의 인간 평가 결과가 있습니다. 주의 깊게 살펴보면 작업은 여전히 코딩, 추론, 수학 지식 탐색 작업과 같은 분석적 작업입니다. 따라서 긴 형식 생성으로 넘어갈 때, 분류 설정에서 수행하는 것과 동일한 작업을 여전히 살펴봐야 할까요? 그것은 우리가 정말로 의문을 가져야 할 부분입니다. 이러한 긴 형식 생성 평가를 수행하기 위해 다양한 플랫폼이 있습니다. Chatbot Arena는 유명한 플랫폼으로, 사람들이 자신의 모델을 호스팅하고, 사용자가 쿼리를 제공하면 다른 모델의 출력을 얻고, 각 출력에 대한 평점을 이 리더보드에 입력하여 많은 다른 사용자의 집계 평점을 통해 다양한 모델의 순위를 매깁니다. 이것은 대규모로 인간 평가가 수행되는 방식의 예입니다. 여전히 어려울 수 있지만, 오늘날 가장 신뢰할 수 있는 방법으로 간주됩니다. 이에 대해서는 긴 논의를 할 수 있지만, 그것이 긴 형식 생성 평가의 현황입니다. 그리고 이 방법을 원하지 않는다면, 다른 옵션은 자동 평가이며, 이에 대해 조금 살펴보겠습니다. 질문 있으신가요? 좋습니다. 좋습니다. 제가 생각하는 종류의 질문은 셰인, 캐머런, 또는 색슨 중 누가 궁극적인 화이트 로터스 알파인가 하는 것입니다. 화이트 로터스 팬 있나요? 몇 분 계시네요. 다행입니다. 슬라이드를 만들고 나서 이 사람들의 사진을 슬라이드에 넣지 않았으면 좋았을 걸 하고 생각했지만, 어쨌든 레딧에서 답변을 보면 농담이 정말 많고 너무 재미있어서 레딧에서 너무 많은 시간을 보냅니다. 클로드에게 같은 질문을 하면 매우 신중한 답변을 줍니다. 읽기에 전혀 재미가 없습니다. 읽고 눈을 굴리며 웃을 수는 있지만, 레딧 답변을 읽는 것이 더 좋습니다. 여기서 보시는 것은 어떤 답변을 선호하는지에 대한 저의 주관적인 생각이며, 이러한 종류의 결정이나 긴 형식 텍스트 평가에서의 주관성은 버그가 아니라 특징입니다. 이것이 우리 인간의 모습입니다. 당신은 이 답변을 좋아할 수도 있지만, 저는 아닐 수도 있습니다. 그렇죠? Z와 함께 이전에 했던 작업에 대해 간단히 언급하자면, 그것은 제가 가장 좋아하는 프로젝트 중 하나인데, 우리는 멋진 발산 측정 방법을 사용하여 생성된 언어와 인간이 쓴 언어 사이의 격차를 측정하는 지표를 만들었습니다. 이 지표는 언어 모델이 인간과 매우 다른 것을 하고 있다는 것을 완전히 알 수 있는 이러한 시나리오에서는 그다지 관련이 없을 수도 있습니다. 그리고 언어 모델이 인간과 매우 다른 것을 하는 것이 괜찮을 수도 있습니다. 하지만 궁금하시다면 저희 논문을 확인해 보세요. 텍스트 요약이라는 훨씬 더 평범한 질문으로 넘어가겠습니다. 문서가 있고, 모델에서 아마도 두 문장 또는 세 문장 요약을 만들고 싶을 것입니다. 그리고 이것은 긴 형식 생성을 평가하는 실제 작업입니다. 모델 A가 요약을 제공하고, 모델 B가 요약을 제공합니다. 어느 것이 더 나을까요? 대부분의 주석가에게 더 나은 것은 무엇일까요? 그리고 우리는 Amazon Mechanical Turk에서 크라우드 소스 주석가를 얻습니다. 이것은 매우 주관적이고 모호하며 불특정한 문제입니다. 무엇을 기준으로 더 나은 것일까요? 우리는 일반적으로 "어느 것이 더 마음에 드세요?"라고 묻습니다. 따라서 주석가들은 무엇을 해야 할지 모를 것입니다. 그들은 이러한 요약에 집중할 수도 있고, 다른 내용에 집중할 수도 있습니다. 어느 것이 더 나을까요? 매우 주관적입니다. 이제 모델 A와 B라는 두 개의 다른 모델이 있다고 가정하고 몇 가지 실험을 수행했습니다. 동일한 모델에서 더 많은 생성을 샘플링하면, 주어진 모델의 샘플 응답 내에서 많은 변동이 있을 수 있다는 것을 알 수 있습니다. 그리고 모델 B의 경우에도 마찬가지일 수 있습니다. 또는 모델 B에서 샘플링된 요약에는 변동이 훨씬 적을 수도 있습니다. 따라서 여기에는 두 가지 현상이 발생하고 있습니다. 내재적 유사성 또는 자기 정렬이라는 개념이 있습니다. 모델이 자신의 생성 내에서 더 많은 변동을 가질 수도 있고, 가지지 않을 수도 있습니다. 그리고 두 개의 다른 모델에서 나온 이 두 분포를 비교하는 교차 정렬 또는 외재적 유사성이라는 현상이 있습니다. 그리고 이러한 두 가지 다른 유사성 측정 방법은 인간 평가를 최대한 활용하는 방법을 결정하는 데 매우 중요합니다.

이것을 좀 더 구체화하기 위해, 두 모델의 생성 결과를 가져와서 birth 점수와 같은 것을 사용하여 두 모델의 생성 결과 간의 유사성을 계산하는 정렬이라는 개념을 정의합니다. 각 모델에서 k개의 생성을 샘플링하고 평균 유사성을 살펴봅니다. a와 b가 같으면 자기 정렬이라고 하고, 그렇지 않으면 교차 정렬이라고 합니다. 그리고 자기 정렬에서는 동일한 생성을 보지 않습니다. 여기서 i는 j와 같지 않아야 하고, j는 l과 같지 않아야 합니다. 그리고 두 모델 간의 최대 자기 정렬에서 교차 정렬의 개념을 뺀 분리 가능성이라는 개념을 정의합니다. 여기서 직관은 비교되는 두 모델의 출력 생성 결과 간에 얼마나 많은 교차 가변성이 있는지, 각 모델 자체의 출력 생성 결과 또는 자기 정렬 내에서 보이는 기준 수준의 가변성을 조정한 것입니다. 좋습니다. 질문 있으신가요?

좋습니다. 그러면 이것은 어떻게 보일까요? 주어진 벤치마크에서 분리 가능성 값을 플롯하면 됩니다. 이것은 대화 요약 벤치마크입니다. 모델 쌍에 따라 개별 인스턴스는 매우 다른, 매우 큰 분리 가능성 점수 분포를 갖는 것을 알 수 있습니다. 낮은 분리 가능성 인스턴스는 이렇게 보일 수 있고, 높은 분리 가능성 인스턴스는 이렇게 보일 수 있습니다. 따라서 높은 분리 가능성 인스턴스의 핵심 관찰은 인간 평가자가 어떤 모델이 다른지 또는 어떤 생성이 다른 생성과 다른지 알 수 있다는 것입니다. 더 좋거나 나쁘다는 것은 주관적일 수 있지만, 두 생성을 구별할 수 있습니다. 반면에 낮은 분리 가능성은 인간이 이러한 생성을 구별할 수 없는 경우입니다. 왜냐하면 서로 매우 유사하기 때문입니다. 좋습니다. 따라서 분리 가능성은 우리의 분리 가능성 개념을 사용하여 해당 인스턴스의 속성입니다. 따라서 많은 다른 모델 쌍에 대해 많은 다른 작업에서 분리 가능성의 이러한 분포를 계산하면, 다른 모델 쌍은 여기서 다른 분포를 갖는 것을 알 수 있습니다. 더 유사하지 않은 모델은 더 높은 평균 분리 가능성을 갖는데, 이는 예상되는 바입니다. 따라서 우리는 이 분리 가능성 개념을 사용하여 나머지 인스턴스에 비해 인간 평가를 위해 보낼 더 나은 인스턴스를 파악하려고 합니다. 좋습니다. 분리 가능성이 실제로 의미 있는 것인지 어떻게 알 수 있을까요? 분리 가능성 개념을 검증하기 위해, 우리는 인간 실험을 수행했습니다. 여기서 우리는 평가 일관성을 살펴보았습니다. 즉, 평가자가 동일한 모델에서 나온 다른 샘플에 걸쳐 매번 얼마나 충실하게 하나의 모델을 선택하는지를 살펴보았습니다. 따라서 더 높은 분리 가능성은 더 높은 평가 일관성과 일치해야 합니다. 그리고 그것이 우리가 포착하려고 했던 것입니다. 그리고 결과는 거의 동일했습니다. 여기서 우리는 다른 모델 쌍을 살펴보고, 몇 가지 다른 데이터 세트에서 이러한 모든 모델 쌍에 대한 생성에 대한 주석을 수집했습니다. 그리고 이것은 인간 평가자의 추세입니다. 궁금하실까 봐 말씀드리자면, 우리는 GBD3를 평가자로 사용하여 이것을 실행하기도 했습니다. 그리고 여기서 큰 편향을 볼 수 있습니다. GPD3는 항상 다른 모델보다 자신의 생성을 훨씬 더 좋아합니다. 그리고 이것은 훨씬 더 멀리 떨어진 데이터 세트 또는 모델 쌍에서 더 많이 발생합니다. 훨씬 작은 모델인 flan의 경우, GBD3는 항상 자신의 생성을 더 좋아합니다.

네

네. 그리고 이것은 훨씬 더 멀리 떨어진 데이터 세트 또는 모델 쌍에서 더 많이 발생합니다. 훨씬 작은 모델인 flan의 경우, GBD3는 항상 자신의 생성을 더 좋아합니다.

네.

네. 그리고 그들이 다른 값을 가졌다면 이 결과가 달라졌을까요? 아, 그래서 우리는 인간 주석가 세트를 수집하고 그들의 결정을 평균냅니다. 따라서 집계적으로는 이러한 측정값을 더 신뢰할 수 있을지도 모르겠지만, 30명 정도의 다른 평가자 세트가 있었다면 다른 추세를 얻을 수도 있지만, 평균적으로는 말이 될 것입니다.
알겠습니다. 알겠습니다. 그러면 이것을 ELO 평점에 어떻게 적용할 수 있을까요? ELO 평점은 챗봇 아레나와 같은 곳에서 사용할 수 있는 것입니다. 모델 순위를 매기는 데 인기 있는 방법입니다. 그리고 체스 선수의 평점을 매기는 데 사용된 지표를 기반으로 합니다. 시간 관계상 자세히 설명하지는 않겠지만, 인간이 어떤 모델 생성을 좋아했는지 살펴보고, 업데이트 알고리즘을 기반으로 이를 집계합니다. 따라서 우리는 분리 가능성 개념을 인식하는 가중치를 사용하여 이를 업데이트했습니다. 아이디어는 서로 더 구별 가능하거나 분리 가능한 예시만 고려하는 것이었습니다. 따라서 우리의 분리 가능성 개념을 사용하여 분리 가능성 가중치 ELO를 사용하면 서로 다른 모델 간의 차이가 상당히 줄어드는 것을 알 수 있습니다. 그리고 이것은 모든 테스트 예시를 동일하게 취급한다면 특정 모델에 대해 훨씬 더 과장된 평가를 내릴 수 있으며, 이는 우리의 분리 가능성이 처리한다는 생각을 갖게 합니다. 좋습니다. 하지만 이것은 예시의 어려움이나 판별 가능성을 특징 짓는 완벽한 개념은 결코 아닙니다. 모델 쌍 자체에 확실히 의존적입니다. 입력 프롬프트에 대한 어떤 것도 밝히지 않습니다. 우리가 아는 한, 많은 다른 모델 쌍에서 분리 가능한 인스턴스에 대해 많은 분석을 시도했습니다. 우리에게 눈에 띄는 것은 없었고, 추론 시간 확장이 필요하지만, 평가 및 추론 시간 확장은 이제 모두가 좋아합니다. 하지만 우리는 평가를 위해 이것을 해야 했습니다. 여기 있습니다.

네. 조정하면 다른 결과가 나올 것입니다. 원래 ELO가 더 나은지, 아니면 이 조정된 ELO가 더 나은지 어떻게 알 수 있습니까? 네, 좋은 질문입니다. 좋은 답변은 없습니다.

순위가 바뀌는 것을 보지 못했습니다.

반드시 그런 것은 아닙니다. 하지만 다른 가중치 요소일 뿐입니다. 가중치가 정확했는지 말씀드릴 수는 없습니다. 분리 가능성이 의미 있는 일을 하고 있다는 유일한 증거는 인간이 분리 가능성이라는 개념에 동의하는 경향이 있다는 인간 실험을 통해서입니다. ELO에 대해서는 실행하지 않았습니다. 새로운 인간 실험을 실행할 수도 있었지만, 평가자에게 일부 예시를 다른 예시보다 덜 고려하도록 요청하는 실험을 설계하기 어렵습니다. 여기서 더 이야기할 수 있습니다. 다른 질문 있으신가요? 좋습니다. 좋습니다. 완벽하지 않고 추론 시간 확장이 필요하지만, 실험에서 배운 한 가지 좋은 점은 언어 모델에 있는 이 가변성 개념이 우리가 구성할 수 있는 좋은 기준선에 대한 매우 좋은 아이디어를 제공한다는 것입니다. 따라서 예시 또는 이러한 샘플의 분산이 크다면, 반드시 틀린 것은 아니라는 것을 알고 있다면, 단순히 병합하여 더 가치 있는 생성을 얻을 수 있다는 것을 의미할 수 있습니다. 그러나 몇 번만 발생하거나 몇 번만 샘플링되는 이러한 이상값 중 일부는 환각을 포함할 수 있습니다. 따라서 다른 추론 시간 확장 접근 방식에서 우리는 이러한 모든 생성을 단일 응답으로 결합하려고 합니다. 그리고 이것은 며칠 전 LLM Monkeys 프레젠테이션에서 본 적이 있을 수도 있는 작업과는 다릅니다. LLM Monkeys 프레젠테이션에서는 여러 생성 세트에서 단일 생성을 선택합니다. 따라서 질문은 이러한 응답을 어떻게 결합할까요? 매우 간단한 대답은 언어 모델을 사용하는 것입니다. 우리는 그렇게 하지 않았지만, 그리고 다행히도 이러한 생성을 결합하기 위해 언어 모델을 사용하는 것만으로는 잘 작동하지 않는다는 결과를 얻었습니다. 따라서 우리는 이러한 원래 시퀀스를 살펴보고 어휘 중첩을 기반으로 정렬하려고 하는 그래프 기반 방법을 사용합니다. 이러한 생성을 기반으로 일종의 어휘 부분 순서 그래프를 구축하고, 병합에 사용하는 특정 하이퍼파라미터를 기반으로 차이가 있는 노드를 병합합니다. 따라서 매우 빠르게 언어 모델에 합의를 구축하도록 요청하는 것보다 약간 더 나은 결과를 얻습니다. 하지만 이것은 기준선 역할을 하기 위한 것일 뿐입니다. 더 많은 합의를 기반으로 병합할수록 더 나은 성능을 보이지만, 이는 Savon이 구축한 fact 점수를 기반으로 합니다. 하지만 이 fact 점수는 점점 더 평범한 것을 말할 수 있지만 점수는 높기 때문에 약간 오해의 소지가 있을 수 있습니다. 따라서 이것은 기준선 역할을 할 수 있으며, 긴 형식 생성에서 검증자가 없는 경우에만 이러한 종류의 기준선이 필요하다는 것을 기억하십시오. 그리고 모델이 동일한 것을 반복해서 생성하면 모델이 가장 확신하는 것일 수 있으며, 단일 생성을 샘플링하는 접근 방식보다 성능이 더 좋다는 가정에 기반합니다. 좋습니다. 따라서 테스트 데이터 분포를 재고합니다. 따라서 우리는 현재보다 훨씬 더 직접적으로 긴 형식 생성을 평가해야 합니다. 이상적인 테스트 데이터 분포를 찾는 것은 헛된 추구일 수 있으며, 과거에 여러 번 시도했기 때문에 이렇게 말합니다. 매우 어렵습니다. 좋은 데이터 분포인 벤치마크를 구축하려고 노력했습니다. 좋은 데이터 분포를 얻기 위해 벤치마크를 다운샘플링하려고 노력했습니다. 이것은 매우 어렵고, 아마도 최선의 목표가 아닐 것입니다. 그러나 테스트 세트가 완벽하지 않다는 점을 고려하여 벤치마크 성능을 맥락화해야 합니다. 따라서 MMLU 등에서 볼 수 있는 이러한 수치는 어느 정도 감안해야 하며, 분류에서 랜덤 기준선과 같이 긴 형식 생성에서 더 내재적인 기준선이 필요합니다. 따라서 지금까지 저의 메시지는 언어 모델을 평가하는 것은 매우 어렵다는 것입니다. 따라서 창의적이고 매우 엉뚱한 방법을 살펴봐야 할 수도 있습니다. 우리가 추구하고 있는 엉뚱한 아이디어에 대한 몇 장의 슬라이드가 있으며, 끝나기 전에 매우 빠르게 살펴보겠습니다. 따라서 평가 대상을 재고해야 합니다. 현재 우리는 측정하려고 하는 텍스트의 이산 토큰을 보고 있습니다. 많은 어려움이 있다는 것을 알 수 있습니다. 엉뚱하게 해보면 어떨까요? 완전히 다른 것을 시도해 봅시다. 모델의 출력 확률 또는 출력 점수, 또는 모델의 최종 매개변수를 살펴보고 그것들을 평가할 수 있는지 봅시다. 다시 말씀드리지만, 이것이 언어 모델의 모습입니다. 이것은 언어 모델의 만화입니다. 마지막에 몇 가지 확률을 얻습니다. 여기서 몇 가지 연산, 최대값의 수학 모델을 수행하고, 마지막에 이러한 확률을 얻고, 이러한 확률을 기반으로 한 번에 한 단어를 샘플링합니다. 따라서 이러한 언어 모델 출력은 일반적으로 단어 자체로 생각되지만, 실제로 모델이 생성하는 것은 점수 벡터 또는 확률 벡터입니다. 따라서 이러한 점수 벡터는 많은 정보를 담고 있습니다. 자세히 설명하지는 않겠지만, 언어 모델은 매우 낮은 차원의 공간에서 매우 높은 차원의 어휘 공간으로 큰 투영을 수행하기 때문에 softmax 병목 현상을 갖는 경향이 있습니다. 그리고 언어 모델을 고유하게 만드는 특정 속성이 있다는 것이 밝혀졌습니다. 따라서 기본적으로 모든 언어 모델 출력은 임의의 D개의 출력의 고유한 선형 조합으로 표현될 수 있으며, 그 집합 또는 그 기반은 언어 모델 서명으로 생각할 수 있으며, 언어 모델 서명으로 많은 멋진 작업을 수행할 수 있습니다. 자세히 설명하지는 않겠지만, 우리는 GPD 3.5가 70억 개의 매개변수를 가진 모델이라는 것을 알아냈습니다. 제 학생인 Matt가 여기서 연구를 주도했습니다. 하지만 이러한 언어 모델 서명은 다른 종류의 문제에도 사용할 수 있습니다. 우리는 이것들을 평가에 직접 사용할 수 있습니다. 그리고 이것이 요즘 제가 탐구하려고 하는 질문입니다. 언어 없이 언어 모델을 평가할 수 있을까요? 이러한 모델 서명을 사용하여 학습 중 체크포인트에서 체크포인트로 어떻게 변하는지 연구하여 모델이 학습됨에 따라 의미 있는 변화가 있는지 확인할 수 있습니다. 그리고 성능만 보는 대신, 공격에 대한 취약성과 같은 기준을 살펴볼 수 있으며, 이는 서로 다른 언어 모델을 비교하는 새로운 축을 형성할 수 있습니다. 그리고 Sanjiva가 며칠 전에 보여준 기술이라고 불리는 잠재 표현과 같이 모델 기능에 대한 단서를 제공하는 다른 내부 표현이 있을 수 있습니다. 이러한 기술을 기반으로 모델을 비교할 수 있을지도 모릅니다. 또는 일종의 임베딩 레이어와 서로 직접 비교할 수 있는 해당 레이어의 속성을 살펴볼 수도 있습니다. 따라서 여기서 발표를 마치겠습니다. 많은 미래 아이디어에 영감을 준 Benrech의 블로그에 큰 감사를 표합니다. 그리고 꼭 확인해 보셔야 하며, 평가에 실제로 집중하지 않으면 언어 모델의 밝고 빛나는 미래는 있을 수 없습니다. 그리고 우리는 엉뚱하고 창의적이며 올바른 평가를 얻고 결과를 맥락화하는 데 집중하고 싶습니다. 감사합니다. 이것이 제 연구실입니다.
[박수]
네.
몇 장 전 슬라이드에서 서명과 필수 모델을 아는 것에 대해 더 자세히 말씀해 주시겠어요? 아, 네, 물론입니다. 말씀드릴 수 있습니다. 아마 오프라인에서 이야기할 수 있을 것 같습니다. 기본적으로 당시 GBD 인터페이스의 API에서 얻을 수 있었던 이러한 언어 모델에서 d개의 출력 벡터를 수집할 수 있다면, API에서 사용할 수 있는 몇 가지 논리적 편향을 기반으로 전체 확률 벡터를 계산할 수 있다는 것을 보여줄 수 있었습니다. 그리고 이것을 여러 번 수행하고 선형 종속성이 있는 시점을 파악함으로써 숨겨진 차원을 파악할 수 있었습니다. 이는 모든 것이 트랜스포머 모델이기 때문에 GPT 아키텍처의 크기가 어느 정도인지 대략적으로 알려줍니다. 오프라인에서 더 자세히 이야기할 수 있습니다. 매우 간략한 요약입니다.
네, 그리고 저기 있는 제 학생 Matt가 이 작업을 했습니다. Matt와 직접 이야기해 보실 수 있습니다.
흥미롭네요. 그리고 이 앨범이 항상 D개의 것들의 중간 조합이라고 말씀하셨는데, D가 엄청나네요.
네. D는 어휘 크기가 아닙니다. 숨겨진 차원의 크기인데, 네, 크지만 수만 개의 토큰은 아닙니다. 수천 개 정도입니다. 네, 이제 수십만 개입니다.
이것은 질문이 아니라 의견입니다. 슬라이드를 평가로 끝내셨는데, 대규모 언어 모델 개발이 평가 질문에 의해 동기 부여되었다는 점을 지적하고 싶었습니다. 앙드레 마르코프가 1913년에 논문을 썼을 때, 그는 푸시킨의 텍스트를 얼마나 잘 생성할 수 있는지 평가하려고 했습니다. 이것이 원죄 또는 원래 문제입니다.
네, 모두 연결되어 있습니다. 네, 물론입니다.
네.
훌륭한 발표였습니다. 정말 감사합니다. 이러한 실제 작업을 결국 벤치마크로 공개하여 정보를 제공할 생각은 없으신가요? 네. 데이터 공개와 마찬가지로 공개할 예정입니다. 일부 데이터는 IRB 하에 있습니다. 따라서 얼마나 많은 데이터를 공개할 수 있을지는 모르겠지만, 네, 모두 공개될 예정입니다. 이것들은 대부분 매우 새로운 결과입니다. 하지만 네, 이것들을 벤치마크로 사용할 수 있습니다. 하지만 네, 아마도 일부는 벤치마크를 구축하는 것이 요점이 아닐 수도 있지만, 네, 우리가 할 수 있는 모든 것을 공개할 것입니다.
네. 이미지 작업이 아닌 것을 보셨는지 궁금했습니다. 음... 아, 이것은 모두 이미지에 대해 제가 말한 것과 같습니다. 이것은 모두 탐색적인 것입니다. 모델 또는 모델의 이미지를 기반으로 모델을 평가한 결과는 없습니다. 그것이 당신이 의미한 바인가요? 아뇨, 아뇨, 아뇨. 그래서 저는 훨씬 낮은 품질의 이미지 데이터 세트를 재구성하는 논문이 있다고 생각했습니다.
그리고 그 벤치마크가 모델 순위를 알려주는 데 있어 ImageNet만큼이나 좋다는 것을 발견했습니다. 네. 데이터 세트 증류와 같습니다. 정확히는 아닙니다. 다른 데이터 세트를 만들고
어쨌든 오프라인에서 더 말씀드릴 수 있을 것 같습니다. 알겠습니다. 네.
알겠습니다. 모두 감사합니다.

