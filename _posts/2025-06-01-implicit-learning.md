---
layout: post
title: Neuro Symbolic AI
date: 2025-06-01
categories: [artificial intelligence]
tags: [artificial general intelligence]

---

# [Yilun Du - Implicit Learning with Energy-Based Models](https://www.youtube.com/watch?v=As93L2pwR-E) 

## [Abstract](https://notebooklm.google.com/notebook/92268c9b-3179-4744-bf85-302ae395777b/audio)

Deep learning has performed well on internet datasets, but still faces challenges when applied to complex embodied domains. In this talk, I’ll discuss how energy based models are a useful tool for constructing deep learning systems in embodied domains. First, I will illustrate how energy based optimization enables us to compose multiple different models together, enabling us to compositionally understand different visual scenes. Next, I will illustrate how an energy based optimization procedure enables us to generate diverse behaviors with respect to test time constraints, enabling us to adapt, at test time, to different goals and rewards. Finally, I’ll talk about how energy optimization enables us to iteratively reason and adapt to novel problems, leveraging more complex optimization procedures to solve more complex algorithmic problems at test time.

### About the Speaker 
Yilun Du is a fourth year graduate student at MIT EECS. Previously, he has done a research fellowship at OpenAI, and research internships at FAIR, DeepMind and Google Brain. He  is supported by an NSF Graduate Fellowship and was a finalist for Qualcomm and Open Philanthropy fellowships.

---

**에너지 기반 모델(EBMs)**의 **함축적 학습**에 대한 내용을 다룹니다. 강연자는 EBMs가 **로봇 학습 및 계획, 이미지 생성, 그리고 알고리즘적 추론**과 같은 다양한 문제에 유용하게 적용될 수 있다고 설명합니다. 특히, EBMs의 핵심 강점은 **다른 모델과의 합성**을 통해 복잡한 개념을 표현하고, **테스트 시점 제약 조건**을 통합하여 다양한 행동에 **적응**하며, **반복적인 최적화**를 통해 **학습 데이터 분포와 다른 입력에도 일반화**할 수 있다는 점을 강조합니다. 이 모델들은 **확산 모델**과 밀접한 관련이 있으며, **에너지 함수**를 정의하고 **경사 기반 최적화**를 통해 낮은 에너지를 가진 샘플, 즉 올바른 해답이나 원하는 특성을 지닌 데이터를 생성하거나 탐색하는 방식으로 작동합니다.


<iframe width="600" height="400" src="https://www.youtube.com/embed/As93L2pwR-E?si=Sa9GOsLF3wPBtoRZ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

---


## 에너지 기반 모델(EBM) 개요

에너지 기반 모델(Energy-Based Model, EBM)은 **데이터의 확률 분포를 나타내기 위해 '에너지 함수'를 사용하는 모델**입니다. 기존의 전형적인 기계 학습 모델과는 다른 접근 방식을 사용합니다.

핵심 내용은 다음과 같습니다.

1.  **에너지 함수 정의**: EBM은 특정 개념(예: 미소)이나 데이터의 구조를 나타내는 **에너지 함수 E(x)**를 가집니다. 이 함수는 입력 x(예: 이미지)를 받아 스칼라 에너지 값 하나를 출력합니다.
2.  **일치도에 따른 에너지 값**: 에너지 함수는 입력 x가 모델이 나타내고자 하는 개념이나 데이터 분포와 **얼마나 잘 일치하는지**를 에너지 값의 크기로 표현합니다.
    *   개념과 **더 일치하는 입력** (예: 미소 짓는 얼굴 이미지)에는 **낮은 에너지 값**을 할당하며, 이 값은 음의 무한대까지 낮아질 수 있습니다.
    *   개념과 **덜 일치하는 입력** (예: 미소 짓지 않는 얼굴 이미지)에는 **높은 에너지 값**을 할당합니다.
3.  **확률 분포와의 관계 (볼츠만 분포)**: 이렇게 정의된 에너지 함수 E(x)는 데이터 x가 나타날 확률 분포 P(x)를 정의하는 데 사용되며, 일반적으로 **볼츠만 분포(Boltzmann distribution)** 형태로 표현됩니다.
    *   **P(x) ∝ exp(-E(x))**
    *   이는 **에너지가 낮은 데이터 포인트일수록 지수 함수 exp(-E(x)) 값이 커져서 해당 데이터가 나타날 확률 P(x)가 높아진다**는 것을 의미합니다. 즉, 에너지가 낮을수록 해당 데이터가 더 '가능성 있는' 데이터라고 간주됩니다.
4.  **데이터 생성 및 활용 (에너지 최적화)**: EBM을 사용하여 데이터를 생성하거나 문제를 해결할 때는, 무작위 입력에서 시작하여 에너지 함수 E(x)를 **최소화하는 과정(에너지 최적화)**을 수행합니다. 이는 에너지 함수의 음수 기울기를 따라 이동하는 그래디언트 기반 최적화 절차(예: Langevin Dynamics)를 통해 이루어지며, 에너지가 낮은 지점(즉, 모델이 학습한 데이터 분포에 잘 맞는 데이터)을 찾게 됩니다. 확산 모델(Diffusion models) 역시 이러한 반복적인 생성/최적화 과정을 사용하며, EBM의 일종으로 볼 수 있습니다.
5.  **데이터 구조 및 불확실성 포착**: EBM은 이러한 방식으로 임의로 복잡한 확률 분포를 모델링하고 데이터의 구조를 포착할 수 있습니다. 특히 여러 가능한 결과가 존재하는 다봉적인(multimodal) 불확실성을 나타내는 데 유용합니다.

요약하자면, EBM은 데이터의 일치도를 나타내는 스칼라 에너지 함수를 정의하고, 이 에너지를 볼츠만 분포 형태로 변환하여 데이터 공간에 대한 확률 분포를 나타내는 모델이며, 에너지 최적화 과정을 통해 데이터를 생성하거나 문제를 해결합니다. 이러한 표현 방식은 모델의 합성 가능성, 다양한 제약 조건 적용, 분포 변화에 대한 적응력 등의 장점을 가집니다.

---

## EBM 에너지 함수의 의미와 활용

**에너지 기반 모델(EBM)의 에너지 함수가 무엇을 나타내는지** 설명해 드리겠습니다.

에너지 함수는 기본적으로 특정 **개념(concept)의 일치도**를 나타내는 스칼라 값을 출력하는 함수입니다. 입력(예: 이미지 X)을 받아 하나의 스칼라 에너지 값으로 매핑하며, 이 에너지 값은 입력이 모델이 나타내고자 하는 개념과 **얼마나 잘 일치하는지**를 표현합니다.

구체적으로:

*   **낮은 에너지 값**: 입력이 모델이 나타내는 개념(예: 미소 짓는 얼굴)과 **더 일치하거나 개념을 만족할수록** 낮은 에너지 값을 할당합니다. 이 값은 음의 무한대까지 낮아질 수 있습니다. 즉, 에너지가 낮을수록 해당 입력은 모델이 학습한 데이터 분포에 더 잘 맞는 **'가능성 있는(possible)' 데이터**로 간주됩니다.
*   **높은 에너지 값**: 입력이 모델이 나타내는 개념과 **덜 일치하거나 만족하지 않을수록** 높은 에너지 값을 할당합니다.

이렇게 정의된 에너지 함수 E(x)는 볼츠만 분포(Boltzmann distribution) P(x) ∝ exp(-E(x))를 통해 데이터 x가 나타날 **확률 분포**를 나타내는 데 사용됩니다. 이는 **에너지가 낮을수록 데이터의 확률이 높아짐**을 의미합니다.

에너지 함수는 다양한 맥락에서 다음과 같은 것들을 나타낼 수 있습니다:

*   **개념 또는 속성**: '미소', '여성 얼굴', '남성 얼굴' 등 특정 이미지 속의 개념을 나타낼 수 있습니다.
*   **개념 또는 제약 조건의 조합**: 두 에너지 함수를 더함으로써 두 개념/제약 조건 **모두**를 만족하는 경우('AND' 관계)를 나타낼 수 있습니다 (예: 미소 짓는 여성 얼굴). 에너지 분포들의 합에서 샘플링하거나 LogSumExp 근사를 통해 두 개념/제약 조건 **중 하나 이상**을 만족하는 경우('OR' 관계)를 나타낼 수도 있습니다. 원치 않는 개념의 에너지 함수를 음수화하여 더함으로써 특정 개념은 만족하지만 다른 특정 개념은 **만족하지 않는 경우**('BUT NOT' 관계)를 나타낼 수도 있습니다 (예: 미소 짓지만 여성이 아닌 얼굴).
*   **객체 간의 관계**: 복잡한 장면에서 객체 간의 특정 공간적 또는 논리적 관계(예: '작은 빨간색 금속 원통이 작은 녹색 고무 큐브 아래에 있음')를 나타낼 수 있습니다. 여러 관계를 나타내는 에너지 함수들을 더하여 더 복잡한 관계를 만족하는 장면을 생성할 수 있습니다.
*   **데이터의 숨겨진 요소 또는 요인**: 이미지와 같은 데이터를 비지도 학습 방식으로 분석하여 카메라 위치, 조명 방향, 조명 강도 또는 얼굴의 특징, 배경, 헤어/피부 색상과 같은 **기저의 요인(underlying factors)**을 나타내는 여러 에너지 함수로 분해할 수 있습니다.
*   **궤적의 타당성 및 제약 조건**: 로봇 계획에서 가능한 궤적 공간 내에서 **합리적인(reasonable) 궤적**이 무엇인지 나타내는 에너지 함수로 사용될 수 있습니다. 또한, 시작/종료 상태 제약 조건, 회피해야 할 장애물, 특정 목표 위치 등 **테스트 시 적용되는 제약 조건이나 비용 함수**를 나타내는 에너지 함수로도 사용될 수 있습니다. 이러한 제약 에너지는 학습되지 않고 수동으로 지정될 수도 있습니다.
*   **알고리즘 문제의 해의 정확성**: 행렬 덧셈이나 최단 경로 계산과 같은 알고리즘 문제에서 주어진 입력에 대한 잠재적 해(solution)가 **올바른지 여부**를 나타낼 수 있습니다. 올바른 해에는 낮은 에너지를, 잘못된 해에는 높은 에너지를 할당합니다.

요약하자면, 에너지 함수는 데이터가 특정 개념, 속성, 관계, 제약 조건 또는 문제의 올바른 해와 얼마나 일치하는지를 스칼라 에너지 값으로 표현하고, 이를 통해 데이터 공간의 확률 분포를 나타내는 유연한 도구입니다. 이러한 표현 방식은 특히 모델을 결합하거나 테스트 시 새로운 제약 조건을 적용하는 데 유리합니다.

---

## Energy Optimization

에너지 기반 모델(EBM)에서 **에너지 최적화(Energy Optimization)**는 기본적으로 **데이터의 에너지 함수 값을 최소화하는 과정**을 의미하며, 이는 다양한 문제를 해결하는 데 유용하게 활용됩니다. 에너지 최적화는 무작위 노이즈에서 시작하여 에너지 함수의 그라디언트를 따라 최적화(예: Langevin Dynamics)를 수행하여 에너지가 낮은 지점(즉, 모델이 표현하는 개념에 맞는 데이터)을 찾는 반복적인 계산 절차입니다.

에너지 최적화가 해결할 수 있는 문제와 그 유용성은 다음과 같습니다.

*   **데이터 생성 (Data Generation)**: 에너지 함수는 특정 개념이나 데이터의 구조를 낮은 에너지 값으로 인코딩합니다. 에너지 최적화를 통해 무작위 노이즈에서 시작하여 에너지를 최소화하는 방향으로 탐색하면, 학습된 에너지 함수가 나타내는 개념에 맞는 데이터를 생성할 수 있습니다 (예: '미소 짓는 얼굴' 에너지 함수로 미소 짓는 얼굴 이미지 생성). 이는 디퓨전 모델을 포함한 여러 생성 모델의 기본 메커니즘과 연결됩니다.

*   **개념 또는 제약 조건의 유연한 결합 및 복잡한 데이터 생성 (Flexible Composition and Complex Data Generation)**: 서로 다른 개념이나 제약 조건을 학습하거나 정의한 여러 에너지 함수가 있을 때, 이들을 수학적으로 결합(예: 에너지 함수 합산)하여 새로운 에너지 함수를 만들고 이를 최적화함으로써 **복잡한 요구사항을 동시에 만족하는 데이터를 생성**할 수 있습니다.
    *   두 개념 **모두** 만족: 에너지 함수들을 더한 후 최적화. (예: '미소 짓는 얼굴' + '여성 얼굴' 에너지 → 미소 짓는 여성 얼굴 생성).
    *   특정 개념을 **제외**: 원치 않는 개념의 에너지 함수에 음수를 곱하여 더한 후 최적화. (예: '미소 짓는 얼굴' - '여성 얼굴' 에너지 → 미소 짓는 남성 얼굴 생성).
    *   이를 통해 학습 시 보지 못했던 더 복잡한 개념의 조합이나 객체 관계를 포함하는 장면을 생성하는 데 유용합니다. 또한, 학습된 에너지 함수뿐만 아니라 **수동으로 정의된(handcrafted) 제약 조건**을 나타내는 에너지 함수와도 자연스럽게 결합하여 사용할 수 있습니다.

*   **로봇 계획 및 동작 합성 (Robot Planning and Behavior Synthesis)**: 로봇 궤적의 합리성을 나타내는 학습된 에너지 함수와 목표 상태, 시작/종료 상태, 장애물 회피 등 **테스트 시점에 주어지는 제약 조건**을 나타내는 에너지 함수를 결합(덧셈)하고, 이를 최적화하여 해당 제약을 만족하는 로봇의 궤적이나 동작을 합성할 수 있습니다. 이를 통해 별도의 학습 없이도 다양한 작업을 수행하도록 로봇의 동작을 쉽게 적응시킬 수 있습니다. 학습된 에너지 함수의 연속성 덕분에 전통적인 제약 기반 최적화보다 더 부드러운 최적화 과정을 거치고 지역 최저점 문제를 피하는 데 도움이 될 수 있습니다.

*   **알고리즘적 추론 및 문제 해결 (Algorithmic Reasoning and Problem Solving)**: 덧셈, 행렬 곱셈, 최단 경로 계산 등 알고리즘적인 문제에서, 문제 입력과 가능한 해를 받아 정답일 때 낮은 에너지를 부여하는 에너지 함수를 학습한 후, 에너지 최적화를 통해 가장 낮은 에너지(즉, 정답)를 가지는 해를 찾는 방식으로 문제를 해결할 수 있습니다.

*   **학습 분포에서 벗어난 문제에 대한 일반화 (Generalization to Out-of-Distribution Problems)**: 에너지 최적화 과정은 해를 찾기 위한 반복적인 계산 방식(iterative computation)을 사용합니다. 학습 시 접하지 못한 더 어렵거나 규모가 큰 문제(예: 학습된 그래프 크기보다 큰 그래프에서의 최단 경로)에 대해, 최적화 단계를 더 많이 수행함으로써 해를 찾을 수 있습니다. 이는 예측을 단방향으로 수행하는 모델에 비해 학습 분포 밖의 문제에 대한 일반화 능력을 향상시키는 데 기여합니다.

*   **다봉성 및 불확실성 포착 (Capturing Multimodality and Uncertainty)**: 에너지 기반 모델은 데이터 공간의 복잡하고 다봉적인 확률 분포를 유연하게 모델링할 수 있으며, 에너지 최적화 과정을 통해 이러한 여러 가능한 해(낮은 에너지 지점들)를 탐색하고 포착할 수 있습니다. 이는 자율주행의 미래 예측과 같이 여러 가능한 결과가 존재하는 불확실한 문제에서 유용합니다.

요약하자면, 에너지 최적화는 EBM에서 학습된 에너지 함수가 인코딩하는 데이터 분포를 탐색하고, 낮은 에너지를 갖는 데이터를 찾아내는 핵심 메커니즘입니다. 이를 통해 **복잡한 개념의 결합, 다양한 제약 조건에 대한 적응, 학습 시 접하지 못한 문제에 대한 일반화, 그리고 데이터의 복잡한 다봉성 포착** 등 다양한 응용 분야에서 유용하게 활용될 수 있습니다.

---

## EBM 에너지 함수 학습

에너지 기반 모델(EBM)의 에너지 함수는 주로 다음과 같은 방식으로 학습됩니다.

1.  **실제 데이터 에너지 최소화 및 가짜 데이터 에너지 최대화:**
    *   가장 일반적인 학습 방식은 **Wake-Sleep** 또는 **Contrastive Divergence**라고 불리는 절차를 사용하는 것입니다.
    *   핵심 아이디어는 **실제 데이터(real data)의 에너지 값을 최소화**하고, 모델이 생성한 **가짜 데이터("hallucinations")의 에너지 값을 최대화**하는 것입니다.
    *   학습 초반에는 에너지 함수가 대체로 균일하여 실제 데이터와 가짜 데이터 모두 높은 에너지를 가집니다.
    *   학습이 진행됨에 따라 실제 데이터의 에너지는 점차 낮아지는 반면, 가짜 데이터("hallucinations")의 에너지는 높아지게 됩니다.
    *   이 학습 방식에서 "hallucinations"은 무작위 노이즈(random noise)에서 시작하여 현재 학습된 에너지 함수에 대한 **그라디언트 디센트(gradient descent)**와 같은 최적화 절차를 실행하여 생성됩니다.

2.  **에너지 함수의 그라디언트 학습:**
    *   에너지 함수를 학습하는 **대안적인 방법**은 디퓨전 모델(diffusion models)에서 사용하는 방식을 따르는 것입니다.
    *   이는 에너지 값 자체를 명시적으로 학습하는 대신, **에너지 함수의 그라디언트(gradient)를 학습**하는 방식입니다.
    *   이미지의 노이즈를 제거하는 방법(denoising)을 학습함으로써 에너지 경관(energy landscape)의 그라디언트를 지도 학습(supervise)할 수 있습니다.
    *   디퓨전 모델은 에너지 기반 모델의 특정 인스턴스(instantiation)로 볼 수 있으며, 이들은 기본적으로 에너지 함수의 그라디언트를 학습합니다. 따라서 디퓨전 모델 학습은 에너지 함수의 그라디언트를 학습하는 한 형태입니다.

이러한 학습 목표(실제 데이터 에너지는 낮추고 가짜 데이터 에너지는 높이는 것)는 파라미터 수가 제한된 네트워크를 사용하여 데이터의 근본적인 생성 분포(underlying generative distribution)를 모델링하려는 시도와 연결되며, 이를 통해 과적합(overfitting)을 방지하는 데 도움이 됩니다.

---

## EBM Training

에너지 기반 모델(EBM)은 다음과 같은 방식으로 훈련됩니다.

에너지 기반 모델 학습의 핵심 아이디어는 **실제 데이터의 에너지 값을 낮추고 모델이 생성한 가짜 데이터(종종 "hallucinations"라고 불림)의 에너지 값을 높이는 것**입니다. 이것은 데이터 분포를 모델링하기 위한 일반적인 접근 방식과 유사하며, 실제 데이터의 가능도(likelihood)를 최대화하고 다른 모든 데이터 포인트의 가능도를 최소화하려고 시도합니다.

훈련 방법은 크게 두 가지로 설명됩니다.

1.  **Wake-Sleep 또는 Contrastive Divergence 방식:**
    *   **"Wake" 단계:** 실제 훈련 데이터 포인트에 대해 에너지 함수 값을 최소화합니다. 즉, 실제 데이터는 낮은 에너지를 갖도록 모델을 조정합니다.
    *   **"Sleep" 단계:** 모델이 생성한 "hallucinations" 또는 가짜 데이터에 대해 에너지 함수 값을 최대화합니다.
    *   이때 가짜 데이터("hallucinations")는 **현재 학습된 에너지 함수에 대한 반복적인 최적화 절차(예: 그라디언트 디센트)**를 무작위 노이즈에서 시작하여 실행함으로써 생성됩니다. 모델은 현재 에너지 함수에서 낮은 에너지 영역을 탐색하여 샘플을 생성하는 것입니다.
    *   이 과정은 **모델(에너지 값을 높이려 함)과 샘플러(낮은 에너지 지점을 찾으려 함) 사이에 일종의 "긴장(tension)"을 유발**합니다. 훈련이 진행됨에 따라 올바른 음성 샘플(correct negative samples)을 생성하기가 점점 더 어려워지고 더 많은 MCMC(Markov Chain Monte Carlo) 단계를 요구하게 됩니다.
    *   이러한 문제를 완화하기 위해 "chaos term"이라는 것을 추가하여 샘플러가 에너지 함수가 샘플의 에너지 값을 높이는 와중에도 여전히 낮은 에너지를 얻을 수 있도록 장려할 수 있습니다. 별도의 분포를 사용하여 음성 샘플을 생성하는 것도 도움이 될 수 있지만, 이 샘플이 모델에서 가장 "적대적인(adversarial)" 모드가 아닐 수 있다는 단점이 있습니다.

2.  **Denoising Objective 방식 (Diffusion Models와 유사):**
    *   에너지 값을 명시적으로 훈련하는 대신, **에너지 함수의 *그라디언트(gradient)*를 훈련**할 수 있습니다.
    *   이는 이미지를 "노이즈 제거(denoise)"하는 방법을 학습함으로써 이루어집니다. 자료에서는 확산 모델(diffusion models)이 본질적으로 에너지 기반 모델의 특정 인스턴스이며, 노이즈 제거를 통해 에너지 함수의 그라디언트를 학습한다고 설명합니다. 이 방식에서도 이미지 생성 과정은 노이즈에서 시작하여 최종 이미지를 점진적으로 구성하는 반복적인 절차를 따릅니다.

요약하자면, EBM은 **실제 데이터의 에너지 감소**와 **모델 생성 데이터의 에너지 증가**를 목표로 하며, 이를 위해 Contrastive Divergence와 같은 방식을 사용하거나 확산 모델처럼 **에너지 함수의 그라디언트 학습**을 통해 훈련될 수 있습니다. 후자의 방식은 노이즈 제거 과정과 밀접하게 관련되어 있습니다.

---

## EBM 에너지 함수 결합 방법

에너지 기반 모델(EBM)의 **에너지 함수를 구성하거나 다른 에너지 함수와 결합하는 세 가지 주요 방법**이 제시되었습니다. 이러한 결합은 특히 **테스트 시(test time)에 별도의 추가 학습이나 미세 조정 없이 직접 적용 가능**하다는 큰 장점이 있습니다.

에너지 함수를 구성하는 세 가지 방법은 주로 기존의 에너지 함수 E1과 E2를 수학적으로 조합하는 방식이며, 이는 나타내고자 하는 개념이나 제약 조건의 논리적 관계에 따라 달라집니다.

1.  **두 개념/제약 조건 모두를 만족하는 경우 (AND 관계):**
    *   **에너지 함수를 단순히 더하는 방식**을 사용합니다: `E_composed = E1 + E2`.
    *   이 합성된 에너지 함수는 E1과 E2가 나타내는 두 개념이나 제약 조건 **모두를 만족하는 입력에 낮은 에너지 값**을 할당합니다.
    *   예시: '미소 짓는 얼굴' 에너지 함수와 '여성 얼굴' 에너지 함수를 더하면 '미소 짓는 여성 얼굴'에 낮은 에너지를 할당하는 모델을 얻습니다. 로봇 계획에서는 궤적 타당성 EBM과 시작/종료 상태 제약 에너지 함수를 더하여 해당 제약을 만족하는 타당한 궤적을 찾습니다. 여러 객체 간의 관계를 나타내는 에너지 함수들을 더하여 복잡한 장면을 생성할 수도 있습니다.

2.  **두 개념/제약 조건 중 하나라도 만족하는 경우 (OR 관계):**
    *   **해당 에너지 함수가 나타내는 분포들의 합에서 샘플링하는 방식**을 사용하며, 이는 에너지 함수들에 대한 LogSumExp 근사를 통해 근사될 수 있습니다.
    *   이 방식을 사용하면 E1 **또는** E2 중 **하나를 만족하는 샘플**을 생성할 수 있습니다.
    *   예시: '여성 얼굴' 또는 '미소 짓는 얼굴' 중 하나에 해당하는 샘플을 생성할 수 있습니다.

3.  **하나의 개념/제약 조건은 만족하지만 다른 하나는 만족하지 않는 경우 (BUT NOT 관계):**
    *   원치 않는 개념/제약 조건에 해당하는 에너지 함수를 **음수화(negate)한 후 다른 에너지 함수에 더하는 방식**을 사용합니다: `E_composed = E2 - E1` 또는 `-E1 + E2`.
    *   이 방식을 사용하면 E2는 만족하지만 E1은 만족하지 않는 샘플을 생성할 수 있습니다.
    *   예시: '미소 짓는 얼굴'이지만 '여성 얼굴'이 아닌 샘플(즉, 미소 짓는 남성 얼굴)을 생성할 수 있습니다.

이러한 방식으로 구성된 합성 에너지 함수를 이용하면, 무작위 입력에서 시작하여 에너지 함수를 최소화하는 그래디언트 기반 최적화 절차(예: Langevin Dynamics)를 통해 **복잡한 요구사항을 만족하는 데이터나 해(solution)를 생성**할 수 있습니다. 확산 모델(Diffusion models)도 이러한 반복적인 생성/최적화 과정을 사용하며 EBM의 일종으로 볼 수 있으며, 이와 같은 에너지 함수의 합성 연산이 적용될 수 있습니다.

이러한 에너지 함수의 구성 및 결합 방식은 **학습된(learned) 에너지 함수**뿐만 아니라 **수동으로 지정된(handcrafted) 제약 조건**을 나타내는 에너지 함수와도 자연스럽게 결합될 수 있어, 학습된 모델에 새로운 테스트 시 제약 조건을 유연하게 적용할 수 있게 합니다. 또한, 학습된 EBM을 비지도 학습 방식으로 독립적인 여러 에너지 함수로 분해한 후, 이를 재조합하여 새로운 데이터를 생성하는 데 활용할 수도 있습니다.

---

## 기존 ML 모델과의 차별점

에너지 기반 모델(EBM)은 기존 기계 학습 모델과 비교하여 다음과 같은 차별점을 갖습니다:

1.  **개념 표현 및 데이터 생성 방식의 차이**:
    *   기존의 많은 기계 학습 시스템(예: 객체 탐지)은 훈련 데이터와 유사한 테스트 분포에서 잘 작동하지만, 환경이 자주 바뀌는 실생활이나 로봇과 같은 구체화된(embodied) 환경에서는 성능이 저하될 수 있습니다. 이는 훈련 및 테스트 분포가 상당히 다를 수 있기 때문입니다.
    *   EBM은 특정 개념(예: 미소 짓는 얼굴)을 **에너지 함수**로 표현합니다. 이 함수는 입력(예: 이미지)을 하나의 스칼라 에너지 값으로 매핑하며, 개념과 일치하지 않는 입력에는 매우 높은 에너지 값을 할당하고, 개념을 만족하는 입력에는 낮은 에너지 값을 할당합니다. 이 에너지 값은 음의 무한대에서 무한대까지의 범위를 가질 수 있습니다.
    *   EBM은 이 에너지 함수를 이용하여 데이터의 확률 분포를 볼츠만 분포(확률 ∝ e<sup>-에너지</sup>)로 표현합니다.
    *   데이터 생성은 무작위 노이즈 입력에서 시작하여 에너지 함수에 대한 기울기 기반 절차(예: Langevin Dynamics)를 실행하여 에너지 값을 점진적으로 낮추고 개념과 일치하는 데이터를 생성하는 방식으로 이루어집니다. Diffusion 모델도 이러한 에너지 기반 모델의 한 형태로 볼 수 있으며, 노이즈에서 시작하는 반복적인 생성 프로세스를 사용합니다. 이러한 반복적인 생성 프로세스는 EBM과 diffusion 모델 모두에게 중요한 특징이며, 더 어려운 문제에 계산 능력을 조정하거나 다른 모델을 결합할 수 있게 해줍니다.

2.  **합성 가능성(Compositionality)**:
    *   EBM은 서로 다른 개념이나 요소를 나타내는 별도로 학습된 에너지 함수를 단순히 더함으로써 새로운 에너지 함수를 만들고, 이를 최적화하여 두 가지 개념을 모두 만족하는 데이터를 생성할 수 있다는 **자연스러운 합성 방식**을 제공합니다. 예를 들어, '미소 짓는 얼굴' 에너지 함수와 '여성 얼굴' 에너지 함수를 더하면 '미소 짓는 여성 얼굴'을 생성할 수 있습니다.
    *   이러한 합성은 학습된 모델을 추가적인 미세 조정 없이 **테스트 시에 직접 적용**하여 수행할 수 있습니다.
    *   기존의 생성 모델은 이러한 방식으로 서로 합성하는 것이 어렵습니다. EBM의 합성 능력은 훈련 시 보지 못한 더 복잡한 개념이나 관계(예: 객체 관계)를 갖는 장면을 생성하는 데 유용합니다.

3.  **적응성(Adaptability) 및 유연성**:
    *   EBM은 로봇 계획과 같은 구체화된 학습 환경에서 **매우 적응 가능한 행동 합성**을 가능하게 합니다. 기본적인 궤적 에너지 함수에 목표, 보상 또는 테스트 시 제약 조건을 나타내는 비용 함수를 추가하면, 특정 조건을 만족하는 궤적을 합성할 수 있습니다. 이를 통해 예상치 못한 장애물이나 외부 제약 조건에 적응할 수 있습니다.
    *   이러한 제약 조건 에너지 함수는 학습될 필요 없이 **수동으로 지정**될 수 있습니다. 이 프레임워크는 강화 학습, 목표 계획(매우 긴 탐색 경로 계획 포함), 제약 조건이 있는 궤적 최적화에 사용될 수 있습니다.
    *   학습된 EBM 모델의 연속성은 기존의 딱딱한 제약 조건과 시뮬레이터를 사용하는 방법과 비교하여 더 부드러운 최적화 지형을 제공하며, 이는 지역 최저점에 빠지는 것을 방지하는 데 도움이 될 수 있습니다.

4.  **반복 계산 및 추론 능력**:
    *   EBM은 덧셈, 행렬 곱셈, 최단 경로 계산과 같은 **알고리즘적 문제**를 최소 에너지 솔루션을 찾는 에너지 최적화 절차로 공식화할 수 있습니다.
    *   이 **반복적인 계산** 방식은 훈련 시 보지 못한 더 어렵거나 분포를 벗어난(out-of-distribution) 문제 인스턴스에 대해 **일반화**하는 것을 가능하게 합니다. 테스트 시 더 많은 계산 단계를 실행함으로써 해결책을 점진적으로 개선할 수 있습니다. 재귀 네트워크와 같은 다른 모델은 더 큰 문제에 대해 일반화하는 데 어려움을 겪는 것과 대조됩니다.
    *   EBM이 솔루션을 직접 생성하기보다는 '검증자'(주어진 솔루션이 정확한지 확인)의 역할을 학습한다고 볼 수 있으며, 이러한 검증 작업은 솔루션 생성보다 일반화하기가 더 쉬울 수 있습니다. 이러한 반복적인 개선 프로세스는 피드백을 제공하고 강건성을 높여줍니다.

5.  **불확실성 및 다봉성(Multimodality) 처리**:
    *   EBM은 **임의로 복잡한 확률 분포**를 모델링할 수 있습니다. 이 때문에 차량의 미래 목표 위치 예측과 같이 상당한 불확실성이나 복잡한 다봉성 출력을 갖는 작업에서 회귀와 같은 지도 학습 모델이나 간단한 가우시안 혼합 모델보다 잠재적으로 더 나은 성능을 발휘할 수 있습니다. 지도 학습은 다봉성 출력을 흐릿하게 만들 수 있습니다. 가우시안 혼합 모델은 데이터가 가질 수 있는 모든 복잡한 분포를 정확하게 모델링하는 데 한계가 있을 수 있습니다.

요약하자면, EBM은 에너지 함수를 통한 개념 표현, 반복적인 생성/최적화 프로세스, 그리고 에너지 함수의 합성, 적응성 및 반복 계산을 가능하게 하는 구조를 통해 기존 기계 학습 모델과는 다른 강력한 능력을 제공합니다. 특히 분포 변화, 복잡한 제약 조건, 불확실성 및 다봉성을 다루는 데 유용합니다.

---

## EBM이 합성 가능한 이유

에너지 기반 모델(EBM)이 합성 가능한 주된 이유는 **개념을 에너지 함수로 표현하고, 이러한 에너지 함수들을 수학적으로 결합(주로 덧셈)하여 새로운 개념을 나타내는 에너지 함수를 생성할 수 있기 때문입니다**. 이 합성은 **테스트 시(test time)에 추가적인 미세 조정(fine-tuning) 없이 직접 적용 가능**하다는 중요한 특징이 있습니다.

구체적으로 살펴보겠습니다.

1.  **개념의 에너지 함수 표현**: EBM에서 특정 개념(예: 미소 짓는 얼굴, 여성 얼굴, 짧은 경로)은 입력(예: 이미지, 로봇 궤적, 그래프 솔루션)에 대해 스칼라 에너지 값을 반환하는 **에너지 함수**로 표현됩니다. 이 에너지 함수는 해당 개념과 일치하는 입력에는 낮은 에너지 값을, 일치하지 않는 입력에는 높은 에너지 값을 할당합니다. 에너지 값은 음의 무한대에서 무한대까지의 범위를 가질 수 있습니다.
2.  **에너지 함수의 덧셈을 통한 합성**: 서로 다른 개념을 나타내는 별도로 학습된 에너지 함수 E1과 E2가 있을 때, 이 두 함수를 단순히 더하여 새로운 에너지 함수 E_composed = E1 + E2를 만들 수 있습니다. 이렇게 합성된 에너지 함수는 E1과 E2가 나타내는 두 개념 **모두를 만족하는 입력에 낮은 에너지 값을 할당**하게 됩니다. 예를 들어, '미소 짓는 얼굴' 에너지 함수와 '여성 얼굴' 에너지 함수를 더하면, '미소 짓는 여성 얼굴'에 낮은 에너지를 할당하는 새로운 에너지 함수를 얻게 됩니다.
3.  **합성된 에너지 함수의 최적화**: 합성된 에너지 함수(E1 + E2)를 얻은 후, 무작위 노이즈 입력에서 시작하여 에너지 함수에 대한 기울기 기반 최적화 절차(예: Langevin Dynamics)를 실행하여 에너지 값을 점진적으로 낮추면, 결과적으로 **두 개념을 모두 만족하는 데이터(예: 미소 짓는 여성 얼굴 이미지)**를 생성할 수 있습니다. Diffusion 모델도 이러한 반복적인 생성/최적화 프로세스를 사용하며 EBM의 일종으로 볼 수 있습니다.
4.  **테스트 시 적용**: 이러한 에너지 함수의 합성은 모델 학습 후 테스트 시에 바로 적용할 수 있으며, 별도의 추가 학습이나 미세 조정이 필요 없습니다. 이는 다른 유형의 모델(예: 지도 학습 모델)과 비교되는 큰 차이점입니다.
5.  **다양한 합성 가능성**: 단순히 두 개념을 모두 만족하는 것 외에도, 에너지 함수 연산을 통해 다양한 방식으로 개념을 합성할 수 있습니다. 예를 들어, E1과 E2 중 *하나*를 만족하는 샘플을 생성하거나 (LogSumExp 근사), E2는 만족하지만 E1은 만족하지 *않는* 샘플을 생성할 수도 있습니다 (E2 - E1).
6.  **적용 범위**: 이러한 합성 능력은 이미지(얼굴 속성, 객체 관계), 로봇 계획(궤적, 목표, 제약 조건 결합), 알고리즘적 추론(여러 연산 합성) 등 다양한 도메인에 적용될 수 있습니다. 학습된 모델 외에 **수동으로 지정된 제약 조건**을 나타내는 에너지 함수를 기존 학습된 에너지 함수와 합성하여 사용할 수도 있습니다.

요약하자면, EBM은 **개념을 수학적으로 조작 가능한 '에너지 함수'로 표현**하며, 이 함수들을 더하거나 다른 연산을 적용하여 새로운 개념을 표현하는 에너지 함수를 만들고, 이를 통해 원하는 속성을 가진 데이터를 생성하는 **자연스럽고 유연한 합성 메커니즘**을 제공하기 때문에 합성 가능하다고 할 수 있습니다. 이 메커니즘은 테스트 시에 별도의 학습 없이 동작하며, 학습 시 보지 못한 복잡한 조합이나 제약 조건을 다루는 데 유용합니다.

---

## 불확실성 모델링

에너지 기반 모델(EBM)은 다음과 같은 방식으로 불확실성을 모델링합니다:

1.  **에너지 함수를 통한 확률 분포 표현**: EBM은 개념이나 데이터의 구조를 **에너지 함수** E(x)로 표현합니다. 이 에너지 함수는 입력(x)이 해당 개념이나 데이터 분포와 얼마나 일치하는지를 스칼라 값으로 나타냅니다. 이 에너지를 사용하여 데이터 포인트 x의 확률 분포를 **볼츠만 분포** 형태로 정의합니다: P(x) ∝ exp(-E(x)). 즉, 에너지가 낮은 데이터 포인트는 확률이 높고, 에너지가 높은 데이터 포인트는 확률이 낮습니다. 이 방식을 통해 EBM은 입력 공간에 대한 확률 분포를 모델링하며, 이는 데이터의 불확실성을 포착하는 기본 메커니즘이 됩니다.

2.  **복잡하고 다봉적인(Multimodal) 분포 모델링 능력**: 지도 학습의 회귀 모델과 같은 기존 모델은 출력이 다봉성(여러 개의 가능한 결과 모드)을 가질 때 어려움을 겪을 수 있습니다. 예를 들어, 차량의 미래 목표 위치 예측과 같이 여러 가능한 경로가 있는 상황에서는 출력이 여러 모드로 나타날 수 있습니다. 회귀 모델은 이러한 다봉적인 출력을 평균화하여 흐릿하게 만들 수 있습니다. 가우시안 혼합 모델(Mixture of Gaussians)은 다봉성을 모델링할 수 있지만, 데이터가 가질 수 있는 모든 복잡한 분포를 정확하게 모델링하는 데 한계가 있을 수 있습니다 (예: 2D 공간에서 고리 모양의 분포). EBM은 이러한 모델과 달리 **임의로 복잡한 확률 분포**를 모델링할 수 있는 일반적인 목적의 모델입니다. 따라서 EBM은 상당한 불확실성이나 복잡한 다봉성 출력을 갖는 작업에서 **다양하고 필요한 모든 다봉성을 포착하여 더 선명하게 솔루션을 생성**할 수 있습니다.

3.  **샘플링을 통한 불확실성 반영**: EBM에서 데이터를 생성하거나 추론하는 과정은 일반적으로 에너지 함수에 대한 기울기 기반 최적화 또는 샘플링 절차(예: Langevin Dynamics 또는 Diffusion 모델의 반복적 프로세스)를 통해 이루어집니다. 이 과정은 노이즈에서 시작하여 에너지 랜드스케이프를 탐색하며 낮은 에너지 지점(높은 확률 지점)을 찾아가는 방식입니다. 샘플링 절차 자체는 종종 **확률적(stochastic)** 특성을 가지며, 이는 동일한 시작 노이즈나 동일한 문제 인스턴스에 대해서도 **다양한 가능한 솔루션**을 생성하게 합니다. 이는 모델이 학습한 분포 내에 존재하는 불확실성과 다양한 가능성을 반영합니다. 예를 들어, 로봇 계획에서 EBM은 시작 상태에서 목표 상태까지 도달하는 "어느 한 경로"를 계획하려 하며, 이는 해당 경로가 도달할 "어떤 확률"을 가짐을 의미합니다. 환경의 동적 변화나 불확실성으로 인해 계획이 실패하면 재계획(replan)을 통해 새로운 경로를 찾을 수 있습니다.

요약하자면, EBM은 **에너지 함수를 통해 데이터의 확률 분포를 정의**하고, 이 방식이 **임의로 복잡하고 다봉적인 분포를 효과적으로 포착**할 수 있게 함으로써 불확실성을 모델링합니다. 또한, **확률적인 샘플링/최적화 과정**을 통해 이러한 불확실성을 반영하는 다양한 출력을 생성할 수 있습니다. 이러한 능력은 특히 출력에 여러 가능한 결과가 존재하는 문제(예: 자율 주행 차량의 목표 위치 예측)에서 유용합니다.


---

## EBM에서 확률 분포

에너지 기반 모델(EBM)은 **에너지 함수를 사용하여 입력 데이터의 확률 분포를 정의하고 나타냅니다**.

구체적으로 다음과 같은 방식으로 이루어집니다.

1.  **에너지 함수의 정의**: EBM은 특정 개념(예: 미소 짓는 얼굴)이나 데이터의 구조를 나타내는 **에너지 함수 E(x)**를 가집니다. 이 함수는 입력 x(예: 이미지)를 받아 하나의 스칼라 에너지 값으로 변환합니다.
2.  **에너지 값과 일치도**: 에너지 함수는 입력 x가 해당 개념이나 데이터 분포와 얼마나 잘 일치하는지를 에너지 값의 크기로 나타냅니다.
    *   해당 개념이나 분포에 **더 일치하는 입력** (예: 미소 짓는 얼굴 이미지)에는 **낮은 에너지 값**을 할당합니다. 이 에너지 값은 음의 무한대까지 낮아질 수 있습니다.
    *   해당 개념이나 분포에 **덜 일치하는 입력** (예: 미소 짓지 않는 얼굴 이미지)에는 **높은 에너지 값**을 할당합니다.
3.  **확률 분포로의 변환 (볼츠만 분포)**: 이렇게 정의된 에너지 함수 E(x)는 데이터 포인트 x에 대한 확률 분포 P(x)를 나타내는 데 사용됩니다. 이 관계는 일반적으로 **볼츠만 분포(Boltzmann distribution)** 형태로 정의됩니다.
    *   **P(x) ∝ exp(-E(x))**
    *   이는 **에너지가 낮은 데이터 포인트일수록 exp(-E(x)) 값이 커져서 해당 데이터가 나타날 확률 P(x)가 높다**는 것을 의미합니다. 반대로 에너지가 높은 데이터 포인트는 나타날 확률이 낮습니다.
4.  **복잡한 분포 모델링**: 이러한 에너지 기반의 확률 분포 표현 방식은 **임의로 복잡한 확률 분포**를 모델링할 수 있게 합니다. 이는 특히 여러 가능한 결과가 존재하는 다봉적인(multimodal) 불확실성을 포착하는 데 유용합니다. 예를 들어, 자율 주행 차량의 미래 목표 위치 예측과 같이 여러 안전한 경로가 가능한 상황에서 EBM은 이러한 다양한 가능성(모드)을 흐릿하게 평균화하지 않고 각각을 포착하여 선명한 솔루션을 생성할 수 있습니다.

요약하자면, EBM은 **개념과의 일치도를 나타내는 스칼라 에너지 함수**를 정의하고, 이 에너지를 사용하여 **볼츠만 분포 형태의 확률 분포** P(x) ∝ exp(-E(x))를 만듦으로써, 입력 데이터 공간에 대한 불확실성과 복잡한 구조를 모델링합니다.

---

## EBM 결합 방식 및 응용

에너지 기반 모델(EBM)은 주로 **에너지 함수를 수학적으로 결합하는 방식**을 통해 다른 모델이나 제약 조건과 결합될 수 있습니다. 이 결합성은 특히 **테스트 시(test time)에 별도의 추가 학습이나 미세 조정(fine-tuning) 없이 직접 적용 가능**하다는 점이 큰 특징입니다.

EBM이 다른 모델과 결합되는 구체적인 방법은 다음과 같습니다.

1.  **개념 또는 제약 조건의 에너지 함수 표현**: EBM은 특정 개념(예: 미소 짓는 얼굴, 여성 얼굴)이나 원하는 제약 조건(예: 로봇 궤적의 시작/종료 상태, 특정 객체 배치)을 **에너지 함수 E(x)**로 표현합니다. 이 함수는 입력 x가 해당 개념이나 제약 조건을 얼마나 잘 만족하는지에 따라 스칼라 에너지 값을 반환하며, 만족할수록 낮은 에너지를 할당합니다.
2.  **에너지 함수의 합을 통한 합성**: 서로 다른 개념이나 제약 조건을 나타내는 에너지 함수 E1과 E2가 있을 때, 이들을 단순히 **더하여 새로운 합성된 에너지 함수 E_composed = E1 + E2를 생성**할 수 있습니다. 이렇게 합성된 에너지 함수는 **E1과 E2가 나타내는 두 개념/제약 조건 모두를 만족하는 입력에 낮은 에너지 값을 할당**하게 됩니다.
    *   예를 들어, '미소 짓는 얼굴' 에너지 함수와 '여성 얼굴' 에너지 함수를 더하면, '미소 짓는 여성 얼굴'에 낮은 에너지를 할당하는 새로운 에너지 함수를 얻습니다.
    *   로봇 계획에서는 궤적의 일반적인 타당성을 나타내는 학습된 에너지 함수와, 특정 시작 상태 및 목표 상태와 같은 테스트 시 제약 조건을 나타내는 에너지 함수를 더하여, 해당 제약 조건을 만족하는 타당한 궤적을 찾는 데 사용될 수 있습니다.
3.  **다른 수학적 연산을 통한 합성**: 단순 덧셈 외에도 다른 연산을 사용하여 다양한 결합 효과를 얻을 수 있습니다:
    *   두 에너지 함수에 해당하는 분포의 합(LogSumExp 근사)을 통해 **E1 또는 E2 중 하나를 만족하는 샘플**을 생성할 수 있습니다.
    *   한쪽 에너지 함수를 음수화하여 더함으로써(예: E2 - E1), **E2는 만족하지만 E1은 만족하지 않는 샘플**을 생성할 수 있습니다.
4.  **합성된 에너지 함수의 최적화/샘플링**: 이렇게 합성된 에너지 함수(E_composed)를 이용하여 데이터를 생성하거나 문제를 해결할 때, 무작위 입력에서 시작하여 에너지 함수에 대한 기울기 기반 최적화 절차(예: Langevin Dynamics)를 수행합니다. 이 과정은 합성된 에너지 함수의 에너지 랜드스케이프를 탐색하며 에너지가 낮은 지점(즉, 합성된 개념/제약 조건을 잘 만족하는 데이터)을 찾게 됩니다. Diffusion 모델도 이러한 반복적인 생성/최적화 과정을 사용하며 EBM의 일종으로 볼 수 있습니다.
5.  **다양한 형태의 제약 조건 결합**: EBM의 학습된 에너지 함수는 **수동으로 지정된(handcrafted) 제약 조건**을 나타내는 에너지 함수와도 자연스럽게 결합될 수 있습니다. 이는 학습된 모델에 새로운 테스트 시 제약 조건을 유연하게 적용할 수 있게 합니다. 예를 들어, 학습된 궤적 EBM에 수동으로 지정된 장애물 회피 에너지 함수를 추가하여 장애물을 피하는 궤적을 생성할 수 있습니다.
6.  **분해 및 재조합**: 학습된 EBM이나 이미지로부터 여러 개의 독립적인 에너지 함수(예: 얼굴의 특징들, 장면의 요소들)를 비지도 학습 방식으로 분해해낸 다음, 이 독립적인 에너지 함수들을 원하는 대로 **재조합하여 새로운 이미지를 생성**하거나 특정 요소만 수정할 수도 있습니다.

이러한 방식의 결합 능력은 이미지(얼굴 속성, 객체 관계 합성), 로봇 계획(다양한 목표, 제약 조건 동시 만족), 알고리즘적 추론(여러 연산 합성) 등 다양한 분야에 적용될 수 있습니다. EBM은 개념이나 제약 조건을 통일된 '에너지 함수' 형태로 표현하기 때문에, 이러한 함수들을 수학적으로 조작하여 복잡한 요구사항을 표현하고 충족하는 데이터를 생성하는 유연한 프레임워크를 제공합니다.

---

## 체화형 학습의 특징과 도전

제공된 자료에 따르면, 임베디드 학습(Embodied Learning)은 **기존의 전형적인 기계 학습 응용 분야와는 여러 면에서 상당한 차이**가 있습니다. 이러한 차이 때문에 기존 모델들이 임베디드 환경에서 잘 작동하지 않는 경우가 많습니다.

임베디드 학습이 다른 점들은 다음과 같습니다.

*   **심각한 분포 변화 (Substantial Distribution Shift)**: 기존 기계 학습 시스템은 학습 데이터의 분포(train distribution)와 테스트 데이터의 분포(test distribution)가 매우 유사할 때 잘 작동합니다. 하지만 임베디드 환경, 예를 들어 로봇이 실제 세계를 돌아다니는 상황에서는 **환경이 지속적으로 변화**하며, 이로 인해 학습 시에는 보지 못했던 **새로운 상황, 객체, 제약 조건**에 직면하게 됩니다. 예를 들어, 학습 시에는 테이블 위에만 있던 객체가 다른 곳에 있거나, 로봇이 평소 다니던 경로가 갑자기 막히는 상황 등 훈련 데이터와는 상당히 다른 테스트 분포에서 작동해야 합니다.
*   **지속적인 학습 및 경험 통합 (Continuous Learning & Integration of Experiences)**: 임베디드 에이전트는 환경을 탐색하면서 **과거의 경험을 지속적으로 학습하고 통합**해야 합니다. 단순히 한 번 학습된 모델을 적용하는 것에서 벗어나, 새로운 정보를 계속해서 모델에 반영해야 합니다.
*   **새로운 대상 및 제약 조건에 대한 적응력 (Adaptability to New Objects and Constraints)**: 환경 변화로 인해 이전에 보지 못했던 장애물이나 새로운 제약 조건에 **신속하게 적응**할 수 있어야 합니다. 예를 들어, 로봇이 특정 램프나 다른 객체를 만나 경로를 변경해야 하는 상황 등입니다.
*   **동적이고 불확실한 환경에서의 작동 (Operation in Dynamic and Uncertain Environments)**: 임베디드 환경은 정적이지 않습니다. 객체가 움직이거나 (예: 다른 차량), 시야가 가려지는(occlusion) 등 **환경의 동적 변화와 불확실성**이 존재합니다. 이로 인해 계획이 실패할 수 있으며, 상황 변화에 따라 **재계획(replan)이 필요**할 수 있습니다.
*   **다양한 모달리티 입력 (Multimodal Inputs)**: 임베디드 에이전트는 시각, 촉각, 청각 등 **여러 종류의 입력**을 동시에 처리해야 하는 경우가 많습니다.
*   **제한된 데이터에서의 학습 (Learning with Limited Data)**: 실제 환경에서는 대규모의 레이블링된 데이터를 항상 얻기 어렵기 때문에, **제한된 데이터**로도 효과적으로 학습해야 합니다.
*   **비가역적인 세계 및 안전 (Irreversible World and Safety)**: 실제 환경에서의 행동은 되돌릴 수 없으므로, **안전**을 최우선으로 고려하며 신중하게 행동해야 합니다.

에너지 기반 모델(EBM)은 이러한 임베디드 학습 환경의 어려움 중 일부, 특히 모델 결합을 통한 점진적 학습, 테스트 시 제약 조건 통합을 통한 적응력, 그리고 분포 변화에 대처하는 능력 등에서 유용한 도구로 제시되었습니다.

---


## 에너지 기반 모델 활용 사례

에너지 기반 모델(EBM)이 **무엇에 유용한지** 설명해 드리겠습니다.

에너지 기반 모델(EBM)은 데이터의 확률 분포를 에너지 함수로 나타내고, 에너지를 최소화하는 방식으로 데이터 생성 또는 문제 해결을 수행하는 모델입니다. 이러한 접근 방식은 특히 다음과 같은 여러 면에서 유용함을 제공합니다.

1.  **개념 또는 제약 조건의 유연한 결합 (Compositionality)**:
    *   EBM의 가장 큰 장점 중 하나는 서로 다른 개념이나 제약 조건을 나타내는 여러 에너지 함수를 **테스트 시점에 추가 학습이나 미세 조정 없이 직접 수학적으로 결합**하여 사용할 수 있다는 것입니다.
    *   두 개념 **모두**를 만족하는 데이터를 생성하고 싶다면 해당 에너지 함수들을 더할 수 있습니다 (예: '미소 짓는 얼굴'과 '여성 얼굴' 에너지 함수를 더하여 '미소 짓는 여성 얼굴' 생성).
    *   두 개념 **중 하나 이상**을 만족하는 데이터를 생성하고 싶다면 에너지 분포들의 합에서 샘플링하거나 LogSumExp 근사를 사용할 수 있습니다.
    *   특정 개념은 만족하지만 다른 특정 개념은 **만족하지 않는** 데이터를 생성하고 싶다면 원치 않는 개념의 에너지 함수를 음수화하여 더할 수 있습니다 (예: '미소 짓는 얼굴'에서 '여성 얼굴' 에너지를 빼서 '미소 짓는 남성 얼굴' 생성).
    *   이러한 방식으로 **복잡한 요구사항을 만족하는 데이터나 장면을 생성**할 수 있으며, 이는 전통적인 생성 모델로는 쉽게 하기 어려운 작업입니다.
    *   학습된 에너지 함수뿐만 아니라 **수동으로 지정된(handcrafted) 제약 조건**을 나타내는 에너지 함수와도 자연스럽게 결합될 수 있습니다.
    *   비지도 학습을 통해 이미지 등에서 **기저의 요인(underlying factors)**을 나타내는 여러 에너지 함수를 발견하고, 이를 재조합하여 새로운 이미지를 생성하는 데 활용할 수도 있습니다.

2.  **다양한 동작 및 제약 조건에 대한 적응력 (Adaptability)**:
    *   로봇 계획 분야에서 EBM은 **테스트 시점에 새로운 제약 조건이나 목표를 쉽게 통합**하여 로봇의 동작을 적응시키는 데 유용합니다.
    *   예를 들어, 합리적인 궤적을 나타내는 학습된 에너지 함수에 시작/종료 상태 제약 조건, 장애물 회피, 특정 목표 위치 등 **수동으로 지정될 수 있는** 추가적인 제약 조건을 나타내는 에너지 함수를 더함으로써, 해당 제약을 만족하는 궤적을 효과적으로 생성할 수 있습니다.
    *   이는 단일한 학습된 궤적 모델을 가지고도 별도의 학습 없이 다양한 작업을 수행할 수 있게 합니다.
    *   학습된 에너지 함수의 연속성 덕분에 전통적인 제약 조건 기반 최적화보다 더 부드러운 최적화 과정을 거치며 지역 최저점 문제에 덜 취약할 수 있습니다.

3.  **반복 계산을 통한 일반화 능력 (Iterative Computation for Generalization)**:
    *   EBM은 최적화 과정을 통해 해를 찾는 반복적인 계산 방식을 사용합니다.
    *   이는 모델이 학습 시 접하지 못한 **더 어렵거나 학습 분포와 다른(out-of-distribution) 문제 인스턴스**에 대해 일반화하는 데 도움을 줍니다.
    *   예를 들어, 작은 크기의 그래프에서 최단 경로를 계산하도록 학습된 EBM은 더 큰 크기의 그래프에 대해서도 더 많은 최적화 단계를 수행함으로써 올바른 해를 찾을 수 있습니다. 이는 단방향 예측 모델(예: Recurrent Network)이 학습 분포 밖에서 쉽게 실패하는 것과 대조적입니다.
    *   여러 연산을 순차적으로 적용하는 문제(예: 여러 행렬 덧셈)에서도 중간 결과가 학습 분포를 벗어나더라도 반복 계산을 통해 최종적인 정확도를 유지하는 데 유용합니다.

4.  **다봉성(Multimodality) 및 불확실성 포착**:
    *   EBM은 **데이터 공간의 복잡하고 다봉적인 확률 분포를 유연하게 모델링**할 수 있습니다.
    *   이는 특히 자율주행 차량의 미래 목표 위치 예측과 같이 **불확실성이 높거나 여러 가능한 결과**가 있는 문제에서 유용합니다.
    *   단일 가우시안이나 간단한 가우시안 혼합 모델로는 포착하기 어려운 **임의로 복잡한 다봉성 분포**를 효과적으로 나타낼 수 있어, 문제의 모든 가능한 해를 더 정확하게 반영할 수 있습니다.

요약하자면, 에너지 기반 모델은 에너지 함수를 통한 유연한 분포 표현과 반복적인 에너지 최적화 과정을 통해 **복잡한 개념의 결합, 다양한 제약 조건에 대한 적응, 학습 분포를 벗어나는 문제에 대한 일반화, 그리고 데이터의 다봉성 및 불확실성 포착** 등 다양한 면에서 유용함을 제공합니다.

---

## EBM 조합 문제 일반화 이유

에너지 기반 모델(EBM)이 조합 문제(combinatorial problems)에 **일반화(generalize)**되는 주된 이유는 다음과 같습니다.

1.  **에너지 함수가 '검증자(Verifier)' 역할을 학습하기 때문입니다.**
    *   알고리즘적 추론 문제(algorithmic reasoning problems)에서 에너지 함수는 입력(예: 그래프와 경로)과 가능한 해(예: 해당 경로가 최단 경로인지 여부)를 받아 에너지 값을 출력하도록 학습됩니다.
    *   이때, **정답(correct solution)에는 낮은 에너지**를 할당하고, **오답(incorrect solution)에는 높은 에너지**를 할당하도록 학습됩니다.
    *   자료에서는 이를 에너지 함수가 일종의 **'검증자(verifier)'** 역할을 한다고 설명합니다. 즉, 주어진 해가 문제의 정답인지 아닌지를 *확인하는* 방법을 배우는 것입니다.
    *   **정답을 '명시적으로 생성'하는 방법을 학습하는 것보다, 주어진 해가 정답인지 '검증'하는 방법을 학습하는 것이 더 쉬우며, 이 '검증' 기능이 더 큰 문제 인스턴스에 대해 더 잘 일반화될 수 있다**고 제안합니다. 이는 P 문제(다항 시간 안에 풀 수 있는 문제)와 NP 문제(해답이 주어졌을 때 다항 시간 안에 검증할 수 있는 문제)의 관계와 유사한 논리입니다.

2.  **해를 찾기 위해 '반복적인 계산(Iterative Computation)'을 수행하기 때문입니다.**
    *   조합 문제의 해를 찾는 과정은 **에너지 최적화(Energy Optimization)**, 즉 에너지 함수의 그라디언트(gradient)를 따라 에너지를 최소화하는 방식으로 이루어집니다.
    *   이 과정은 무작위 노이즈(random noise)에서 시작하여 여러 단계를 거쳐 점진적으로 해를 개선해 나가는 **반복적인 절차(iterative procedure)**입니다.
    *   이러한 반복적인 계산 또는 **반복적인 개선(iterative refinement)** 과정이 모델이 학습 시 접하지 못한 **학습 분포 변화(distribution shift)**에 적응하는 데 도움이 됩니다.
    *   문제가 학습 데이터와 다르거나 더 어렵더라도, 반복적인 과정을 통해 중간 단계의 오류를 수정하고 해를 점진적으로 개선해 나갈 기회를 가집니다. 이는 예측을 단방향으로 수행하는 모델이 학습 분포 밖의 입력에 쉽게 취약해지는 것과 대조됩니다. 자료에서는 더 큰 그래프에 대해 최단 경로를 찾을 때, 더 많은 최적화 단계를 수행함으로써 정확한 해를 복구할 수 있음을 보여줍니다.

따라서, EBM이 조합 문제에서 일반화 능력을 보이는 것은 에너지 함수가 해를 **'검증하는' 기능을 학습**하고, 해를 찾기 위해 **'반복적인 계산'을 활용**하여 학습 분포 변화에 적응할 수 있기 때문이라고 설명하고 있습니다.

다만, 이러한 조합 문제, 특히 매우 이산적(discrete)이고 제약 조건이 많은 공간에서 EBM을 직접 학습하고 샘플링하는 것은 여전히 어려운 문제이며, 이를 해결하기 위해 저차원 잠재 공간(lower-dimensional latent spaces)으로 문제를 변환하는 등의 방법이 모색될 수 있다고 언급됩니다.

---

## EBM의 단점

에너지 기반 모델(EBM)의 단점 또는 해결해야 할 과제는 다음과 같습니다.

*   **학습의 어려움 및 샘플링 문제 (Difficulty in Training and Sampling)**:
    *   에너지 함수 학습은 실제 데이터의 에너지 값을 낮추고 모델이 생성한 가짜 데이터("hallucinations")의 에너지 값을 높이는 방식으로 이루어집니다. 이 과정에서 가짜 데이터인 "hallucinations"을 생성하기 위해 현재 학습된 에너지 함수에 대한 반복적인 최적화 절차(예: 그라디언트 디센트)를 실행해야 합니다.
    *   학습이 진행됨에 따라 에너지 함수는 샘플(가짜 데이터)의 에너지 값을 높이려고 하지만, 샘플러는 에너지 함수의 낮은 에너지 지점을 찾으려고 합니다. 이러한 **긴장 관계(tension)** 때문에 학습 중 시간이 지날수록 **올바른 음성 샘플(correct negative samples)을 생성하기가 점점 더 어려워지고 더 많은 MCMC(Markov Chain Monte Carlo) 단계를 요구**하게 됩니다.
    *   특히 **고차원의 이산적(discrete)이고 조합적인(combinatorial) 공간**에서 EBM을 직접 학습하고 샘플링하는 것은 **매우 어려운 문제**입니다. 이러한 공간에서는 샘플링이 매우 어렵기 때문입니다. 이를 해결하기 위해 저차원의 연속적인 잠재 공간으로 문제를 변환하는 방법 등이 모색될 수 있습니다.

*   **높은 계산 비용 및 실시간 애플리케이션의 한계 (High Computational Cost and Limitations for Real-time Applications)**:
    *   EBM에서 데이터를 생성하거나 조합 문제의 해를 찾는 과정은 무작위 노이즈에서 시작하여 에너지 함수의 그라디언트를 따라 에너지를 최소화하는 **반복적인 최적화 절차(iterative optimization procedure)**를 수행하는 방식입니다.
    *   이러한 반복적인 계산은 해를 찾기 위해 여러 단계의 그라디언트 디센트 스텝을 필요로 합니다. 작은 문제에서는 몇 단계로 충분할 수 있지만 (예: 크기 10 그래프에서 10단계), **더 크거나 어려운 문제에서는 훨씬 더 많은 계산 단계(예: 크기 30 그래프에서 40단계)**가 필요합니다.
    *   이로 인해 EBM을 사용한 해 찾기 과정은 **매우 느릴 수 있으며**, **실시간 애플리케이션에는 적합하지 않을 수 있습니다**. 낮은 지연 시간이 중요한 환경에서는 이 방식이 유용하지 않을 수 있다고 언급됩니다.

*   **동적 환경 및 엄격한 제약 조건 처리의 한계 (Limitations in Handling Dynamic Environments and Hard Constraints)**:
    *   EBM을 사용하여 생성된 계획이나 궤적(예: 로봇 계획)은 환경의 변화에 **반응적(reactive)이지 않습니다**. 만약 환경의 역학(dynamics)이 변하면 계획을 **재계획(replan)해야** 합니다.
    *   에너지 최적화 과정을 통해 **엄격한 제약 조건(hard constraints)을 만족시키는 것이 보장되지 않습니다**. 학습된 에너지 함수는 연속적이고 부드러운 최적화 경로를 제공하여 지역 최저점 문제를 피하는 데 도움이 될 수 있지만, 최적화 중의 **중간 단계 솔루션이 항상 제약 조건을 만족시키는 것은 아니며**, 최종 솔루션도 *일반적으로* 만족시키지만 *보장되는 것은 아닙니다*.
    *   따라서 만약 엄격한 제약 조건 만족이 필수적이라면, EBM 최적화 후 전통적인 최적화 기반 방법을 추가로 실행하여 경로 제약을 보장해야 할 수 있습니다.

*   **결과의 무작위성 및 개념 비분리성 가능성 (Potential for Result Variability and Lack of Perfect Disentanglement)**:
    *   해를 찾는 에너지 최적화 절차는 무작위 노이즈에서 시작하는 **확률적(stochastic) 과정**이며, 이로 인해 동일한 입력에 대해서도 약간씩 다른 최종 솔루션이 나올 수 있습니다.
    *   여러 에너지 함수를 합성할 때, 학습된 에너지 함수가 각 개념을 **완벽하게 분리(disentangle)하지 못했을 가능성**이 있으며, 이로 인해 합성 결과가 예상과 약간 다를 수 있습니다.

요약하자면, EBM은 조합성, 일반화, 다양한 분포 모델링 등 여러 강력한 장점을 가지고 있지만, **학습 및 샘플링의 기술적인 어려움, 솔루션 탐색 과정의 높은 계산 비용으로 인한 속도 문제, 동적 환경 및 엄격한 제약 조건 처리의 제한적인 보장** 등은 해결해야 할 주요 과제라고 할 수 있습니다.
