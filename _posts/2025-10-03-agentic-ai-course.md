---
layout: post
title: Agentic AI MOOC
date: 2025-10-03
categories: [artificial intelligence]
tags: [artificial general intelligence]

---


# [CS294-196 (Agentic AI MOOC) - Lecture 1 {Yann Dubois}](https://www.youtube.com/watch?v=btq1TqMFrxE)


## Abstract

LLM 훈련 파이프라인의 세 가지 주요 단계인 사전 훈련(pre-training), 고전적인 후처리(classic post-training) 또는 RLHF, 그리고 **추론을 위한 강화 학습(RL for reasoning)**에 중점을 둡니다. 특히 데이터 수집 및 품질 관리, 아키텍처, 훈련 알고리즘, 평가, 시스템 및 인프라 등 LLM 훈련에 필수적인 주요 측면들을 상세히 다루며, 사전 훈련 단계에서의 다음 단어 예측 방법론과 후처리 단계에서의 **지도 미세 조정(SFT) 및 강화 학습을 통한 정렬(alignment)**의 중요성을 강조합니다.

대규모 언어 모델(LLM) 훈련 파이프라인은 일반적으로 세 가지 주요 단계로 구성되며, 각 단계는 고유한 목표와 병목 현상을 가지고 있습니다.

### 대규모 언어 모델 훈련 파이프라인의 주요 단계

LLM 훈련 파이프라인에는 **사전 훈련(Pre-training)**, **고전적인 사후 훈련(Classic Post-training)** 또는 **RLHF**, 그리고 **추론(Reasoning)** 단계가 있습니다. 후자의 두 단계는 종종 **사후 훈련(Post-training)**으로 통칭되기도 합니다.

#### 1. 사전 훈련 (Pre-training)

*   **목표:** **인터넷에 있는 모든 것을 예측하는 것**을 목표로, 가장 높은 수준에서는 **다음 단어를 예측**하는 것입니다. 이 단계를 통해 모델은 세상의 모든 것을 배우게 됩니다.
*   **데이터 규모:** 일반적으로 약 **10조 개의 토큰**이 필요합니다. 현재 모델(예: Llama 4, Deepseek V3, Llama 3)은 15조에서 40조 개의 토큰으로 훈련됩니다. 이는 대략 200억 개 이상의 고유 웹페이지에 해당합니다.
*   **시간 및 비용:** 훈련하는 데 몇 달이 걸리며, 컴퓨팅 비용은 대략 **1,000만 달러** 규모입니다.
*   **세부 사항:** 이 단계에서 모델은 훈련 데이터의 양을 최대한 많이 투입하는 것이 중요합니다. 좋은 데이터를 확보하기 위해 인터넷에서 발견되는 대량의 원시 데이터(예: Common Crawl의 2,500억 페이지)에서 HTML 텍스트를 추출하고, 유해 콘텐츠 필터링, 중복 제거, 휴리스틱 필터링 (예: 문서 길이 기반), 그리고 모델 기반 필터링 (예: Wikipedia에서 참조될 가능성이 높은 페이지 예측) 등의 과정을 거칩니다.

| 병목 현상 | 세부 설명 |
| :--- | :--- |
| **좋은 데이터와 충분한 데이터** | 인터넷 데이터의 대부분이 품질이 낮고(dirty), 필요한 토큰 수가 매우 많기 때문에 고품질의 대규모 데이터를 확보하는 것이 어렵습니다. |
| **컴퓨팅 자원** | 훈련에 수개월이 걸리고 1,000만 달러 규모의 비용이 소요되는 등 막대한 컴퓨팅 자원이 필요합니다. |

#### 2. 고전적인 사후 훈련 (Classic Post-training) 또는 RLHF

*   **목표:** 사전 훈련을 통해 인터넷의 모든 것을 알게 된 모델을 실제로 **사용자의 선호도를 극대화**하는 모델로 만드는 것입니다. 이는 **정렬(Alignment)** 또는 **명령 따르기(Instruction Following)**라고도 불리며, 모델이 실제 세계 작업에서 유용하도록 유도합니다. ChatGPT가 널리 퍼진 것은 주로 이 사후 훈련 덕분입니다.
*   **데이터 규모:** 대략 **10만 개** 문제 규모의 데이터를 사용합니다.
*   **시간 및 비용:** 며칠이 걸리며, 사전 훈련에 비해 상당히 저렴합니다. 컴퓨팅 비용은 대략 1만 달러에서 10만 달러 규모입니다.
*   **세부 사항:** 이 단계는 오픈 소스 커뮤니티에서 컴퓨팅 측면에서 접근성이 더 높기 때문에 주로 집중하는 분야입니다.
    *   **지도 미세 조정(SFT, Supervised Fine-Tuning):** 사후 훈련의 첫 번째 주요 방법으로, 모델이 원하는 답변을 제공하도록 매우 적은 양의 고품질 데이터(예: 인간이 작성한 질문-답변)를 사용하여 미세 조정합니다. SFT는 원하는 스타일이나 포맷, 명령어 따르기, 도구 사용 등을 학습시킬 수 있습니다.
    *   **RLHF (인간 피드백 기반 강화 학습):** 모델이 인간의 선호도를 극대화하도록 최적화하는 단계입니다. 이는 더 주관적인 선호도를 최적화하는 데 사용됩니다.

| 병목 현상 | 세부 설명 |
| :--- | :--- |
| **데이터 및 평가(Evals)** | 사전 훈련 데이터와 달리 사후 훈련 데이터(질문-답변 쌍)는 희소하고 수집하는 데 비용이 많이 듭니다. 또한, 진척 상황을 알기 위한 좋은 평가 지표(evals)를 만드는 것도 중요합니다. |

#### 3. 추론 (Reasoning)

*   **목표:** 객관적인 답이 있는 질문(예: 수학 또는 코딩 대회)에 대해 **답변을 제공하기 전에 모델이 깊이 생각하도록** 가르치는 것입니다. DeepSeek R1이나 Kimmy와 같은 모델이 이 단계를 사용합니다.
*   **데이터 규모:** 대략 **100만 개** 문제 규모의 데이터(추정치)를 사용합니다.
*   **시간 및 비용:** 몇 주가 걸리며, 컴퓨팅 비용은 대략 **100만 달러** 규모입니다.
*   **세부 사항:** 이 단계에서는 검증 가능한 보상(verifiable rewards)이 있는 어려운 작업(예: 코딩 테스트 케이스 통과)을 사용하여 훈련합니다. 이 과정에서는 **강화 학습(RL)**이 사용되며, 행동 복제(SFT) 대신 원하는 행동을 극대화하는 방식입니다.

| 병목 현상 | 세부 설명 |
| :--- | :--- |
| **RL 환경 구축 및 해킹** | 강화 학습에서는 보상을 최적화하는 과정에서 모델이 의도하지 않은 방식으로 보상을 최적화하는 **자기 해킹(self-hacking)** 방식이 발생할 수 있습니다. 따라서 견고한 RL 환경을 구축하는 것이 중요합니다. |
| **인프라(Infra)** | RL 훈련, 특히 에이전트 작업에서는 롤아웃(rollouts, 모델 출력 생성) 시간이 매우 길어질 수 있으므로, GPU가 대기 상태에 빠지지 않도록 인프라를 효율적으로 관리하는 것이 핵심 병목 현상입니다. |

---

### LLM 훈련의 실질적인 중요 요소

LLM을 훈련할 때 고려해야 할 5가지 큰 요소가 있지만 (아키텍처, 알고리즘, 데이터, 평가, 시스템), 실제로 가장 중요한 것은 다음과 같습니다:

1.  **데이터:** 알고리즘보다 훨씬 중요하며, 좋은 데이터 세트를 구축하는 데 시간을 투자하는 것이 가장 중요합니다.
2.  **평가(Evaluation):** 진행 상황을 파악하기 위해 매우 중요합니다. 좋은 평가 지표(evals)가 없으면 높은 수준의 하이퍼파라미터 선택 방법을 알 수 없습니다. 특히, LLM에서는 정답을 알기 어려운 **개방형 평가(open-ended evaluation)**가 어렵고 자동화하기 힘듭니다.
3.  **시스템 및 인프라:** 모델을 더 크게 확장하고 더 오래 훈련할 수 있도록 보장합니다. 더 나은 인프라는 더 나은 성능으로 이어집니다.

<iframe width="600" height="400" src="https://www.youtube.com/embed/btq1TqMFrxE?si=bRbqgEZi_XH68lPb" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>