---
layout: post
title: Joint Embedding Predictive Architecture (JEPA)
date: 2026-01-15
categories: [artificial intelligence]
tags: [artificial general intelligence]

---


# Joint Embedding Predictive Architecture (JEPA)

## Joint Embedding Predictive Architecture (JEPA) 개요

Joint Embedding Predictive Architecture (JEPA)는 Yann LeCun이 2022년에 제안한 자가 지도 학습(Self-Supervised Learning) 프레임워크로, 입력 데이터를 임베딩 공간에서 예측하는 방식으로 동작합니다. 기존 생성 모델(예: autoregressive 모델)이 픽셀 수준이나 토큰 수준에서 세부 사항을 재구성하는 데 초점을 맞춘 반면, JEPA는 추상적인 표현 공간에서 예측을 수행하여 노이즈나 예측 불가능한 세부 사항을 무시하고 고수준의 의미론적(semantic) 특징을 학습합니다. 이는 에너지 기반 모델링과 정규화 기법(예: VICReg)을 통해 정보 내용을 최대화하며, 이미지, 비디오, 오디오 등 다양한 모달리티에 적용 가능합니다. JEPA는 고급 기계 지능(Advanced Machine Intelligence, AMI)을 향한 단계로, 세계 모델(World Model)을 형성하여 추론과 계획을 지원합니다.

JEPA의 발전은 2023년부터 본격화되었으며, 2024~2026년 사이에 다양한 변형이 등장했습니다. 아래에서 최신 논문과 기술 문서를 상세히 설명하겠습니다. 설명은 논문의 배경, 방법론, 실험 결과, 그리고 함의를 중심으로 하며, 테이블로 주요 논문을 요약했습니다.

<iframe width="600" height="400" src="https://www.youtube.com/embed/yUmDRxV0krg?si=p4bS_3dR70JIIfQN" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

## 주요 JEPA 변형 논문 및 기술 문서 상세 설명

### 1. I-JEPA: Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture (2023)
이 논문은 JEPA의 첫 구체적 구현으로, 이미지 기반 자가 지도 학습을 제안합니다. 배경: 기존 대비 학습(Contrastive Learning)이 데이터 증강에 의존하는 문제를 해결하기 위해, 마스킹된 이미지 영역을 임베딩 공간에서 예측합니다. 방법론: 이미지에서 큰 블록을 마스킹하고, 주변 맥락에서 대상 블록의 표현을 예측하는 비생성적 접근. VICReg 정규화를 사용해 표현의 정보성을 유지하며, Vision Transformer(ViT)를 인코더로 활용합니다. 실험: ImageNet에서 72시간 이내 학습으로, 분류(79.9% top-1 정확도), 객체 카운팅, 깊이 예측 등에서 강력한 성능. 데이터 증강 없이도 SOTA(State-of-the-Art) 달성. 함의: JEPA가 효율적이고 확장 가능함을 증명, 후속 비디오/멀티모달 확장의 기반.

### 2. V-JEPA: Video Joint Embedding Predictive Architecture (2024)
Meta에서 발표한 비디오 버전 JEPA로, 비디오의 시공간 영역을 예측합니다. 배경: 인간처럼 비디오를 관찰하며 세계 이해를 학습하는 AMI 비전 실현. 방법론: 비생성 모델로, 비디오의 대규모 시공간 영역을 마스킹하고 추상 표현 공간에서 예측. 마스킹 전략은 랜덤 패치 대신 의미 있는 큰 블록 사용. 인코더와 예측기를 공유하며, 레이블 없이 200만+ 비디오로 사전 학습. 실험: Kinetics-400(81.9% 정확도), Something-Something-v2(72.2%), ImageNet(77.9% top-1, 비디오만으로 이미지 작업 수행)에서 동결 평가(frozen evaluation)로 SOTA. 레이블 효율성 우수(5~50% 레이블로 성능 우위). 학습 속도 1.5~6배 효율적. 함의: 물리적 세계 모델로 작용, 행동 인식과 객체 상호작용 이해 강화. 오디오 추가나 장기 예측 확장 제안. Creative Commons 라이선스로 공개.

### 3. VL-JEPA: Joint Embedding Predictive Architecture for Vision-language (2025)
비전-언어 모델로 JEPA를 확장한 최신 작업. 배경: 기존 VLM(Vision-Language Model)이 토큰 기반 생성으로 비효율적임을 해결. 방법론: JEPA를 사용해 텍스트의 연속 임베딩을 예측, Llama 3 기반 예측기 초기화. 이미지-텍스트 쌍에서 마스킹된 텍스트를 임베딩으로 예측하며, 선택적 디코딩 지원. 실험: 더 적은 파라미터로 SOTA VLM과 경쟁(예: 이미지 캡셔닝, VQA에서 우수). 오픈 보캐뷸러리 작업 지원. 함의: 효율적 VLM으로, 추상 공간 예측이 세부 재구성보다 우수함 증명. Meta 연구자들이 제안.

<iframe width="600" height="400" src="https://www.youtube.com/embed/TQx-GP7j-Hc?si=MpALsL1j_ehM4KaS" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

<iframe width="600" height="400" src="https://www.youtube.com/embed/BolXXGOosVI?si=p11_GidcNKhqYz2F" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

### 4. MC-JEPA: A Joint-Embedding Predictive Architecture for Self-Supervised Learning of Motion and Content Features (2023, 확장 2024+)
모션과 콘텐츠를 공동 학습. 배경: 이미지 특징과 광학 흐름(Optical Flow)을 통합. 방법론: 공유 인코더로 콘텐츠(객체 모양)와 모션(움직임)을 예측. 실험: 의미 분할(Semantic Segmentation)과 광학 흐름 벤치마크에서 SOTA. 함의: 두 작업이 상호 보완, 비디오 JEPA 확장의 초기 예.

### 5. D-JEPA: Denoising with a Joint-Embedding Predictive Architecture (2024)
JEPA를 생성 모델링에 통합. 배경: JEPA를 마스킹된 이미지 모델링으로 재해석. 방법론: 디퓨전 손실(Diffusion Loss)을 추가해 연속 공간 생성. 오토레그레시브 방식으로 데이터 생성. 실험: ImageNet 생성에서 SOTA FID 스코어. 함의: JEPA의 생성 확장, 노이즈 제거에 강력.

### 6. Audio-JEPA: Joint-Embedding Predictive Architecture for Audio Representation Learning (2025)
오디오 도메인 JEPA. 배경: 스펙트로그램 패치를 마스킹. 방법론: 잠재 표현 예측으로 오디오/음성 분류. 실험: SOTA 성능. 함의: JEPA의 멀티모달 확장.

### 7. Text-JEPA: A Joint-Embedding Predictive Architecture for the Conversion of Natural Language into First-Order Logic (2025)
텍스트 JEPA. 배경: 자연어를 논리 형식으로 변환. 방법론: 자가 지도 학습으로 텍스트 임베딩 예측. 실험: QA 시스템에서 설명 가능성 향상. 함의: 논리 추론 강화.

### 8. 기타 최신 변형 (2024-2025)
- **DMT-JEPA (2024)**: 차별적 마스킹 타겟으로 지역 의미 이해 강화. ImageNet 분류 등에서 우수.
- **Stem-JEPA (2024)**: 음악 스템 호환성 추정, timbre/harmony 학습.
- **JEPA-Reasoner (2025)**: 잠재 추론과 토큰 생성 분리, 멀티스레드 추론 지원.
- **Brain-JEPA (2024)**: 뇌 활동 분석, 마스킹으로 인구통계/질병 예측 SOTA.
- **T-JEPA (2024)**: 테이블 데이터 자가 지도 학습, 증강 없이 예측.
- **ACT-JEPA (2025)**: 정책 표현 학습, 모방 학습에서 일반화 향상.
- **3D-JEPA (2025)**: 3D 포인트 클라우드 학습, 로봇/AR 적용.

## JEPA 관련 최신 설문 및 튜토리얼 문서
- **Tutorial on JEPA (2025, TechRxiv)**: JEPA 기초(맥락-타겟 생성, 잠재 예측), 모달리티별 구현(이미지/비디오/오디오/포인트 클라우드), 애플리케이션(다운스트림 태스크, 에이전트 AI), 미래 방향(6G 네트워크 등 도전) 상세. JEPA를 다단계 예측기로 세계 모델 통합 제안.
- **Survey on JEPA and World Models (2025, SSRN)**: JEPA와 세계 모델 통합 프레임워크 제시. 자율 AI 발전, 상호 보완성 논의. 비판적 전망: 통합으로 일반화/계획 향상, 도전은 도메인 확장.

| 논문 제목 | 발표 연도 | 주요 모달리티 | 핵심 기여 | 성능 하이라이트 | 출처 |
|----------|----------|---------------|----------|------------------|------|
| I-JEPA | 2023 | 이미지 | 마스킹 기반 임베딩 예측 | ImageNet 79.9% top-1 | arXiv/CVPR  |
| V-JEPA | 2024 | 비디오 | 시공간 마스킹, 동결 평가 | Kinetics-400 81.9% | Meta Blog  |
| VL-JEPA | 2025 | 비전-언어 | 연속 임베딩 예측 | VQA/캡셔닝 SOTA | arXiv  |
| D-JEPA | 2024 | 이미지 생성 | 디퓨전 통합 | ImageNet FID SOTA | ICLR  |
| Audio-JEPA | 2025 | 오디오 | 스펙트로그램 예측 | 오디오 분류 SOTA | arXiv  |
| Text-JEPA | 2025 | 텍스트 | 논리 변환 | QA 설명 가능성 향상 | ACM  |
| Brain-JEPA | 2024 | 뇌 신호 | 시공간 마스킹 | 질병 진단 SOTA | Hugging Face  |
| ACT-JEPA | 2025 | 정책 학습 | 행동 예측 | 모방 학습 일반화 | arXiv  |

